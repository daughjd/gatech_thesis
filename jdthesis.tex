\documentclass{gatech-thesis}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{wrapfig}
\usepackage{changepage}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{url}

%%
%% This example is adapted from ucthesis.tex, a part of the
%% UCTHESIS class package...
%%
\title{Search for Neutrino Transients Using IceCube and DeepCore}
\author{Jacob D. Daughhetee}
\principaladviser{Professor Ignacio Taboada}
\committeechair{Professor Pablo Laguna}
\firstreader{Professor Nepomuk Otte}
\secondreader{Professor Sven Simon\\(Earth and Atmospheric Science)}
\thirdreader{Professor John Wise}
%%\fourthreader{Professor }
\department{School of Physics}
\degree{Doctor of Philosophy}
\copyrightyear{2015}
\submitdate{January 2015}
\bibfiles{jdthesis}
%% The following are the defaults
%%    \titlepagetrue
%%    \signaturepagetrue
%%    \copyrightfalse
%%    \figurespagetrue
%%    \tablespagetrue
%%    \contentspagetrue
%%    \dedicationheadingfalse
\bibpagetrue
%%    \thesisproposalfalse
%%    \strictmarginstrue
\begin{document}
\bibliographystyle{gatech-thesis}
%%
\begin{preliminary}
\begin{dedication}
\null\vfil
{\large
\begin{center}

\end{center}}
\vfil\null
\end{dedication}

\begin{preface}
This dissertation is based on data acquired with the IceCube Neutrino Observatory whose maintenance and operation is the result of an immense international collaborative effort. The bulk of the work pertaining to experimental hardware, data acquisition, reconstruction algorithms, and simulation presented in this document can be attributed to many IceCube collaborators. However, the refinement of the event selection and subsequent analysis of the data are the original work of the author.
\end{preface}

\begin{acknowledgements}
I want to thank my fellow graduate student office mates whose constant distractions helped me retain my sanity.
\end{acknowledgements}
% print table of contents, figures and tables here.
\contents
% if you need a "List of Symbols or Abbreviations" look into
% gatech-thesis-gloss.sty.

\begin{summary}

% Long Comments
\long\def\/*#1*/{}
\/*
*/

Observations indicate that there is a correlation between long duration gamma-ray bursts (GRBs) and core-collapse supernovae (SNe).  The leading model for GRB production assumes that relativistic jets are generated by the core-collapse within the progenitor star.  Charged particles undergo Fermi-acceleration within internal shocks of these jets and subsequently give rise to gamma ray emission once the jets breach the surrounding stellar envelope.  Very few SNe result in the occurrence of GRBs, however, it has been suggested that a significant fraction of core-collapse SNe manage to produce mildly relativistic jets.  These jets are insufficiently energetic to break through the envelope and are effectively 'choked' resulting in a lack of observed gamma ray emission.  In both the failed and successful GRB scenario, neutrino production can occur if protons are accelerated in the internal shocks of these jets.  These neutrinos may be detectable by the IceCube neutrino observatory and its low energy extension DeepCore. This thesis presents the methods and results of a dedicated search for temporal and spatial clustering of neutrino events during the IceCube 2012 data season. Examination of 22,040 neutrino event candidates acquired over a detector livetime of 330 days revealed no statisically significant transient source of neutrino emission. Limits on the rate of choked GRBs in the nearby universe for possible values of neutrino emission model parameters are presented.


\end{summary}

\end{preliminary}
%%
\chapter{Introduction}
The expansion of traditional optical astronomy into wavelengths unobservable to the human eye revealed myriad phenomena previously unknown to science. Use of wavebands of light spanning several orders of magnitude allowed for the discovery of completely new astronomical sources. Additionally, it allowed for the study of inherently different physical processes within and around source objects. Yet, for all the vast advances in our understanding of the universe the opening up of the electromagnetic spectrum has brought us, it relies entirely upon the physical properties of its messenger particle, the photon.

Absorption of light, either by intervening matter or other background photons, limits the number and type of source objects optical astronomy can hope to either observe or characterize. In order to explore regions of high density as well as very high-energy processes, entirely different methods of observation are required. The limitations imposed by light-based astronomy have led to the dedicated investigation of other particles and phenomena as potential cosmic messengers. This rapidly developing field, often referred to as multi-messenger astronomy, attempts to explore physical regions inaccessible to standard astronomy through the use of the highest energy cosmic rays, gravitational radiation, and high-energy neutrinos. These channels provide a unique window into the universe albeit each with their own detection challenges.

The neutrino in particular provides many excellent properties for potential use as an astrophysical messenger. Due to its very low probability of interaction, it is able to provide information from some of the densest regions within the interiors of sources. Additionally, neutrinos are able to stream freely as they propagate from their origin without suffering absorption in intervening matter. Therefore any successfully extracted neutrino signal would provide unperturbed information about the physics of the source. These characteristics also make neutrinos exceptionally difficult to detect. Nonetheless, the possible insight into high-energy astrophysics neutrinos can provide has spurred the development of large-scale detectors sensitive to expected astrophysical fluxes.

One such detector, the IceCube Neutrino Observatory \cite{2006APh....26..155I}, was constructed specifically to search for high-energy neutrinos ($\geq$1 TeV) of astrophysical origin. The experiment has taken data continuously since 2005 in both partial (2005-2011) and fully constructed (2011-present) configurations. As the experiment has matured, many different types of analysis methods have been developed to look for specific astrophysical signatures such as Gamma-ray Bursts (GRBs), Active Galactic Nuclei (AGN), and diffuse fluxes of neutrinos. These analyses have primarily focused on energetic ($\geq$1 TeV) muon tracks produced by muon neutrinos interacting within or outside of the detection volume. With the advent of DeepCore\cite{2012APh....35..615A}, the energy threshold of the combined detector has been lowered significantly allowing for the detection of neutrino events as low as 10 GeV. While the addition of DeepCore has already shown to be immensely useful for both the observation of neutrino oscillations and the sensitivity of indirect dark matter searches, there has been little incorporation of lower energy muon neutrino events into the traditional IceCube transient and steady-state point source searches.

Although the effective area of the IceCube-DeepCore detector at sub-TeV energies is significantly reduced, the use of traditional analysis methods on these lower energy events can provide an additional probe for possible neutrino sources in a lower energy regime. In the event that nearby neutrino sources are characterized by either soft-spectra or an energy cutoff, an analysis optimized to make use of low energy events can actually be more sensitive than analyses of the traditional IceCube data sample. The use of lower energy events is not without its drawbacks, however. The accuracy of directional reconstructions in IceCube depends heavily upon the number of light sensors that register photons from a given neutrino event. How many sensors are triggered is directly related to the energy deposited by the interacting neutrino resulting in lower energy events suffering from much worse resolution on average. One of the nearly irreducible backgrounds in IceCube analyses, the flux of atmospheric neutrinos, is strongly energy dependent as well. Due to the very steep spectrum of these neutrinos produced in cosmic ray air showers, any soft-spectrum astrophysical steady source will be exceedingly difficult to parse out from background.

When these difficulties are taken into consideration, it becomes readily apparent that an analysis designed to search for transient astrophysical neutrino sources is the optimal way to use a low energy muon neutrino sample in IceCube. Some potential sources include gamma-ray bursts (GRBs) \cite{2014arXiv1410.0679K}, core-collapse supernovae (SNe) with accompanying jets \cite{2004PhRvL..93r1101R}, and active galactic nuclei (AGN) \cite{2009APh....31..138B}. Whether these astrophysical events are accompanied by a substantial flux of energetic neutrinos is still unknown. Theorist predictions of neutrino emission from these sources indicate that a dedicated IceCube transient search may be sensitive for some values of model parameters. The aforementioned SNe harboring energetic jets are one of the more promising possible sources of neutrino emission. In a similar fashion to GRBs, these events produce neutrinos via internal collisions within the jet. However, unlike GRBs, the jets from these progenitors are insufficiently energetic to break out of the surrounding stellar envelope and are effectively 'choked' off preventing the escape of gamma-rays. Due to the lack of strong gamma-ray emission, it is unknown what fraction of core-collapse SNe have jets, but it is likely that the rate of these events is much higher than that of fully-fledged GRBs.

The work presented in this thesis details the development and results of an IceCube-DeepCore analysis optimized to search for transient soft-spectra neutrino sources with a specific focus on neutrino emission from potential choked GRBs. This is accomplished through the modification and application of previously developed time-dependent point source search techniques to a set of neutrino events much lower in energy than the typical IceCube sample. The following sections of this document will describe the acquisition of the data for this analysis, the selection criteria imposed on that data, and the analysis methods applied on the final set of events. In addition, pertinent background information pertaining to neutrino physics and methods in neutrino astronomy will be provided. Neutrino emission models for multiple source classes will also be discussed with a focus on the predicted neutrino emission from choked GRBs. Lastly, the details of the analysis result will be interpreted in light of its implications on the possible values of choked GRB model parameters.

\chapter{Neutrino Properties}
The neutrino is an electrically neutral particle that interacts only via the weak nuclear force and gravity. Its cross-section for interaction with ordinary matter is exceedingly small making the experimental detection of the neutrino a difficult task. It is classified as a lepton in the Standard Model, meaning that it is an elementary particle with $\frac{1}{2}$-integer spin (a fermion) and no strong force interaction. Neutrinos come in three variations with one corresponding to each of the three charged leptons present in the standard model: the electron, the muon, and the tau. The neutrino is the lightest of the  elementary fermions by a wide margin, and though observations indicate neutrinos are massive, currently only upper limits on the mass of individual neutrino types are known.

%% History
The first evidence for the existence of the neutrino came through the observation of the energy distribution of electrons emitted by nuclei undergoing $\beta$-decay. In 1930, Wolfgang Pauli postulated the existence of a light, electrically neutral particle as a solution to the problem of missing energy and momentum \cite{PauliLetter}. The idea was not unanimously well-received as he was ultimately suggesting a nigh impossible to detect particle as the solution to the conundrum. However, the idea of Pauli's hypothetical particle seemed much more palatable than the troubling alternative that energy or momentum may not be universally conserved. Confirmation of the existence of this proposed particle would not occur until 1956 through the detection of anti-electron neutrinos streaming from the nuclear fission reactors of the Savannah River Plant \cite{1956Sci...124..103C}.

Despite its discovery nearly six decades ago, a complete understanding of neutrino properties still eludes the physics community. The results of additional neutrino detection experiments revealed that neutrinos held some unsuspected peculiar properties. The most notable of these experiments was the detection of the flux of solar neutrinos via inverse beta decay in Homestake Mine by Ray Davis \cite{PhysRevLett.20.1205}. The experiment only measured one third of the expected neutrino flux predicted by the solar model. The conflict would eventually be resolved when it was determined that neutrinos undergo flavor oscillations as they propagate. This specific deviation of neutrino behavior from theoretical predictions in some sense encapsulates the trend of neutrino behavior defying expectations. As of today, many important properties of the neutrino are still undetermined such as the absolute value of the mass eigenstates, the possibility of the neutrino being its own antiparticle, ordering of the mass eigenstates (hierarchy problem), number of neutrino types, and the possibility of CP violation during oscillations. 

While these issues are extremely interesting in their own right, this section will only attempt describe the neutrino properties relevant to the analysis being presented. Thus, the primary topic of discussion will be the interaction modes of high energy neutrinos. In addition, the phenomenon of neutrino flavor oscillations will also be described in brief due to the important role flavor composition at Earth of a given source flux plays in the capability of detection by IceCube.

\section{Interactions in Matter}
One of the defining characteristics of the neutrino is the ability to stream through large distances of ordinary matter unperturbed. However, neutrinos do occasionally interact with normal matter through several different interactions of varying complexity. The energy of the neutrino and the composition of the target material determine the most likely mode of interaction. With that in mind, this section will only attempt to detail the neutrino-matter interaction of greatest importance in IceCube, i.e. the scattering of a neutrino with a target nucleon. While many other interaction processes can also occur (neutrino-electron scattering, inverse beta decay, coherent scattering with nuclei, etc.), the cross-section for these processes is far smaller than neutrino-nucleon scattering for neutrinos with energies of interest to IceCube and Deepcore ($E_{\nu} \geq$ 10 GeV).

\subsection{Neutrino-Nucleon Scattering}

The dominant mode of interaction for neutrinos detected by IceCube is through scattering with a nucleon of a hydrogen or oxygen atom within the ice. This interaction is carried out via the weak nuclear force and involves the exchange between the neutrino and the nucleon of either a $W^{+}$, $W^{-}$, or Z boson. Interactions in which a charged W boson is exchanged are referred to as charged-current (CC) and have the following form:
\begin{eqnarray}
\nu_{l} + N \rightarrow l + X
\end{eqnarray}
A neutrino of flavor $l$ will yield its counterpart charged lepton while the final state $X$ of nucleon $N$ will be dependent on the magnitude of the momentum exchange in the interaction and can range from ejection of a nucleon from the nucleus to total breakup and a particle shower. Neutral-current (NC) interactions occur via exchange of the chargeless Z boson and as such yield a different end state:
\begin{eqnarray}
\nu_{l} + N \rightarrow \nu_{l}' + X
\end{eqnarray}
Unlike CC interactions, only the energy imparted to the target nucleus will be observable as the outgoing neutrino $\nu_{l}'$ will carry some fraction of the energy. As is the case in the CC scenario, the end state for the nucleon in NC interactions will vary according to the energy departed by the neutrino.

How deeply these interactions probe the internal structure of the target nucleon is ultimately dependent on energy of the incoming neutrino. For neutrinos below 20 GeV in energy, the exact manner in which the neutrino interacts with the target nucleon can be quite complicated as there are three categories of scattering interaction possible. These modes include quasi-elastic scattering, resonance production, and deep inelastic scattering \cite{2012RvMP...84.1307F}. Nearly all interaction products from neutrino events detected by IceCube are the result of deep inelastic scattering. However, other modes of interaction can be much more prevalent in a DeepCore sample as the energy threshold of the detector extends to approximately 10 GeV. Figure \ref{fig:neutrino_scattering} shows the relative size of the cross-section for each of the aforementioned processes as a function of neutrino energy. The higher number of possible end states provided by quasi-elastic and resonance scattering make simulation and proper estimation of the cross-section for neutrino events of 1-20 GeV events a fairly complex ordeal. Quasi-elastic scattering and resonance production will only be briefly described as very few of the neutrino events present in the final event sample of the presented analysis interacted through either of these means.

\begin{wrapfigure}{l}{0.5\textwidth}
  \begin{center}
    \includegraphics[width=0.48\textwidth,keepaspectratio]{neutrino_nucelon_crosssections.png}
  \end{center}
  \caption{Total inclusive cross-section is plotted above as a function of energy along with individual cross-sections for different interaction channels. Above a few tens of GeV, it's clear that deep inelastic scattering begins to dominate \cite{2012RvMP...84.1307F}.}
  \label{fig:neutrino_scattering}
\end{wrapfigure}

In the elastic (NC) or quasi-elastic (CC) scenario, an incoming neutrino will scatter off of the nucleon as a whole imparting some of its energy to the target. This is usually sufficient to eject the target nucleon and in some cases additional nucleons from the nucleus. The CC scattering is referred to as quasi-elastic due to the fact that it requires the conversion of the target proton(neutron) into a neutron(proton) to ensure charge conservation. This type of scattering does not probe the internal constituents of the nucleus (quarks and gluons collectively referred to as partons). The absolute cross-section, however, will depend heavily on nuclear properties of the target atom. Examination of Figure \ref{fig:neutrino_scattering} shows that even in a DeepCore dominated event sample this process will very rare given the threshold for neutrino detection is approximately 10 GeV.

Above 1 GeV in neutrino energy, resonance production overtakes quasi-elastic scattering in relative contribution to the total interaction cross-section. In this form of inelastic scattering, the momentum transfer provided by the neutrino collision can excite the nucleon target. The decay of this excited state will generally involve the emission of an energetic meson (a particle consisting of a quark-anti-quark pair). At lower energies the emitted meson will usually be a single pion ($\pi^{+},\pi^{-}$), however at higher energies multiple pions can be produced as well as strange mesons such as the kaon. An example muon neutrino resonance interaction takes the following form:

\begin{eqnarray}
\nu_{\mu} + N \rightarrow \mu^{-} + N^{*} \rightarrow \mu^{-} + N + \pi^{+}
\end{eqnarray}
In the scenario described above, the exited nucleon \textit{N} decays and emits a charged pion. Other end states yielding either charged or neutral pions are possible as well for either CC or NC interactions. An additional possibility is coherent scattering with the entirety of the nucleus. The momentum transfer in this case is rather low and results in little nuclear recoil and a heavily forward-scattered pion.

As the momentum transfer in the interaction increases, more exotic resonances can arise which in turn can result in the production of multiple pions or heavier mesons. The large number of possible end states for these interactions makes proper simulation of the process complicated. This can be significant for specialized analyses focusing on the lowest energy events in DeepCore as neutrinos interacting via resonance production actually make a sizable contribution to the event sample. What is ultimately important, however, is that the total light yield from these interactions is estimated correctly by the simulation used for analysis purposes. While elastic scattering and resonance production are significant for the lowest energy analyses or possible future low energy extensions for IceCube, these processes only add a minor contribution to the event sample used in the analysis presented in this thesis.

\subsection{Deep Inelastic Scattering}
The most important neutrino-matter interaction for IceCube is that of deep inelastic scattering. As Figure \ref{fig:neutrino_scattering} shows, it is far and away the dominant mode of interaction for neutrinos above the threshold energy for detection in IceCube ($\sim$100 GeV). At these energies, the wavelength of the incoming neutrino becomes sufficiently small enough to begin probing the internal parton structure of the target proton or neutron. The energy imparted to these internal components will break the nucleon apart. This breakup will result in the formation of a shower of short-lived hadrons (a particle composed of quarks) which will decay into more stable particles. The generation of these hadronic cascades during nucleon breakup is often referred to as hadronization of the nucleon's constituent quarks, and the formation of these particles is necessitated by quark confinement, a property of the strong nuclear interaction. The particles within this shower will suffer energy losses through either collisions or ionization of surrounding material until the energy is fully dissipated. A diagram detailing the deep inelastic scattering process for a muon neutrino is shown in Figure \ref{fig:dis_scattering}.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.7\textwidth,keepaspectratio]{dis.png}
  \end{center}
  \caption{Feynman diagram of a $\nu_{\mu}$ undergoing deep inelastic scattering with a nucleon via charged-current interaction. A large momentum exchange through a charged $W$ boson leads to breakup of the nucleon \cite{2012RvMP...84.1307F}.}
  \label{fig:dis_scattering}
\end{figure}

Like the scattering modes mentioned earlier, deep inelastic scattering can occur through either a CC or NC interaction. The hadronic shower will occur regardless of the interaction mode, while an end-state lepton will only be present in the CC scenario. Neutrino experiments can infer the properties of the interacting neutrino through examination of the end state products. The three possible leptons (e$^-$, $\mu$, $\tau$) produced in CC interactions each exhibit different behavior as they interact in the detection medium allowing for flavor identification of the original neutrino. In the case of NC scattering, however, only the hadronic cascade is observable by the experiment. This not only prevents flavor identification of the neutrino, but it also makes estimation of the neutrino energy very difficult as some fraction of the total energy is carried away by an outgoing neutrino.

\begin{figure}[ht]
\centering
\begin{minipage}[b]{0.45\linewidth}
\includegraphics[width=0.95\textwidth]{DIS_CS_Neutrino.png}
\caption{Charged current (thin), neutral current (dashed), and total inclusive (solid) deep inelastic scattering cross-section by energy for neutrinos \cite{Gandhi:1998ri}.}
\label{fig:NeutrinoDIS_CS}
\end{minipage}
\quad
\begin{minipage}[b]{0.45\linewidth}

\includegraphics[width=1.0\textwidth]{DIS_CS_AntiNeutrino.png}

\caption{Charged current (thin), neutral current (dashed), and total inclusive (solid) deep inelastic scattering cross-section by energy for anti-neutrinos \cite{Gandhi:1998ri}.}
\label{fig:AntiNeutrinoDIS_CS}
\end{minipage}
\end{figure}

The cross-section for deep inelastic scattering increases with increasing neutrino energy as Figures \ref{fig:NeutrinoDIS_CS} and \ref{fig:AntiNeutrinoDIS_CS} show. While this increasing cross-section is generally favorable for detection purposes due to higher probability to interact in neutrino telescopes, a larger interaction cross-section at higher energies also leads to absorption of neutrinos within the Earth. Absorption is fairly infrequent for neutrinos below 10 TeV, but it becomes increasingly relevant for astrophysical neutrinos at the PeV or EeV scale \cite{2013arXiv1304.4891K}.

\subsection{Propagation of Interaction Products}
The products leftover from high-energy neutrino-nucleon interactions are generally very energetic as well. These particles will subsequently undergo many forms of energy loss and possibly decay as they travel through a detection medium. In a given interaction, the total light yield and its spatial extent are contingent on the products of the interaction. The products themselves are ultimately determined by both the flavor of the primary neutrino and whether the neutrino interaction was of the charged- or neutral-current variety. This section will detail the important energy loss mechanisms for interaction products and how these losses produce an observable signal in the IceCube detector. There are many modes of interaction for charged particles in materials some of which are highly energy dependent and stochastic in nature. Because IceCube was designed specifically to detect and track high energy muons, the description of energy loss mechanisms will primarily focus on how these mechanisms pertain to muons. The same interactions will occur for other charged particles produced in neutrino interactions, however, these particles fail under normal circumstances to produce tracks long enough to be resolved by IceCube.

The primary energy loss mechanisms for charged leptons are ionization, bremsstrahlung, pair-production, and photo-nuclear interactions \cite{2001PhRvD..63i4020I}. These losses are described below:

\begin{adjustwidth}{2.5em}{2.5em}
\setlength{\parindent}{0pt}
\textbf{Ionization} - Ionization energy losses occur as the charged particle transfers energy to the electrons of intervening atoms and molecules. This energy loss is fairly continuous and is the dominant loss mode for sub-TeV muons in the ice \cite{2001ADNDT..78..183G}.

\textbf{Bremsstrahlung} - Energy losses via bremsstrahlung involve the deflection of the particle by another charged particle (typically a positively charged nucleus). This deflection results in the emission of a photon with energy equivalent to the kinetic energy lost in the interaction. These photons can often be energetic enough to produce a electromagnetic shower through electron-positron pair-production.

\textbf{Pair-production} - As the name might suggest, pair-production is the process in which an electron-positron pair is produced in the Coulomb field of an atomic nucleus \cite{2001ADNDT..78..183G}. If the energy imparted to the electron-positron pair is large enough, an electromagnetic shower can be produced.

\textbf{Photo-nuclear} - At high energies, charged leptons can undergo inelastic scattering with nuclei and in the process create hadron showers \cite{2003PhRvD..67c4027B}. The process is similar to the neutrino-nucleon scattering processes described earlier but with photon-exchange taking place rather than $W$ or $Z$ boson exchange.
\end{adjustwidth}
\setlength{\parindent}{17.5pt}
The relative contribution to total energy loss these interactions provide will vary with particle energy. In practice, this is commonly parameterized so the average range of the particle in a given medium can be more easily computed \cite{2001ADNDT..78..183G}:
\begin{equation}\label{eq:stopping_power}
-\frac{dE}{dX} = a(E) + b(E) E
\end{equation}
Here $a(E)$ represents the more or less continuous energy losses from ionization (electron stopping power) while the $b(E)$ term includes the contributions from the various radiative energy loss processes (bremsstrahlung, pair-production, and photo-nuclear). The relative strength of these processes with respect to stopping power can be seen in Figure \ref{fig:MuonStoppingPower}. By integrating the stopping power from Eq. \ref{eq:stopping_power}, we arrive at the following expression for particle range
\begin{equation}\label{eq:range}
R(E) = \int_{E_0}^{E}[a(E') + b(E')E']dE'
\end{equation}
with $E_0$ being the final energy of the particle \cite{2001ADNDT..78..183G}.
With a few approximations Eq. \ref{eq:range} can be simplified into a simple expression yielding particle range from just the initial energy \cite{2001ADNDT..78..183G}:
\begin{equation}\label{eq:range_approx}
R(E) \approx (1/b)ln(1+\frac{E}{E_{c}})
\end{equation}
The term $E_{c}$ or critical energy is given by $E_{c}=\frac{a}{b}$ and it is the energy at which radiative and ionization energy losses become equivalent. The above approximation for particle range can be obtained from the assumption that $E$ is very large and that energy losses are continuous. These assumptions must neglect the stochastic nature of many of these energy losses and therefore any range derived this way is more representative of an average range and individual particles are subject to large fluctuations.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.9\textwidth,keepaspectratio]{MuonStoppingPower.png}
  \end{center}
  \caption{Total stopping power (solid line) of muons in copper as a function of kinetic energy. Contributions from individual processes are shown as well as the energy ranges in which these processes are most dominant. \cite{2001ADNDT..78..183G}.}
  \label{fig:MuonStoppingPower}
\end{figure}

The range equation from Eq. \ref{eq:range_approx} can be used to estimate the range of an energetic muon in IceCube. Using a muon critical energy value of $E_{c}=675$ GeV in water and a value of $b=2.959 \cdot 10^{-6}$ g$^{-1}$cm$^{2}$ for radiative losses, we find a range of approximately 3 km for a 1 TeV muon (values for $E_{c}$ and $b$ obtained from \cite{2001ADNDT..78..183G}). The most important thing to note is that these losses determine both the brightness and length of muon tracks within the IceCube detector and ultimately how well their direction can be resolved. The physical dimensions of the IceCube detector were designed with the range of these muon tracks in mind. Interestingly, the effective area of the IceCube detector is greatly enhanced due to the ability of energetic muons to penetrate so deeply in matter.

\subsection{Cherenkov Emission}
While the previously described processes are most relevant for particle energy loss, the most import energy loss mechanism for particle detection in IceCube is the production of Cherenkov light. Cherenkov emission only makes a small contribution to total energy loss by charged particles in materials. It does however produce light in the visible spectrum yielding an easily detectable signal. This emission occurs when a charged particle travels through a dielectric medium at a greater speed than the phase velocity of light in that medium. The particle will polarize the surrounding material as it travels, however the particle will have moved on before enough time has elapsed for the material to relax resulting in a build up of wave fronts. A shock front is produced that takes the shape of a cone about the particle track. This light cone has an opening angle given by
\begin{equation}
\cos{\theta}=\frac{1}{n\beta}
\end{equation}
where $n$ is the index of refraction for the material and $\beta$ is the ratio of the particle velocity and the speed of light $c$. A diagram of Cherenkov light cone produced by a muon is shown in \ref{fig:CherenkovDiagram}. If one takes a typical value for the refractive index in ice ($n=1.31$), an angle of roughly $40^{\circ}$ is obtained. Resolution of the Cherenkov cone of a single particle is a powerful identification technique for many particle detectors. However, the Cherenkov light fronts of only the most energetic and long-lived particle tracks are able to be resolved in IceCube due to the large separation distances between individual light sensors.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.65\textwidth,keepaspectratio]{CherenkovCartoon.png}
  \end{center}
  \caption{Diagram of a Cherenkov light cone produced by a muon traveling at speed $v=\beta c$ through a transparent medium with index of refraction $n$. The blue circles show points of light emission along the particle track. Since the muon travels faster than light in this medium, the light emission forms a shock-front analogous to a sonic boom.}
  \label{fig:CherenkovDiagram}
\end{figure}

While they may lack the long propagation lengths of muons, all other charged particles produced during neutrino-nucleon scattering interactions in the ice will emit Cherenkov radiation as they propagate in the ice. This will generate a pool of light centered at the interaction vertex whose photon count is proportional to the energy of the interaction. Light produced in these particle showers and muon tracks is collected by IceCube via sensitive optical sensors located in the ice. The high scattering and absorption lengths of the glacial ice allow for accurate timing of neutrino interaction times and arrival directions. There are several modes of scattering of photons that can occur, however, which can result in degradation of Cherenkov signal \cite{2013arXiv1301.5361I}. Detailed information on detector instrumentation, sensor response to the Cherenkov emission, and the topology of neutrino events in IceCube will be given in Chapters V and VI.


\section{Flavor Oscillations}
The process in which the lepton flavor of a given neutrino can change during its propagation is one of the particle's more intriguing properties. The unexpected observation of neutrino oscillations indicated that the neutrino, which was initially thought to be massless under the Standard Model, must actually have some non-zero mass after all. Recently, many experiments have studied this phenomenon with high precision in hopes of accurately determining the mass-splitting of the neutrino eigenstates as well as if neutrinos undergo CP violation. While the analysis being presented is in no way suited to studying the oscillation phenomenon, how the results are interpreted is very much dependent on how the modeled neutrino flavor ratio for a source evolves from its origin to its arrival at Earth. Thus, this section will describe in short detail the oscillation process over astronomical baselines and through the Earth, the scenarios most applicable to IceCube.

The mixing of neutrino states arises due to the fact that the neutrino mass eigenstates do not directly correspond to the neutrino flavor eigenstates \cite{1978PhRvD..17.2369W}. While neutrinos are produced through weak interactions giving them a definite lepton flavor state, the mass of that neutrino is actually a superposition of three separate mass states. Likewise, the mass state of the neutrino is given by a superposition of the three lepton states. The phase of the mass states will evolve differently during propagation according to the mass differences between the mass eigenstates. Thus, when the neutrino interacts with matter again via the weak force it may be observed in a flavor state other than the its state during production. This mixing possibility can be described by a mixing matrix $U$:
\begin{equation}
\nu_{\alpha} = \sum_{i=1}^{3} U_{\alpha i} \nu_{i}
\end{equation}
where $\nu_{\alpha}$ are the lepton states ($e^-,\mu,\tau$) and $\nu_i$ are the mass states (1,2,3). Using the notation of Nunokawa et. al \cite{2008PrPNP..60..338N}, the matrix $U$ has the form
\begin{equation}
U = \left(\begin{array}{ccc}
1 & 0 & 0 \\
0 & c_{23} & s_{23} \\
0 & -s_{23} & c_{23}
\end{array}\right)
\left(\begin{array}{ccc}
c_{13} & 0 & s_{13}e^{i\delta} \\
0 & 1 & 0 \\
-s_{13}e^{i\delta} & 0 & c_{13}
\end{array}\right)
\left(\begin{array}{ccc}
c_{12} & s_{12} & 0 \\
-s_{12} & c_{12} & 0 \\
0 & 0 & 1
\end{array}\right)
\end{equation}
\begin{equation}
=
\left(\begin{array}{ccc}
c_{12}c_{13} & s_{12}c_{13} & s_{13}e^{i\delta} \\
-s_{12}c_{23}-c_{12}s_{23}s_{13}e^{i\delta} & c_{12}c_{23}-s_{12}s_{23}s_{13}e^{i\delta} & s_{23}c_{13}\\
s_{12}s_{23}-c_{12}c_{23}s_{13}e^{i\delta} & -c_{12}s_{23}-s_{12}c_{23}s_{13}e^{i\delta} & c_{23}c_{13} \\
\end{array}\right)
\end{equation}
In the above, $c_{ij}$ and $s_{ij}$ are shorthand notation for cos $ \theta_{ij}$ and sin $\theta_{ij}$ respectively while $\delta$ is a phase term related to charge-parity (CP) violation. By taking the inner product of the two states after application of the mixing matrix one arrives at Eq. \ref{eq:oscprob} which gives the probability of transition of a neutrino of flavor $\alpha$ to one of flavor $\beta$ during propagation \cite{2008PrPNP..60..338N}.
\begin{equation}\label{eq:oscprob}
P(\nu_{\alpha}\rightarrow \nu_{\beta}) = \left| \sum_{i} U_{\alpha i}^* U_{\beta i} e^{-i\frac{m_{i}^2}{2E}L}\right| ^2
\end{equation}

The full-fledged oscillation probability is complicated, but the most important thing to takeaway from the expression given by Eq. \ref{eq:oscprob} is that the probability of observing the neutrino in a different state depends on the propagation length $L$ and the energy of the particle $E$. In many cases it is possible to only consider a two-flavor oscillation scenario as the third flavor may not participate strongly in the energies and distances being considered. This allows us to reduce the mixing matrix to a two dimensional form making the expression for oscillation probability much simpler \cite{1998PhRvL..81.1562F}:
\begin{equation}\label{eq:2doscprob}
P(\nu_{\alpha}\rightarrow \nu_{\beta}) = sin^22\theta sin^2\left(\frac{1.27\Delta m^2 (eV^2) L (km)}{E_\nu (GeV)}\right)
\end{equation}
This formulation will yield the probability of oscillation provided you know the mixing angle $\theta$ between $\alpha$ and $\beta$ and the mass-squared difference $\Delta m^2$ for the neutrino mass eigenstates. This approximation is commonly used by several experiments (e.g. IceCube \cite{2013PhRvL.111h1801A}) to make measurements on specific mixing angles. Using an accelerator produced neutrino beam with known energy, neutrino detectors can be placed long distances from the source where the oscillation probability given by Eq. \ref{eq:2doscprob} is maximized. These long-baseline experiments provide the most precise measurements of neutrino oscillation parameters. The current best-fit values for these parameters are given in Table \ref{tab:osc_param} \cite{PhysRevD.89.093018}.
\begin{table}[h]
\caption[Best Fit Neutrino Oscillation Parameters]{Best fit values for oscillation mixing angles $\theta_{ij}$ and squared mass differences $\Delta m^{2}$. These values are for the normal mass hierarchy scenario wherein $m_1 < m_2 , m_3$. We define $\Delta m^{2}$ here as $m_{3}^2 - (m_1^2 + m_2^2)/2$ \cite{PhysRevD.89.093018}.\label{tab:osc_param}}
\begin{center}
\begin{tabular}{cc}
\toprule
\textbf{Parameter} &\textbf{ Best Fit}\\
\midrule
sin$^2 \theta_{12}$ & 0.308\\
sin$^2 \theta_{13}$ & 0.0234\\
sin$^2 \theta_{23}$ & 0.437\\
$\delta / \pi$ & 1.39 \\
$\Delta m^2$ & 2.43$\cdot 10^{-3}$ eV\\
\hline
\end{tabular}
\end{center}
\end{table}

The IceCube detector is not uniformly sensitive to all neutrino types. Therefore, it is important to estimate how the flavor ratio of a neutrino flux changes source during its propagation from an astrophysical source to Earth. The method of generation of a neutrino flux at the source will determine the initial flavor flux ratio. The flavor ratio for a given flux is usually given as a relative ratio of the flux of one flavor to the whole, e.g. (1:2:1) with the numbers in the parenthetical relating to the flavors as ($\nu_e$:$\nu_{\mu}$:$\nu_{\tau}$). Some common neutrino production scenarios include muon-suppressed pion decay, pion and muon deacy, and neutron decay which have flavor flux ratios of (0:1:0), (1:2:0), and (1:0:0) respectively. These production mechanisms will be described in more detail in Chapters 3 and 4, but for now we are just interested in how these flavor ratios evolve during propagation to Earth. 

The effect is visualized in Figure \ref{fig:baseline_osc}. Here we see that the initial ratio of (0:1:0) from muon-suppresed pion decay ultimate arrives at Earth with a ratio of approximately (1:2:2). Three-flavor oscillations tend to average out the ratio between the flavors, but there is still a distinct difference at Earth for the differing production mechanisms. When placing limits on the neutrino emission from a possible source, the choice of flavor ratio at Earth will influence the degree to which a flux from that source can be excluded. In the event that astrophysical neutrino sources begin to be resolved, high statistics measurement of their flavor ratios should be able to identify what mode of neutrino production is at work at the source.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.75\textwidth,keepaspectratio]{AstroBaselineOsc.png}
  \end{center}
  \caption{Neutrino flavor ratio space. Oscillation over astronomical baselines will result in a flavor ratio at Earth different than at source. If the flavor ratio for a given source can be measured it can help identify the process through which that source's neutrinos were produced \cite{2014arXiv1412.5106I}.}
  \label{fig:baseline_osc}
\end{figure}

\chapter{Neutrino Astronomy}
As the name suggests, the field of neutrino astronomy is concerned with the detection of neutrinos of extraterrestrial origin for the use of identifying and classifying astrophysical sources. In a similar fashion to the advances made through expansion of traditional optical astronomy to the high-energy regime of x-rays and gamma-rays, it is thought that the unique window provided by neutrino astronomy can facilitate tremendous increases in our understanding of the universe. However, the detection of astrophysical neutrino events presents a exceedingly difficult challenge with detection of an appreciable number of neutrinos from a single source requiring enormous instrumented volumes. Therefore, it is relevant to discuss the potential scientific rewards neutrino astronomy can provide.

\section{Motivation}
Neutrinos provide a novel and unexplored view into furthest reaches of the universe. The small cross-section for neutrino interactions in matter is somewhat of a double-edged sword. While it makes the detection of neutrinos immensely difficult, it also means that neutrinos will stream unperturbed through even the densest astrophysical media. The cross-section for interaction is still very low at even the highest energies. This allows neutrinos to provide information about processes at energies where other possible messengers, such as cosmic rays or photons, are attenuated significantly over astrophysical distances. The development of neutrino astronomy would therefore complement other high-energy astronomy methods quite nicely creating a more complete picture of the high-energy universe.

%% Props


The detection of a neutrino flux from a source and measurement of its spectrum would provide critical insight into the processes governing the densest and most energetic regions of that object.

\section{Multi-Messenger Astronomy}
The complex phenomenology of high-energy astrophysics offers us many possible signals to use as potential cosmic messengers. The merits and pitfalls of neutrino astronomy have been discussed at length already, but there are many other kinds of emission available as well.

 Each of these messengers carries its own advantages and disadvantages. They all give a unique view into the high-energy universe, though, which makes coordination of efforts in all of these fields essential. The scientific output for each of these fields is greatly enhanced by development of the others. In this sense, the maturation of neutrino astronomy will not only yield interesting insights on its own, but it will also contribute greatly to the advancement of physics knowledge from gamma ray astronomy, cosmic ray studies, and gravitational wave detection.
\section{Cosmic Ray - Neutrino Connection}
The Earth's atmosphere is constantly bombarded with a flux of high-energy charged particles known as comsic rays. Discovered in 19XX by Victor Hess, this flux of extraterrestrial radiation is composed of protons and ionized nuclei.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.85\textwidth,keepaspectratio]{CosmicRaySpectrum.jpg}
  \end{center}
  \caption{Cosmic ray energy spectrum at Earth as measured by several experiments \cite{2009PrPNP..63..293B}.}
  \label{fig:cosmicray_spec}
\end{figure}

\subsection{Proton Acceleration}

\subsection{Neutrino Production}
Sources that accelerate high-energy protons are also capable of producing a corresponding flux of neutrinos as well. 

\section{Detection Methods}

The primary method for the detection of high-energy neutrinos is through observation of Cherenkov light produced by interaction secondaries in a transparent medium (typically water or ice). Events can be detected either by surrounding a detection volume with light detectors (as is the case for Super-Kamiokande \cite{2003NIMPA.501..418F}) or by placing the detectors within the detection medium itself (e.g. IceCube, ANTARES \cite{2011NIMPA.656...11A}). The Cherekov light emitted by charged particles in the detection medium is collected by the detectors. The time of arrival of photons at different sensors can then be used to reconstruct events to determine the direction from  which their parent neutrino primary arrived. In order to be sensitive to expected astrophysical fluxes, large instrumented volumes are required. The largest detectors therefore make use of natural media (ice cap, oceans, etc.) to avoid prohibitive instrumentation costs.

Detection with air Cherenkov telescopes is also a possible detection technique for extremely high energy tau neutrino events (10$^{15}$-10$^{18}$ eV) \cite{2013APh....41....7A}. The typical source for IACTs (Imaging Atmospheric Cherenkov Telescope) are particle showers produced via gamma-ray interactions in the atmosphere. These electromagnetic showers produce a pool of Cherenkov light at the ground level which can be used to reconstruct the energy and direction of the gamma ray primary. This method can also be applied to particle showers produced during the decay of long-lived $\tau$ leptons produced in charged current interactions of UHE $\nu_{\tau}$ in the atmosphere. The shape of the Cherenkov light pool in these interactions will be qualitatively different from the light generated via cosmic ray or gamma ray showers allowing these events to be readily identified. This method utilizes the atmosphere above the telescope as its detection volume. Such a large volume is required to potentially observe these rare ``earth-skimming" neutrino events.

Ultra high-energy neutrinos (10$^{18}$-10$^{21}$) can also be detected through the coherent Cherenkov radio emission produced by interactions in media. The ANITA experiment monitors for this radio emission during surveys of the entire Antarctic icecap to search for ultra high-energy GZK neutrino events \cite{2010PhRvD..82b2004G}. 
\chapter{Neutrino Sources}
While no individual extra-solar high-energy neutrino sources have yet to be discovered (with the exception of SN 1987A), there is strong theoretical support  and indirect observational evidence for their existence. The measurement of extremely high-energy cosmic rays implies that a flux of energetic neutrinos must also be present due to the intimate link between hadronic acceleration and neutrino production in high-energy astrophysical environments. Most importantly, the recent discovery of a flux of high-energy neutrinos of astrophysical origin by IceCube \cite{2013Sci...342E...1I} provides confirmation that there must be some sources of high-energy neutrinos though they may be very diffuse.

The energy range of the event selection for this analysis (10-10$^3$ GeV) severely limits the ability to resolve steady neutrino sources because of the high rate of background atmospheric neutrino events. This motivates the use of time-dependent search methods, and as such this section will only consider transient sources of high-energy neutrino emission. Transient sources of emission generally arise from cataclysmic astrophysical events such as massive compact object mergers, supernovae, or gamma-ray bursts. All of these events represent the release of massive amounts of gravitational energy over short time scales. The predicted neutrino emission from some these sources will be discussed in this section. The detection prospects for these sources in the scope of this analysis will be discussed as well.

\section{Active Galactic Nuclei}
The creation of UHECRs requires immensely energetic astrophysical systems capable of accelerating particles to very high energies prior to escape. Most massive galaxies are thought to host a supermassive black hole ($M_{BH} \geq 10^{6}$ $M_{\odot}$) at their center. Periods of intense accretion of matter onto these blacks holes can power relativistic jets of material perpendicular to the accretion disk. Variability in the jets can result in the formation of relativisitic shocks which can accelerate charged particles to extremely high energies, and it is also possible for shocks located within the inner regions of accretion disk to accelerate these protons as well \cite{1996SSRv...75..341S}. Due to the massive release of gravitational energy during accretion, active galactic nuclei (AGN) have been proposed as a possible source for the highest energy cosmic rays \cite{1996SSRv...75..341S}.

Although it is possible for AGN to serve as steady sources of high-energy neutrinos, in the context of the presented analysis we are primarily interested in the possibility of flaring events from AGN in which there is a strong enhancement of neutrino production over short timescales.

\section{Core-Collapse Supernovae}
The lifespan and manner of death for a given star is primarily determined by its mass. Sufficiently massive stars will often end violently in the form of a supernova. The interiors of stars formed with masses greater than 8 $M_{\odot}$ will form dense cores consisting of nuclei with high atomic number such as O,Ne,Mg, and Fe \cite{2003astro.ph..1006H}. 

The detection of several neutrino events in temporal coincidence with supernova 1987A marked the first detection of an extra-solar neutrino source. Neutrino events were observed in three separate detectors a few hours prior to the optical observation. Such an observation was possible due to the close proximity of the progenitor which was located within the Large Magellanic Cloud ($\sim$50 kpc from Earth). Detection of these events confirmed the theoretical prediction of an enormous liberation of energy in the form of neutrinos during the collapse of the core of a massive star.


%% neutrino production from neutron conversion, SN1987A
%% Too low energy in IceCube, but SNDAQ
\begin{wrapfigure}{r}{0.5\textwidth}
  \begin{center}
    \includegraphics[width=0.48\textwidth,keepaspectratio]{NearbySNCatalogue.png}
  \end{center}
  \caption{Observed SNe within 10 Mpc in the years 1999-2008 \cite{2011PhRvD..83l3008K}}
  \label{fig:local_ccsne}
\end{wrapfigure}


\section{Gamma-ray Bursts}
Gamma-ray Bursts (GRBs) are characterized by extraordinarily bright emission of gamma rays over timescales ranging from 0.1-10$^3$ s. The isotropic distribution of detected bursts suggest that they occur at cosmological distances and are not of galactic origin. If one assumes that the emission from these sources is isotropic, then the luminosity required for the observed fluxes at Earth is on the order 10$^{52}$ erg$\cdot$s$^{-1}$ \cite{2004RvMP...76.1143P}. This enormous energy output suggests that GRB emission must be significantly beamed with Earth lying in the line of sight. The large energy budget of GRBs also suggests that they may be possible candidates for the sources of the highest energy cosmic rays. If this is indeed the case, then it stands to reason that high-energy neutrino emission could also be present during GRB events.

This possibility has been investigated with many configurations of the IceCube detector, and there has yet to be a confirmed significant detection of neutrino events in correlation with detected GRBs \cite{2011PhRvL.106n1101A}, \cite{2012Natur.484..351I}.
\subsection{Fireball Model}
%% Fireball model, neutrino production in jets


\subsection{Subphotospheric Model}
%% subphotospheric model

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.85\textwidth,keepaspectratio]{SubPhotoFluence.png}
  \end{center}
  \caption{Energy fluence of $\nu_{\mu}$ and $\bar{\nu}_{\nu}$ from high-luminosity GRB at a redshift of z=0.1 in the sub-photospheric emission model \cite{2013PhRvL.111m1102M}.}
  \label{fig:subphotospheric_nus}
\end{figure}


\section{Choked Gamma-ray Bursts}
The core-collapse of a massive star is thought to be the mostly likely engine for powering the jets responsible for gamma-ray emission from long duration GRBs \cite{2004RvMP...76.1143P}. Proposed models for GRBs posit that gamma ray emission is produced via Fermi-acceleration of either electrons or protons in relativistic shocks within the energetic jets \cite{2004IJMPA..19.2385Z}. Whether these jets are prevalent in a large fraction of core-collapse supernovae is unkown, but it does suggest that a correlation between long duration GRBs and supernovae should exist. In fact, some instances of GRB association with SNe have been observed \cite{2006ARA&A..44..507W}, \cite{2011AN....332..434M} \cite{2003astro.ph..1006H}. However, only a small fraction ($\leq 10^{-3}$) of observed supernovae are known to be associated with GRBs \cite{2003ApJ...599..408B}. 

This has led some to speculate that jet production during core-collapse might occur in a higher fraction of SNe than the observed GRB-SNe fraction would indicate. Rather than achieveing breakout as is the case in the GRB scenario, the jets of these SNe may never breach the surrounding stellar envelope and will be effectively `choked' off. This may be due to either the jets being insufficiently energetic or the surrounding envelope of material being far more massive than what is typical in GRB progenitors. However, if these jets produce neutrinos in a similar manner as the jets of GRBs, it may be possible to observe such a source through its neutrino emission despite the fact that the optical signature remains hidden. In this scenario, a continuum class of objects would exist in which GRBs represent CC-SNe harboring the most energetic of jets.

The observed frequency of CC-SNe in the universe is much higher than that of GRBs \cite{0004-637X-738-2-154}, \cite{2004RvMP...76.1143P}. Thus, it is possible that choked GRBs may occur at a much higher rate than GRBs in the nearby universe making choked GRBs a prime target for time-dependent neutrino analyses. Of course this would require that the neutrino production in the soft jets of choked GRBs is capable of producing neutrinos of energies detectable in IceCube and DeepCore.

\subsection{Neutrino Emission Model}

\begin{equation}
\frac{d\Phi_\nu}{dE}=F_\nu\left\{\begin{array}{cc}
E^{-2} & E > E_{\nu}^{(1)} \\ 
E_{\nu}^{(1)}E^{-3} & E_{\nu}^{(1)}< E < E_{\nu}^{(2)} \\ 
E_{\nu}^{(1)}E_{\nu}^{(2)}E^{-4} & E_{\nu}^{(2)}< E < E_{max}
\end{array}\right.
\end{equation}


%% Flux plot

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.85\textwidth,keepaspectratio]{FluxPlot_Canonical_RespectiveFluenceNorm.png}
  \end{center}
  \caption{Estimated $E^{-2}$-weighted neutrino flux at Earth for a choked GRB at a reference distance of 10 Mpc with canonical model parameters ($\Gamma_{b}=3$, $E_{jet}=3\cdot 10^{51}$ erg). At lower energies, the flux is dominated by neutrinos of pionic origin (dashed line). Neutrinos produced in kaon decays have a harder spectrum and will dominate at higher energies.}
  \label{fig:ref_chkgrb_flux}
\end{figure}

\subsection{Model Limits on SN2008D}
The X-ray telescope of the \textit{SWIFT} satellite detected a bright flash indicating a transient event during observation of NGC 2770 on January, 9, 2008. Followup observations showed that the \textit{SWIFT} source was a core-collapse supernova of type Ib \cite{2008Natur.453..469S}. At a distance of only 27 Mpc, this supernova provided an opportunity for the IceCube detector to search for any corresponding high-energy neutrino emission. There is no clear evidence, however, that SN2008D had an aspherical explosion suggesting the production of energetic jets similar to those described by the RMW/AB model \cite{2008Natur.453..469S}. Nonetheless, a search for high-energy neutrino emission was carried out using the 22-string partial configuration of IceCube \cite{2011A&A...527A..28I}.


%% Add SN2008D Gamma and Ejet Limit plot

\chapter{Detector}
\section{IceCube and IceTop}
The IceCube Neutrino Observatory is a km$^{3}$-scale neutrino detector located deep within the glacial ice of the Antarctic ice sheet at the geographical South Pole. This location provides IceCube with a pristine detection medium in addition to mechanical support for the entirety of the array. The detector consists of 5,160 light sensors known as digital optical modules (DOMs) which are distributed along 86 cables (referred to as strings) that supply power and provide communication to the surface. Each cable is instrumented with 60 DOMs spaced 17 meters apart starting at 1,450 meters below the surface and terminating at 2,450 meters below. An inter-string spacing of 125 meters on average results in a total instrumented volume of approximately 1 km$^{3}$. Figure \ref{fig:icecube} provides a schematic illustrating the detector geometry.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=1.0\textwidth,keepaspectratio]{ArrayWSeasonsLabels.pdf}
  \end{center}
  \caption{Diagram of the IceCube Neutrino Observatory (Courtesy of the IceCube Collaboration).}
  \label{fig:icecube}
\end{figure}

Installation of the IceCube strings took place over several years and required the use of a specialized hot-water drill. In the deployment process, the hot-water drill is used to bore through the ice leaving a water-filled column in which the string and its attached DOMs are lowered. The water column subsequently freezes the cable and all DOMs in place rendering them completely inaccessible from the surface. The deployment of the first IceCube string occurred on January 29th, 2005. The remaining strings were deployed over the next five summer seasons resulting in data seasons of different detector shapes and size. The final string was deployed on December 18, 2010 giving IceCube its ultimate 86-string configuration.

%%% IceTop

In addition to the detectors installed deep in the ice, there are also 81 detector stations (each station consisting of two tanks) at the surface. These tanks, which utilize two of the same light-sensing DOMs as IceCube, comprise the IceTop surface array. The DOMs in these tanks, which are also frozen in place, look for Cherenkov radiation produced by cosmic ray air shower secondaries in the tank ice. By examining the arrival time of charged particles from the shower front, the direction of cosmic rays incident at Earth can be determined. The spatial extent of the shower as well as the total charge deposition in the tank PMTs allow for accurate estimation of the energy of the primary cosmic ray. Data produced from IceTop is used to study cosmic ray composition, spectra, and anisotropy.

Due to the spatial relation of both IceTop and IceCube, they are able to complement the capabilities of each other quite nicely. IceTop's primary purpose is to study air shower physics, but it also serves as a veto for downgoing atmospheric muons and neutrinos in IceCube. This is particularly useful in the search for highly energetic neutrinos of astrophysical origin such as the events reported in \cite{2013Sci...342E...1I} and \cite{2014PhRvL.113j1101A}. Any downgoing event found by these searches that is accompanied by a causually connected air shower signal in IceTop is immediately identified as atmospheric in origin. Alternatively, the background muons detected in IceCube can be used for more detailed study of air-shower composition and energy in IceTop analyses. For more detailed information on the physics goals and detection capabilities of IceTop, see \cite{2013NIMPA.700..188A}.


\section{DeepCore}

DeepCore \cite{2012APh....35..615A} is a sub-detector deployed in tandem with IceCube between 2009 and 2010 that was primarily designed to lower the energy threshold of IceCube. The array consists of eight infill strings located in the center of the IceCube detector in addition to twelve central standard IceCube strings. This configuration gives DeepCore three surrounding layers of IceCube strings to use as an active veto for the primary background of atmospheric muons (see \ref{fig:DeepCoreSchematic}). In order to improve detector response to lower energy neutrinos, $\mathcal{O}$(10-100 GeV), the infill strings of DeepCore have a much closer inter-string separation of 42 m and have 50 DOMs spaced 7 m apart deployed deep in the ice between 2,100 m and 2,450 m. This denser instrumentation allows for better timing and spatial resolution of charged secondaries produced in neutrino interactions. Additional sensitivity to lower energies is gained through the use of the newer Hamamatsu R7081MOD model PMT in the infill string DOMs as opposed to the standard Hamamatsu R7081-02 used in IceCube. This model boasts higher quantum-efficiency in the photocathode for photons at typical Cherenkov wavelengths ($\lambda \sim 400$ nm). In-ice measurements of the high quantum-efficiency (HQE) DOMs showed a 35$\%$ increase in sensitivity to Cherenkov light with respect to the standard IceCube DOMs \cite{2012APh....35..615A}.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.98\textwidth,keepaspectratio]{BoreholeLaserDustLogging.png}
  \end{center}
  \caption{Borehole laser measurements of dust concentrations in the ice as a function of depth. Measurements for several IceCube strings are displayed. Higher values on the y-axis denote higher dust concentrations. The "dust layer" features quite prominently at a depth of 2100m \cite{2013JGlac..59.1117.}.}
  \label{fig:DustLogger}
\end{figure}

The depth selected for deployment of the DeepCore DOMs was determined via examination of the ice properties previously mapped by both the Anatarctic Muon and Neutrino Detector Array (AMANDA) \cite{2006JGRD..11113203A} and pre-existing IceCube configurations \cite{2013JGlac..59.1117.}. These investigations into the optical properties of the ice revealed that the deepest ice (depths $\geq$ 2,100 m) had superior optical qualities with respect to the ice closer to the surface. Additionally, it was determined that a layer of high dust concentration in which light is scattered and absorbed to a much higher degree exists at a depth of 2,000-2,100 m (see Figure \ref{fig:DustLogger}). The eight infill strings also have a section of 10 DOMs with 10 m spacing located just above this dust layer. These DOMs form a veto cap to further increase the detection probability and rejection of directly down-going muons. Figure \ref{fig:DeepCoreSchematic} shows the distribution of the DeepCore DOMs and the spacing and orientation of the DeepCore strings with respect to IceCube as a whole.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.7\textwidth,keepaspectratio]{IC86EDC_DeepCoreDiagram.pdf}
  \end{center}
  \caption{Top down and side-view diagram of DeepCore. The side-view shows the difference in DOM distribution for the infill strings and their relation to the dust layer \cite{2012APh....35..615A}.}
  \label{fig:DeepCoreSchematic}
\end{figure}

%%% Physics in DeepCore
The primary physics goal of the DeepCore installation is to provide increased sensitivity for indirect dark matter searches by improving the IceCube detectors ability to resolve sub-100 GeV neutrino events. In this regard, it has been quite successful in establishing limits on the cross-sections of many WIMP (Weakly Interacting Massive Particle) dark matter models with the Sun \cite{2013PhRvL.110m1302A} and Milky Way \cite{2011PhRvD..84b2004A} as possible sources. The lowering of the detector's energy threshold has also made neutrino oscillation parameter measurements possible due to the high statistics provided by atmospheric neutrino events \cite{2013PhRvL.111h1801A}. Most importantly for the analysis presented in this thesis, however, is the improvement in effective area and resolution DeepCore provides for 30-150 GeV muon neutrinos. As this thesis will demonstrate, including these neutrino events into previously established IceCube point source analysis methods greatly improves IceCube's capability to discover transient events with soft spectra.
\section{Neutrino Events in IceCube}

In order to isolate the sparse neutrino events from the abundance of background cosmic ray muons, it is necessary to fully understand the nature of the detector response to neutrinos and neutrino secondaries interacting within the detector. Neutrinos that are sufficiently energetic to be detected by IceCube will undergo deep inelastic scattering with a nucleon target (see for more information on this process see section \textbf{2.1.1}). The nature of the boson exchange will determine if this process is of the charged-current (CC) or neutral-current (NC) variety. The hit topology of a given neutrino event in IceCube will depend upon the flavor of the neutrino ($\nu_{e}$, $\nu_{\mu}$, $\nu_{\tau}$) as well as the channel through which it interacts with a target nucleon in the ice.
\subsection{Cascades}
In NC interactions of all flavors, a hadronic cascade is produced which yields a roughly isotropic distribution of light. Any spatial extent in the hadronic cascade particles will be much smaller than the DOM separation distance. Thus, the Cherenkov emission from these particles will appear to be a point source of light within the detector. This results in a spherical pattern of DOMs that register light from this type of interaction. The radius of DOMs which are able to detect light from the cascade is determined by the total energy deposited in the ice by the neutrino primary. Events with this hit pattern are referred to as cascades.  The spherical hit pattern produced by these types of interaction will be the same regardless of the neutrino flavor. An example event display for this type of interaction can be seen in Figure \ref{fig:cascade}.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=1.0\textwidth,keepaspectratio]{hese_cascade_event.png}
  \end{center}
  \caption{A high-energy cascade event in IceCube with deposited energy of $210\pm^{29.0}_{25.8}$ TeV \cite{2013Sci...342E...1I}. The colored spheres represent DOMs that have registered light during the event. The size of the spheres are indicative of the total light received by the PMT on that DOM. The color denotes the timing of the hit with red corresponding to earlier times and blue corresponding to later times.}
  \label{fig:cascade}
\end{figure}

Whereas the detector response for NC interactions is flavor independent, the event topology in CC interactions is determined primarily by the lepton flavor of the neutrino. In addition to a hadronic cascade, the CC interaction will also yield an energetic lepton corresponding to the flavor of the interacting neutrino. In the case of $\nu_{e}$ and $\nu_{\tau}$ CC interactions, the resulting hit pattern in IceCube will take the form of a cascade in a similar manner to the NC interactions. While the source of Cherenkov emission is no longer point-like, the length of electron and tau particle tracks are much shorter than the inter-DOM separation distance. Some marginal pointing can be achieved for these events, however, since the light produced in the hadronic and electromangetic cascades in these events is not totally symmetric. For sufficiently energetic $\nu_{\tau}$ events in IceCube, more exotic signatures are possible. These arise from the increased lifetime of the outgoing $\tau$ lepton resulting in two separate light-producing cascades that can be resolved separately either in space or time. As of the writing of this thesis, no events of this type have been observed in IceCube. While the pointing provided by cascade-like events is rather poor, the energy of events of this type that are fully contained in the detector can be reconstructed with good accuracy. 

\subsection{Muon Tracks}
IceCube is designed specifically to be sensitive $\nu_{\mu}$ CC interactions due to superior pointing provided by long-lived muon tracks in the ice. Daughter muons from $\nu_{\mu}$ CC interactions can travel distances ranging from ~300 m ($E_{\nu_{\mu}}\sim 100$ GeV) to several kilometers ($E_{\nu_{\mu}}\geq 1$ TeV) \cite{2001PhRvD..63i4020I}. As these muons travel through the ice, they produce light in electromagnetic showers through both ionization and stochastic radiation losses. Because the muon is traveling faster than the speed of light in the ice (index of refraction $n_{ice} \sim 1.3$), the Cherenkov light generated about the muon track will form a cone which is ultimately aligned with the original neutrino direction. This results in a linear hit pattern in IceCube DOMs, providing a clear signal with good directional information. Muon tracks with the highest contained length in the detector provide the best resolution due to their long lever arm and low kinematic angular difference with respect to the parent neutrino. This allows muon tracks to serve as the primary event type for astronomical purposes. An example of a high-energy track event is shown in Figure \ref{fig:track}.

%% Add muon kinematic angle
\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=1.0\textwidth,keepaspectratio]{hese_track_event.png}
  \end{center}
  \caption{A high-energy track event in IceCube with deposited energy of $71.4 \pm 9.0$ TeV \cite{2013Sci...342E...1I}. The colored spheres represent DOMs that have registered light during the event.}
  \label{fig:track}
\end{figure}


\chapter{Data Acquisition}
Maintaining smooth and efficient data acquisition for a detector consisting of such a large number of sensors presents a formidable challenge. Reconstructing physics events within the detector requires accurate timing of signals received by individual sensors coupled with a high degree of synchronization among all detection elements. In this section, a succinct description of the detection of the light-yield from particle interactions in the ice and the subsequent processing of that data is given. The reader interested in a much more thorough account is encouraged to consult the summary by Abbasi et al. \cite{2009NIMPA.601..294A}. 

\section{The Digital Optical Module}
The essential component of the IceCube detector is the DOM. Each of these sensor units contains a Hamamatsu R7081-02 25 cm photo-multiplier tube (PMT), attached digitizing electronics, and LED flashers all housed within a glass pressure vessel \cite{2006NIMPA.567..214H}. A penetrator cable breaches the pressure vessel to connect the DOM electronics to the supporting string cable enabling DOM-to-DOM as well as DOM-to-surface communications. Figure \ref{fig:domscheme} provides an illustration of the DOM structure and its constituent components while Figure \ref{fig:dompic} gives a picture of a fully assembled DOM in its harness with breakout cable. Absolute quantum-efficiency measurements were made for all DOMs prior to deployment in the ice. In order to estimate how the efficiency might change after the water column in which the DOMs were deployed freezes, studies on the efficiency of DOMs at typical in-ice temperatures were performed in labs at IceCube member institutions. The inclusion of LED flashers at UV wavelength allows the DOM to simulate Cherenkov signals for the purpose of calibrating neighboring DOMs. These LEDs are also used to perform studies on the bulk ice properties near the DOM as well as the optical properties of the ice in the re-frozen water column in which the DOM is located.

\begin{figure}[ht]
\centering
\begin{minipage}[b]{0.45\linewidth}
\includegraphics[width=0.95\textwidth]{DomSchematic.png}
\caption{Schematic detailing DOM structure \cite{2009NIMPA.601..294A}.}
\label{fig:domscheme}
\end{minipage}
\quad
\begin{minipage}[b]{0.45\linewidth}
\begin{center}
\includegraphics[width=0.75\textwidth]{LabDOM.pdf}
\end{center}
\caption{A fully assembled DOM supported by a cable harness.}
\label{fig:dompic}
\end{minipage}
\end{figure}

The operational lifetime of IceCube is tied directly to the survival of the DOMs in the ice. The inability to access these modules necessitated a design with a high probability of survival under intense pressures and cold operating temperatures. The design has so far proven to be quite robust; the DOM survival rate since first installation until the 2013-2014 season is an impressive 98\%. The majority of DOM failures occurred during the freeze-in period of their deployment where the pressures acting on the glass vessel are strongest \cite{2009NIMPA.601..294A}. These failures are likely the result of stress on the breakout cable and its connection. Very few DOMs have suffered from failure of main board electronics components meaning any DOMs that survive through freeze-in will likely have a long lifetime.

\section{Hit Generation}

All data acquisition begins with the registering and processing of photon hits in individual DOMs. The PMT of the DOM is configured so that the photocathode (which converts photons received by the PMT into electrons) is kept grounded while the anode is held at positive high-voltage.  Cherenkov photons from nearby charged secondaries are detected when they intercept the photocathode of the PMT on the underside of the DOM. This generates a small current pulse in the PMT which is subsequently amplified and sent to the main board of the DOM for digitization. After the pulse has been amplified, a local coincidence (LC) signal is sent from the DOM to its nearest and next nearest neighboring DOMs. In the event that a LC signal is then received from one of these neighboring DOMs, it is fed to an ATWD (Analog to Waveform Digitizer) which samples the input pulse 128 times at a rate of 40 Mhz. The capture and digitization process takes 29 $\mu$s, so the main board is equipped with two ATWDs that can run in parallel to minimize the amount of dead time in the DOM \cite{2009NIMPA.601..294A}. If the pulse received from the PMT is longer than the ATWD readout time, an ADC (Analog to Digital Converter) is also present to receive and digitize the signal. Additionally, pulses that fail to trigger LC with other DOMs will still undergo digitization via the ADC rather than the ATWD. The pulses produced by the ADC have much coarser binning in time and therefore poorer resolution on timing of the the pulse.

After digitization the pulses are sent to a FPGA (field-programmable gate array) which handles local coincidence triggering logic, generation of hits, and storing of hit information. This integrated circuit will readout the output from the digitizers and store the information until the hit information is ready to be communicated to the surface. Hits that satisfy the local coincidence criteria are known as HLC (hard local coincidence) while isolated hits that fail to show coincidence in other DOMs are referred to as SLC (soft local coincidence). The designation between HLC and SLC will determine how hits are handled further down the data processing pipeline. While some analyses prefer to work solely in the realm of HLC hit information to minimize the contribution of background effects, many reconstruction algorithms and low-energy analyses will favor the inclusion of SLC hits as they may represent true physics hits from a fainter light source.

Collection of hit information from the DOMs is controlled by DOMHub computers located in the IceCube Laboratory (ICL) at the surface (see Figure \ref{fig:icl}). Each DOMHub machine is responsible for all 60 DOMs on a single IceCube string. The DOMHub computers are equipped with eight DOM readout (DOR) cards each of which is capable of handling communications with up to eight DOMs. The DOR cards signal the run state for the DOM, maintain time synchronization, send software updates, and monitors for any software or hardware failures \cite{2009NIMPA.601..294A}. The entire system is kept synchronized by a master clock updated by GPS and set to UTC. All of these components come together to ensure that all hit times measured by DOMs are reliable without any drift in relative time between separate DOMs or DOMHubs.



\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=1.0\textwidth,keepaspectratio]{ICL.jpg}
  \end{center}
  \caption{The IceCube Laboratory at South Pole. DOMHub computers located within the ICL communicate with all DOMs on a respective string. The cables carrying power and information for each DOM arrive at the ICL and enter the building from either the left or right tower near the ceiling (Photo credit: J. Daughhetee).}
  \label{fig:icl}
\end{figure}

\section{Triggering and Event Building}

The stream of DOM hit information arriving in the ICL must be parsed into physics events before any meaningful data analysis can be performed. During the average snapshot of the detector over a short time period ($\sim 10 \mu$s), there will typically only be DOM hits triggered by noise within the detector. In order to select out only interesting events, the IceCube data acquisition system (DAQ) examines the hit information continuously until a certain hit pattern 'triggers' the system to readout the data and construct a physics event. These triggers generally search for a clustering of events coincident in time that are consistent with a particle track or shower. Due to the diversity of analyses in IceCube, there are several triggers optimized for specific physics events. Only the three triggers taken as input for this analysis will be described, though. They can be summarized as follows:

\begin{adjustwidth}{2.5em}{2.5em}
\setlength{\parindent}{0pt}
\textbf{Simple Majority Trigger (8)} - This is the most commonly used trigger in IceCube analyses. This trigger requires that at least 8 DOMs record an HLC hit within a time window of 5 $\mu$s. When the trigger condition is reached, data during a time window defined by -4$\mu$s to +6$\mu$s with respect to the trigger firing time is readout by the DAQ and recorded as an event.

\textbf{Cylinder Trigger (4)} - Instead of solely using a multiplicity requirement, the cylinder trigger attempts to isolate events that show some clustering in space. It defines a cylinder about a DOM with a height of 75m and a radius of 175m. This cylinder encompasses a vertical section of five DOMs on the central string in addition to the nearest neighboring strings. The trigger condition is satisfied if there are 4 HLC hits within this defined volume in a time span of 1$\mu$s. The DAQ will then readout data from over a time window like that used for SMT8 events (-4$\mu$s,+6$\mu$s).

\textbf{DeepCore Simple Majority Trigger (3)} - This trigger works in a similar fashion to the SMT8 trigger. It has a much lower HLC hit threshold (3), but it only looks for HLC hits in DOMs below the dust layer on strings that comprise DeepCore. Additionaly, the time window is reduced from 5$\mu$s to only 2.5$\mu$s. The readout window for the DAQ is a bit larger than that for SMT8 (-6$\mu$s,6$\mu$s). The high-quantum efficiency of PMTs in DeepCore DOMs combined with the lower HLC hit threshold result in far more noise-induced triggers than SMT8.
\end{adjustwidth}
\setlength{\parindent}{17.5pt}

The total trigger rate for the IceCube detector is about 9 kHz with the SMT8, DCSMT3, and Cylinder Triggers firing at inclusive rates of 2.3 kHz, 280 Hz, and 4 kHz respectively \cite{I3Live}. Many events will satisfy multiple triggers and/or the same trigger multiple times. The DAQ system will merge all concurrent satisfied triggers into a single event physics event which ultimately yields a triggered event rate of about 3 kHz \cite{I3Live}. This trigger rate produces an enormous volume of data of which only a small portion is transferred to the north via satellite. All triggered events are written to tape, however, and this data is eventually transferred from Antarctica to storage at the IceCube data warehouse. These physics events are the input for all IceCube analyses and the beginning of the analysis chain.

\chapter{Event Selection}
A quick comparison between the rate at which atmospheric neutrinos trigger the IceCube and DeepCore detectors ($\sim$ 10 mHz) and the overall event rate ($\sim 3$ kHz) readily shows that the data generated by IceCube is very strongly dominated by background. This background is almost entirely due to energetic muons produced in cosmic ray air showers passing through the detector from above. Due to the large range of physics capabilities of the detector, many different filters exist to reduce the data volume and select out events of interest to specific analyses. Event selection for IceCube analyses generally consists of selecting the appropriate filter(s) for the expected signal followed by application of several iterations of cuts optimized to reduce background to an acceptable level while maintaining efficiency with respect to signal events.

The efficacy of these cuts on real neutrino signal is estimated through the use of large amounts of Monte Carlo (MC) simulation of both neutrinos and background muons. An adapted version of the air-shower simulation code CORSIKA (COsmic Ray SImulations for KAscade) is used to generate datasets of background downgoing  muons produced in cosmic ray air-showers \cite{1998cmcc.book.....H}.  Neutrino event simulation is provided by two different event generators named GENIE and Neutrino Generator. The Neutrino Generator or NUGEN is the primary neutrino event simulation code used in IceCube analyses. The NUGEN code is a version of the ANIS neutrino simulation that is modified specifically with IceCube software \cite{2005CoPhC.172..203G}. This code considers many relevant processes for neutrino propagation and interaction including absorption in the Earth, the rock/ice interface below the detector, and neutral current regeneration. However, NUGEN only considers deep inelastic scattering events in the IceCube detector. Other scattering processes become relevant at energies below $\sim$100 GeV, so at these energies and below the accuracy of cross-sections provided by NUGEN begin to suffer. For this reason we also include neutrino simulation from GENIE (\textbf{G}enerates \textbf{E}vents for \textbf{N}eutrino \textbf{I}nteraction \textbf{E}xperiments) \cite{2010NIMPA.614...87A}. This generator provides more accurate simulation at lower energies due to its consideration of other scattering processes in addition to deep inelastic scattering.

All simulation datasets used during the development of the analysis cuts are listed in Table  \ref{tab:event_sel_sim}. In addition to cut development, GENIE sets 1314, 1414 and NUGEN set 10090 were also used to simulate neutrino signal events for analysis method testing and results calculations. The two neutrino simulation generators are used to predict neutrino event rates for separate energy ranges with GENIE governing predictions between 4-190 GeV and NUGEN simulation covering everything $\geq$190 GeV. The use of different types of neutrino simulation may raise some concern about disagreement in event rate predictions. To ensure a smooth transition at 190 GeV, we normalize the NUGEN simulation so that it is in agreement with GENIE predictions near the transition energy. Background muon simulation is handled primarily by CORSIKA set 9622. Set 9255 provides an additional high-energy component that makes a minor contribution to predicted event rates.
\begin{table}[h]
\caption[Ice Properties Systematic Datasets]{List of simulation sets used in the development of data selection cuts and analysis sensitivity. Background simulation is provided by CORSIKA which simulates the muons produced in cosmic ray air-showers. NUGEN and GENIE neutrino datasets estimate atmospheric neutrino rates as well as the sensitivity of the analysis to neutrino sources. \label{tab:event_sel_sim}}
\begin{center}
\begin{tabular}{ccccc}
\toprule
\textbf{Simulator} & \textbf{Set Number} &\textbf{Type} & \textbf{Spectrum} & \textbf{Energy Range}\\
\midrule
CORSIKA & 9622 & Cosmic Ray Primaries & E$^{-2.6}$ & 600-10$^{5}$ GeV \\
CORSIKA & 9255 & Cosmic Ray Primaries & E$^{-2.0}$ & 10$^{5}$-10$^{11}$ GeV \\
NUGEN & 10090 & $\nu_{\mu}$ + $\bar{\nu_{\mu}}$ & E$^{-2}$ & 10-10$^9$ GeV \\
NUGEN & 10193 & $\nu_{e}$ + $\bar{\nu_{e}}$ & E$^{-2}$ & 10-10$^9$ GeV \\
GENIE & 1314 & $\bar{\nu_{\mu}}$ & E$^{-2.5}$ & 4-190 GeV \\
GENIE & 1414 & $\nu_{\mu}$ & E$^{-2.5}$ & 4-190 GeV \\
GENIE & 1114 & $\bar{\nu_{e}}$ & E$^{-2.5}$ & 4-190 GeV \\
GENIE & 1214 & $\nu_{e}$ & E$^{-2.5}$ & 4-190 GeV \\
\hline
\end{tabular}
\end{center}
\end{table}
The lack of a unified neutrino simulation for this analysis poses some minor inconveniences. This issue will likely be addressed in future iterations of this analysis as the collaboration begins production of GENIE simulation sets encompassing much larger energy ranges. The result for this search will depend on this current GENIE simulation, however,  which is known to have some physical inaccuracies. Sets 1314 and 1414 make some approximations in hadronic propagation and light yield that are known to not be correct. These approximations ultimately lead to a slightly higher than expected event rate for neutrino events below 50 GeV. These factors may possibly make a non-negligible difference in the final sensitivity of the analysis.

\section{Low-energy Channel}

Because of the primary focus of this analysis on a lower-energy event selection, the DeepCore-dominated low-energy filter stream is taken as input. Selecting only events which pass this filter reduces the trigger-level data rate of 3 kHz to a much more manageable 37 Hz. The low-energy filter attempts to select a relatively background free sample by selecting a detection volume about DeepCore that does not extend to edge of the detector. This allows optical sensors outside of the detection volume to serve as dedicated downgoing muon detectors. Events that have hits on DOMs outside the defined detection volume that are causally correlated with the hits inside the volume are able to be identified as background muons. A schematic representation of this filtering algorithm is shown in Figure \ref{fig:DCVetoSketch}. 

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=.5\textwidth,keepaspectratio]{DeepCoreVeto.jpg}
  \end{center}
  \caption{Diagram of a downgoing muon traveling through IceCube into DeepCore. The colored circles indicate DOMs triggered by the muon with red representing earlier times and blue representing later times. The times of DOM hits in a defined veto region are compared to the time and location of the center of gravity (COG) fiducial volume DOM hits in DeepCore. Events that are found to have more than one veto region DOM hit causally correlated with the COG in DeepCore are filtered out as likely downgoing cosmic ray muons \cite{2012APh....35..615A}.}
  \label{fig:DCVetoSketch}
\end{figure}

This filter actually consists of two separate streams which are differentiated by the definition of which DOMs comprise the detection (or fiducial) volume and which DOMs are treated as belonging to the veto region. Inclusion of this additional branch using the relaxed veto allows for increased acceptance of higher-energy upgoing muon neutrino events that may be cut by the more stringent definition. Figure \ref{} shows the fiducial boundaries of the standard and expanded branches of the DeepCore filter. The acceptance rate for the standard and relaxed veto filters is 17.25 Hz and 23.3 Hz respectively. The end result is a reduction in the trigger level data rate of nearly two orders of magnitude yielding a much more manageable data sample on which more advanced background reduction techniques can be applied.

%% Insert Fiducial Boundary Picture

\section{Analysis Specific Cuts}
After reducing the IceCube data stream to specific filters, the selection criteria imposed by differing will begin to diverge in order to optimize sensitivity to their respective target signal. For this analysis, the data given by the low-energy filter is put through a series of cuts optimized to preserve upgoing and contained track-like events from muon neutrino interactions. What event traits to cut on and to what degree was decided through examination of many cut choices on a sample of simulated muon neutrino events and background simulation. The end result is a selection of event cuts that can be grouped into two categories. The first category consists of cuts derived from detector information in the veto region while the second focuses on event quality and reconstruction characteristics. Lastly, a cut developed through machine learning techniques and optimized with respect to analysis sensitivity is applied to achieve a final level data set ready for analysis.

Each iteration of data selection criteria are often referred to as `levels'. In IceCube, the output of the various physics filters is known as Level 1 or L1 data. This L1 data is subject to additional processing involving more CPU intensive reconstructions. This post-processed data is referred to as L2, and it is at this level that specific analyzers in IceCube will begin to impose their own selection criteria. We will continue use of this nomenclature to describe the various steps involved in the event selection process specific to this analysis (L3, L4, etc.). A block diagram of the entire event selection process is provided in Figure \ref{}, and Table \ref{tab:event_rates} provides a level by level summary of the data and MC simulation rates.

Lastly, the event selection process is divided into two catergories based on which branch of the DeepCore filter the event comes from. Events passing the standard filter definition which utilizes a full three surrounding layers of IceCube strings as active veto are referred to as low-energy stream or LES events. Events passing the branch which uses an expanded fiducial volume are referred to as high-energy stream or HES events. As the names suggest, the average energies of events belonging to the HES are higher than those found in the LES branch. The two event streams will largely make use of the same cuts, but there are instances in which the treatment of the two event samples is quite different. These differences will be specifically mentioned.

\subsection{Level 3}
Despite the best efforts of the low-energy filter, downgoing muons created in cosmic-ray air showers still represent the most dominant background at this level. Another large contributor to our event rate at this level is the presence of events triggered by correlated noise in DeepCore DOMs. These noise events consist of a handful of DOMs ($\sim$3-5) in the central part of the detector that all happen to have noise-induced pulses over a time period shorter than the DeepCore trigger window (2.5 $\mu s$). Because these events are almost entirely localized within central DeepCore, these events are only present in the LES portion of the event selection. Due to a lack of proper simulation of these events, it is important to remove them from event processing as early as possible.

The first set of cuts used in L3 focus on isolating and removing noise-induced events. These events generally have very low values of NChannel, a term for the number of DOMs registering light during an event. Additionally, the temporal correlation between the pulses received on these DOMs is not very strong. These characteristics motivate the following event cuts:
\begin{adjustwidth}{2.5em}{2.5em}
\setlength{\parindent}{0pt}
\textbf{MicroHits Cut} - This cut examines a 250 ns time period about the event trigger time to ensure that there are still some hits in spatial and temporal coincidence. The long time window of the DeepCore trigger, 2.5 $\mu$s, can allow noise-induced DOM hits to build up to the required 3 HLC threshold. By cutting on the number of DOM hits and total deposited charge in the DOM PMTs during this 250 ns time period, the number of noise triggered events is significantly reduced.

\textbf{Noise Engine Cut} - The Noise Engine is a modified version of an algorithm designed to identify track-like events in the detector. Its ability to identify tracks has been repurposed so that the algorithm can isolate events that show no track-like characteristics whatsoever. Events whose hits show little to no correlation consistent with a particle track or shower are more than likely noise-induced. The Noise Engine issues the event a pass/fail status.
\end{adjustwidth}
\setlength{\parindent}{17.5pt}
Distribution of hits and charge from the MicroHits cut are shown in Figure \ref{fig:MicroHitAndCharge}. The excess of data with respect to CORSIKA simulation caused by noise triggers is clearly visible.

\begin{figure}\label{fig:MicroHitAndCharge}
\centering
\subfigure[Distribution of DOM Hits from MicroHits Cut]{\label{fig:micro1}\includegraphics[width=70mm]{StdDC_MicroHits.png}}
\subfigure[Distribution of PMT Charge from MicroHits Cut]{\label{fig:micro2}\includegraphics[width=70mm]{StdDC_MicroCharge.png}}
\caption{Plots above show the distributions for number of DOMs hit (a) and total charge deposited (b) for simulation and data. The large excess of data (black) with respect to CORSIKA (red) simulation is explained by the presence of unsimulated noise events.} 
\end{figure}

In addition to these cuts focused on noise events, several cuts designed to eliminate downgoing muons are also employed at this level. These cuts make use of hit information from DOMs belonging to the veto region to search for any indication the event being a downgoing muon. These cuts will typically examine the DOM hits in each event with little or no hit cleaning in order to minimize the chances of leaving out possible veto information. This will also cause a small fraction of potential signal events to be removed from the analysis due to coincident noise events. These cuts include the reapplication of the DeepCore filter with no hit cleaning, a cut on the number of hits allowed in the upper regions of the detector, a cut on events having correlated clusters of veto region hits, examination of the time profile of light deposition in the detector, a cut on the reconstructed vertex position, and finally a cut on the ratio of hits in the veto region to hits in the fiducial region. The exact details of these cuts plots showing the distributions for simulation and data can be found in Appendix A.

The cuts applied to the HES portion of the event selection are nearly the same. A slightly more stringent cut value cut value is chosen for several of the aforementioned cuts due to the smaller veto region for this branch. Noise events are nearly non-existent in this branch of the event selection, so the MicroHits and Noise Engine cuts are not applied. Not all of the listed veto cuts are used for the HES branch either. Due to the longer muon tracks for $\nu_{\mu}$ signal events in the HES branch, the cut on charge ratio between veto region and fiducial region and the cut on the light deposition time profiles are not applied. Events that are part of the LES that only fail these two track-energy dependent cuts are transferred to the HES branch instead of being dropped by the analysis in hopes of retaining potentially interesting events. Distributions of cut parameters for HES simulation and data events are also given in Appendix A.

\subsection{Level 4}
The data volume is reduced substantially at this level by removing all events that are reconstructed as downgoing  in the detector (coming from the Southern sky). The cuts described in the previous level were designed to be used in many low-energy analyses and as such they make no attempt to eliminate all downgoing events. This analysis makes no attempt to use downgoing events due to difficulties in eliminating the muon background enabling us to impose a cut on the reconstructed direction. Another round of cuts utilizing veto information is imposed at this level as well. Lastly, a minimum threshold of DOM hits per event is set. The details of these cuts are listed below.
\begin{adjustwidth}{2.5em}{2.5em}
\setlength{\parindent}{0pt}
\textbf{Reconstruction Cut} - This cut marks the first use of information from event reconstruction algorithms in the event selection. The reconstruction used at this stage consists of a six iteration likelihood method called SPE6. The likelihood algorithm of this method assumes that pulses of light received by DOMs during the event arise from single photon hits. A likelihood value is generated by taking the product sum of the probabilities for DOMs being triggered or not triggered by light from a hypothesis track. This likelihood is then maximized to give a best fit to the hypothesis muon track. Six iterations are performed to ensure that the fit does not lock on to a local minimum in the likelihood space. The zenith direction from this reconstruction serves as the cut parameter, and any events whose reconstructed direction is more than 5$^{\circ}$ above the horizon is cut from the analysis. This cut is also applied to the reconstructed zenith provided by the simpler LineFit reconstruction algorithm which acts as a seed for the SPE6 reconstruction.

\textbf{Hit Threshold Cut} - Even after application of cuts specifically crafted to remove noise events in Level 3, there are still some noise-triggered events present at this data level. These events all have low NChannel (number of DOMs registering light in the cleaned hit series) values and hold a random distribution in reconstructed direction. Signal events that also happen to have low values of NChannel are unable to be reconstructed reliably and are generally of poor quality. For this reason and minimum NChannel threshold of 10 is set to remove these events from further processing.

\textbf{Veto Cuts} - There is still some veto hit information that has yet to be utilized in the event selection process. These cuts make use of a slightly different hit cleaning techniques in an attempt to retain possible non-noise hits from the veto region. The first cut applies the same hit cleaning technique normally used in the analysis, but with slightly looser cleaning settings. The number of veto region hits that occur prior to the event trigger are counted and events showing two or more such hits are removed. Yet another application of the original DeepCore filtering algorithm is also applied. This time an extremely loose hit cleaning is performed to maximize the possibility of finding hits in the veto region caused by incoming muons.
\end{adjustwidth}
\setlength{\parindent}{17.5pt}

Most of these cuts are also present in the HES event selection with exception being the additional application of the DeepCore filter algorithm. The HES branch also makes use of a cut on track quality and two separate cuts designed to eliminate coincident muon events. The track quality examines the space-angle difference between the LineFit and SPE6 reconstructed directions for events. Events whose reconstructions disagree on the track direction by more than 30$^{\circ}$ are removed. When two muons enter the detector simultaneously, the reconstruction methods can be fooled into reconstructing the event as a single upgoing track rather than two separate events. These coincident muon events are removed by splitting the event into two separate hit maps to search for individual tracks. 

This suite of cuts results in a reduction of the data rate by more than two orders of magnitude (2.16 Hz $\rightarrow$ 15.2 mHz). The distribution of reconstructed zenith in Figure \ref{fig:SPE6Recos} shows the ability of these reconstruction based cuts to separate background events. At this point the data rate is still an order of magnitude larger than the atmospheric neutrino rate predicted by MC simulation, so there is still a considerable amount of background events remaining. Distributions for both the LES and HES cut parameters are given in Appendix A.

\begin{figure}
\centering
\subfigure[Distribution of Reconstructed Zenith for LES Events]{\label{fig:res1}\includegraphics[width=70mm]{L4_LES_SPE6ZenithDist.png}}
\subfigure[Distribution of Reconstructed Zenith for HES Events]{\label{fig:res2}\includegraphics[width=70mm]{L4_HES_SPE6ZenithDist.png}}
\caption{Distribution of reconstructed zenith from an 6-iteration SPE fit to nominal DOM hit map. The distributions for LES (a) and HES (b) events show that the bulk of the data at this level consists of downgoing muons. The cut region is given by the shaded areas in the plots above.} 
\label{fig:SPE6Recos}
\end{figure}

\subsection{Level 5}
The selection criteria for this level are solely focused on removing the remaining obvious downgoing muon events. These events have managed to evade previous attempts at removal due to peculiarities in the cleaned DOM hit pattern. Using different hit cleaning settings can sometimes eliminate noise hits that are capable of confusing the reconstruction algorithms. Reapplying the reconstruction methods on these new DOM hit patterns can result in the proper identification of events as downgoing muons. The reconstruction cuts using different hit cleanings are described below.
\begin{adjustwidth}{2.5em}{2.5em}
\setlength{\parindent}{0pt}
\textbf{TightSRT SPE6 Zenith Cut (LES Only)} - The prevalence of noise hits in DeepCore DOMs can often lead reconstructions astray resulting in poor track hypotheses. The event displays of many of the remaining events at this cut level reveal that they are clearly background muons. However, noise hits present in the DOM hit map used for reconstruction can result in mis-reconstruction of these events. By using a hit cleaning algorithm with tighter settings, we can create a hit map that is less likely to include these noise this. Feeding this map to the reconstruction method will often reveal the true downgoing nature of these previously mis-reconstructed events. Events whose zenith is above 5$^{\circ}$ are removed.

\textbf{SPE2 with Early Hits Removed Zenith Cut} - This cut functions similarly to the TightSRT SPE6 just described. Instead of using new hit cleaning settings, the first two hits from the map are dropped. A common class of background event at this level is mis-reconstructed as upgoing due to the presence early hits in the DeepCore region of the detector. In order to fit these hits, the reconstruction method will attempt an upgoing track hypothesis that intersects with the downgoing hit pattern produced by a muon higher up in the detector. A cut is made on the reconstructed zenith of the event after exclusion of the first two DOM hits. Events whose zenith is above 5$^{\circ}$ are removed.
\end{adjustwidth}
\setlength{\parindent}{17.5pt}
These simple cuts result in a reduction of the data rate by a factor of two without incurring substantial $\nu_{\mu}$ signal losses. Distributions of the reconstructed zenith for events from the LES and HES branches are plotted in Figure \ref{fig:NoEarlySPE2}. After these cuts have been made, most of the obvious downgoing muons present in the event sample have been identified and removed. There are still many cosmic ray muon events remaining, but these events have managed to sneak through the outer layers of the detector without depositing much light. Elimination of these background events requires the more advanced techniques described in Level 6.

\begin{figure}
\centering
\subfigure[Neutrino Resolution by Energy]{\label{fig:res1}\includegraphics[width=70mm]{LES_NoEarlySPE2ZenithDistribution_L5.png}}
\subfigure[Neutrino Resolution by Number of DOMs]{\label{fig:res2}\includegraphics[width=70mm]{HES_NoEarlySPE2ZenithDistribution_L5.png}}
\caption{Distribution of reconstructed zenith from an 2-iteration SPE fit to a DOM hit map with the earliest two DOM hits removed. The distributions for LES (a) and HES (b) events show that a large population of mis-reconstructed downgoing muons are able to be identified through this method. The shaded are in subplot (a) shows the cut region; this region is the same for subplot (b) although it is not displayed.}
\label{fig:NoEarlySPE2}
\end{figure}
\subsection{Level 6}
Rather than making simple straight cuts on event parameters, the last step in the event selection process makes use machine learning techniques. The background that still remains at this level is not as easily separated from obvious signal events due to large overlap in the distributions of their event parameters. We can however use the differences in the shapes of these distributions to form a multi-dimensional cut that is optimized for signal retention. Once this cut has been applied, a final straight cut is imposed that ensures event quality for the final level dataset.
\subsubsection{Machine Learning Cut}
The last remaining separation power provided by cut parameter distributions is not easily extracted via simple straight cuts. In order to maximize signal and background simulation, we make use of a machine learning technique known as a boosted decision tree or BDT. 

\cite{MachineLearning}

\subsubsection{Quality Cuts}
Application of the BDT cut yields a manageable dataset that should be fairly pure. Some of the events that remain, however, are of suspect quality and are likely not useful to include in the analysis. By imposing a cut on event quality, we can eliminate some of these events without further degrading the sensitivity of our analysis. The quality parameter that is cut on is an estimation of the resolution of the event known as paraboloid sigma $\sigma_{para}$. This quantity is derived from the final level event reconstruction (see 7.4), and $\sigma_{para}$ itself is described in detail in section 7.5. For the discussion at hand, $\sigma_{para}$ can simply be understood as a measure of the event resolution.

Events having a value of $\sigma_{para}$ greater than 45$^{\circ}$ are removed from the final sample. The choice of a cut at 45$^{\circ}$ stems from a particular feature present in the analysis code. Overall, this quality cut reduces the overall data rate by about $10\%$. The events removed by this cut generally produce very few hits within the detector, and therefore their true direction is incredibly difficult to constrain.

One could argue that the timing information provided by such events could still prove useful in identifying potential neutrino flares. However, certain intricacies of the analysis code prevent these events from contributing to the signal hypothesis. During the analysis of final level events, the reconstructed direction of an event will be checked against a hypothetical source location to determine if the event could be associated with that source. This process involves the comparison of two probability density functions (p.d.f.s) which describe the event distributions expected from signal and background respectively \footnote{The mathematical details of these functions are provided in the discussion of the analysis method in chapter VIII.}. In order to decrease computation time, only events whose signal-to-background ($S/B$) ratio is greater than a specified threshold will actually contribute to the likelihood calculation. This effect is visualized in Figure \ref{fig:QualityCutParameterSpace}. The contours indicate different choices for this $S/B$ threshold, and only events with values of spatial separation ($\Delta \Psi$) and resolution ($\sigma_{para}$) that lie within the contour are able to contribute to a signal hypothesis. In practice we have chosen a $S/B$ threshold value of 8 which imposes a restriction on the maximal allowed value of $\sigma_{para}$ of 45$^{\circ}$.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=.65\textwidth,keepaspectratio]{SignalOverBackground_ContourMap.png}
  \end{center}
  \caption{Contour plot showing the ratio of the signal and background p.d.f. values for different choices of event-source separtion ($\Delta \Psi$) and event resolution ($\sigma_{res}$). Parameter choices that lie within the contours have a signal-to-background ratio large enough to be included in the analysis.}
  \label{fig:QualityCutParameterSpace}
\end{figure}

\section{Event Reconstruction}
An accurate reconstruction of neutrino events in the data sample is critical for optimal performance of any pointing analysis. During the event selection process several iterations of reconstructions are performed so that downgoing muons from cosmic rays can easily be identified. During data selection at lower levels, simpler reconstruction algorithms are often used to prevent a prohibitive amount of required CPU time for processing. These reconstructions then serve as the seed for more advanced techniques used at the analysis level. In this section the seed reconstructions for the final level will be discussed in addition to a detailed description of the final reconstruction whose results are used in the final analysis.

Reconstruction of a physics event within the IceCube detector depends on the geometry of the DOMs registering light from the event, the time of arrival for photons at those DOMs, and the total amount of light received at the individual DOMs. The final reconstruction used on events in this analysis makes use of a likelihood approach to derive a best fit muon track to the DOM hit pattern. This is accomplished by calculating the probability of pulses of light received in DOMs arising a hypothesis muon track in addition to the probability of DOMs seeing \textit{no} light from the track. A likelihood function that depends on track location and direction paramemters in addition to the location and timing of DOM hits is constructed from the product sum of all the hit probabilities. The form of this function is given in Eq. \ref{eq:RecoLLH}.
\begin{equation}
\label{eq:RecoLLH}
\mathcal{L} = \prod_{i}^{N_{hits}} P(t_{res},)
\end{equation}
In order to acquire a good fit to the actual data, the reconstruction method takes the ratio of values of the likelihood function under different choices of track position and direction. Maximization of this ratio leads to a best fit on the track parameters and therefore an estimated direction and location of the secondary muon produced in the muon neutrino interaction.

The reconstruction used at the final level functions somewhat differently from the likelihood based reconstructions used at earlier levels. Whereas earlier methods assumed that the pulses generated in DOMs are the product of single photon hits (SPE), the final level method assumes DOM pulses can originate from any multiplicity of DOM hits.

The method also makes use of the best estimate of ice properties within the detector. Previous reconstructions made use of an empirical function known as the Pandel function to estimate the probability of seeing a hit in DOM at distance $d$ from the hypothesis track with time delay $t$, \ref{eq:Pandel}.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=1.0\textwidth,keepaspectratio]{HES_Upgoing_WithReco.png}
  \end{center}
  \caption{Event display for a final level neutrino track event with reconstructed track. This event is fairly energetic with respect to the usual events in the final sample. The red arrow shows the best fit from the 16-iteration SplineMPE reconstruction. The dust layer is clearly visible through the lack of DOMs registering light in the middle of the event track.}
  \label{fig:HESEventWithReco}
\end{figure}

\begin{figure}
\centering
\subfigure[Neutrino Resolution by Energy]{\label{fig:res1}\includegraphics[width=70mm]{FinalLevel_NeutrinoResolutionByEnergy_JustGENIE.png}}
\subfigure[Neutrino Resolution by Number of DOMs]{\label{fig:res2}\includegraphics[width=70mm]{GENIE_FinalLevel_NeutrinoResolutionByNch.png}}
\caption{Median event resolution as a function of primary neutrino energy (a) and as a function of DOMs receiving light in the event (b). These distributions are for events belonging to the low-energy branch (LES) of the event selection.}
\label{fig:EventRes}
\end{figure}

\section{Error Estimation}
Given a reconstruction for an event, it is essential to have an accurate estimation of the possible error of that reconstruction. While it is impossible to know this error on an event-by-event basis, we can examine a large set of simulated events to obtain a statistical estimation of how well an individual event will be resolved. The reconstruction method used for events in this analysis utilizes a likelihood technique in which the directional parameter space is scanned over to optimize the fit. Events that are well-reconstructed will generally have a likelihood space with a strong minimum well centered about the true direction of the muon. On the other hand, the likelihood space of poorly resolved events is usually characterized by a very broad minimum in which there is no strong preference for a certain direction.

The shape of the likelihood space from event reconstruction correlates quite strongly with the resolution of the event. In order to estimate how well the event is resolved, a paraboloid (a two-dimensional surface defined by parabolic curve) centered to the likelihood minimum is fit to the likelihood space to estimate the depth of the minimum well. The position of the particle track in the detector is removed from the likelihood calculation so that the dimensionality of the likelihood space is reduced to only a two-dimensional surface.

\section{Final Level Data}
After all cuts have been applied, we are left with a sample of 22,040 events during the observation period with an expected neutrino `purity' of about 90$\%$. The bulk of these events are neutrinos of atmospheric origin and they represent an irreducible background for the analysis. While the sample does still contain some background cosmic ray muon events, these events do not greatly affect the sensitivity of our time-dependent analysis. As we are not trying to make any precision measurements on the atmospheric neutrino spectrum, we are able to be a bit loose with our cuts so that signal retention is maximized. The level-by-level data rates expected from simulation as well as the actual data rates are listed in Table \ref{tab:event_rates}. Distributions for various event parameters including reconstructed azimuth, zenith, and event vertex location are shown in Appendix B. Figure \ref{fig:LESEventFinal} shows an event display of a muon track from a typical final level neutrino event in the LES branch of the event selection. 

\begin{table}[h]
\caption[Final level data rate.]{Summary of event rates at each selection level. The amtospheric muon and neutrino rates are estimated through the use of Monte Carlo simulation (MC). The discrepancy between the data rate and summed MC rates at pre-final levels (Filter,L3,L4,L5) can largely be attributed to unsimulated correlated noise events in the detector in addition to a small contribution from $\nu_e$ events. Many early level data cuts focus on these noise events, which are properly handled in newer simulation datasets.\label{tab:event_rates}}
\begin{center}
\begin{tabular}{cccc}
  \hline
 \textbf{Event Type} &\textbf{ Cosmic Ray $\mu$} &\textbf{ Atmospheric $\nu_{\mu}$} &\textbf{ Collected Data}\\
\hline
Filter Level & 38.6 Hz & 7.8 mHz & 40.5 Hz \\ 
Level 3 & 1.79 Hz & 5.58 mHz & 2.16 Hz \\ 
Level 4 & 7.65 mHz & 1.64 mHz & 15.2 mHz \\ 
Level 5 & 2.5 mHz & 1.35 mHz & 6.3 mHz \\ \hline
\textbf{L6 -- Final Level}     & \textbf{0.065 mHz} & \textbf{0.81 mHz} & \textbf{0.774 mHz} \\ \hline
\end{tabular}
\end{center}
\end{table}

We estimate the sensitivity of this event selection to potential neutrino source fluxes by calculating a neutrino effective area through the use of our muon neutrino Monte Carlo simulation. Events are propagated through the detector and forced to interact to prevent simulation of non-interacting events. These events are then given a weight derived from their probability of interaction, the spectrum by which they were generated, the total number of events generated, and the geometrical area over which they were generated. The simulation events that remain after all data selection criteria have been applied are then binned in energy. The geometrical area over which the events were generated is then multiplied by the weighted number of final events in each energy bin to yield a neutrino ``effective" area for the detector. The effective are for the presented analysis after all cuts is shown in Figure \ref{fig:EffAreaFinal}.
\begin{figure}\label{fig:EffAreaFinal}
\centering
\subfigure[Analysis Effective Area NUGEN + GENIE]{\label{fig:goo}\includegraphics[width=75mm]{LowEnTransient_EffArea_GENIE_Nugen.png}}
\subfigure[Analysis Effective Area with just GENIE]{\label{fig:boo}\includegraphics[width=75mm]{LowEnTransient_EffArea_GENIE_DiffBin.png}}
\caption{(a) Muon neutrino effective area after all event selection cuts have been applied. The green line is given by the low-energy GENIE simulation while the black corresponds to the effective area predicted by NUGEN. The red dashed line shows the effective are for a traditional IceCube point source analysis. The presented selection method results in a higher effective are for sub-100 GeV neutrino events. (b) Close-up of the effective area at lower energies.} 
\end{figure}

Comparison of the effective area provided by this selection to previous point source selections shows an improvement at the lowest of detectable energies in IceCube. While the presented method may not be as sensitive as previous searches for harder astrophysical sources ($\sim$E$^{-2}$), we can expect this method to outperform the standard selection for transient sources featuring soft spectra or spectra with a low energy cutoff. Therefore this selection complements IceCube's previous efforts quite nicely as it allows us to increase our sensitivity to a broader range of possible sources. 

%%% Refer to App. A for Pictures of Final Level Distributions
%%% Insert Effective Area Plot

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=1.0\textwidth,keepaspectratio]{LESEventForThesis.png}
  \end{center}
  \caption{Event display for a final level neutrino track event originating in DeepCore. The colored spheres represent DOMs that have registered a hit during the event. The size of the spheres are indicative of the total light received by the PMT on that DOM. The color denotes the timing of the hit with red corresponding to earlier times and blue corresponding to later times.}
  \label{fig:LESEventFinal}
\end{figure}

%% paraboloid \cite{2006APh....25..220N}
\chapter{Systematic Effects}
There are many systematic uncertainties that can affect the interpretation of the results of this analysis. The primary contributors to uncertainty being the \textit{in situ} scattering and absorption properties of the ice medium and the absolute quantum efficiency of the PMTs within the DOMs. While there are other errors that could be considered such as the absolute neutrino cross-section, the contribution they provide to the overall error is negligible.

\section{Ice Properties}
The optical properties of the subsurface ice at the South Pole represent the most difficult systematic effect to adequately measure. This is largely due to the fact that this detection medium is inaccessible from the surface making direct measurement of the optical properties impossible. The scattering and absorption lengths in the ice greatly impact how light will propagate from interaction secondaries. Increased scattering can delay the arrival of photons to DOMs giving a larger spread in possible event interaction times. Assuming an incorrect absorption length can be problematic as well as it leads to inaccurate estimation of the total energy deposited by the neutrino event. If one hopes to reconstruct neutrino events with high enough accuracy for pointing, then it is necessary to develop a detailed and accurate model of the ice in which the detector is located.

Precise modeling of the polar ice is an ongoing task in the IceCube collaboration. Several iterations of models have reduced what was once a severe systematic problem to a relatively mild contribution to total systematic error. Current ice models consider several factors including depth dependence, tilt of ice layers, direction of glacial flow, and grain size distribution \cite{2013arXiv1301.5361I}. The absolute values of the optical properties such as absorption and scattering length are not known, however. Therefore we generate simulation that assumes different values for these parameters to estimate how large of an impact any error in the model will make at the final level of the analysis. To determine the degree to which this uncertainty affects our analysis, three datasets with differing values of absorption and scattering lengths are examined at the final level. The specifics of these datasets are listed in \ref{tab:ice_sets}.

\begin{table}[h]
\caption[Ice Properties Systematic Datasets]{List of simulation sets used to estimate systematic error from improper modeling of the ice properites. The listed percentages give the dataset's deviation from the nominal values. Variation in the absorption and scattering lengths lead to fairly minor changes in the analysis effective area with the strongest effects seen at lower energies.\label{tab:ice_sets}}
\begin{center}
\begin{tabular}{ccccc}
\toprule
\textbf{Simulator} & \textbf{Set Number} &\textbf{Spectrum} & \textbf{Scattering} & \textbf{Absorption}\\
\midrule
NUGEN & 10039 & E$^{-2}$ & -- & +10$\%$ \\
NUGEN & 10040 & E$^{-2}$ & +10$\%$ & -- \\
NUGEN & 10041 & E$^{-2}$ & -7.1$\%$ & -7.1$\%$ \\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.7\textwidth,keepaspectratio]{LowEnTransient_EffArea_SysNugen_IceEffect_300GeVTrunc.png}
  \end{center}
  \caption{Effective area at final event level as a function of energy for different possible ice properties. Increased absorption and scattering lead to very little degradation of the neutrino effective area at even the lowest energies.}
  \label{fig:IceSysEffArea}
\end{figure}
Examination of the effective areas plotted in Figure \ref{fig:IceSysEffArea} indicate that the variation of ice model parameters does not lead to a particularly large change in the final level effective area. While this source of systematic error had previously been quite dominant in IceCube analyses, the development of more sophisticated and accurate ice models has done much to alleviate this uncertainty. It is evident that the change in effective area between systematic sets is energy dependent, however. This likely due to the fact that lower energy events have fewer hits in the detector. While the lack of a few hits due to higher absorption or scattering is unlikely to prevent an energetic event from triggering the detector, losing even a single photon hit can result in a threshold event lacking a sufficiently bright signal to pass through all selection criteria. For this analysis we will take a conservative approach to our systematic error by assuming the worst case scenario from the systematic studies. In this case, we will assume the contribution to systematic error provided by innacurrate modeling of the ice to be a 3$\%$ effect. This value corresponds to the degree to which the effective area is degraded at the lowest energies of the analysis under the scenario in which ice absorption is increased by 10$\%$.

\section{DOM Quantum Efficiency}
The absolute value of the quantum-efficiency of in-ice DOMs is not entirely constrained. Measurements on the quantum-efficiency of all DOMs were taken prior to deployment, but the deployment process and subsequent years spent in the ice may have changed the sensitivity of each DOM differently. Because DOM efficiency relates directly to the ability of the DOMs to measure light, incorrect estimate of this parameter will lead to comparable error in any results derived from the analysis. There have been attempts within the collaboration to quantify how well the efficiency is known through dedicated study of minimally ionizing muons from the data. These studies have allowed us to limit the possible range of values the true efficiency could take.

In our consideration of DOM efficiency as a source of error for this analysis, simulation datasets with varying levels of DOM efficiency are used. As was done for the ice systematic sets, the final level effective area for the analysis using the different DOM efficiency sets will be studied. Unlike the ice systematic sets, the efficiency sets listed in Table \ref{tab:eff_sets} were not generated independently. All sets originate from a single parent dataset generated at a DOM efficiency 120$\%$ of the nominal value. This parent dataset is only processed to the DOM hit level prior to any triggering or filtering. To generate the listed datasets at their respective efficiencies, a fraction of the photons present in the parent dataset are dropped from the event in accordance with the desired efficiency. We consider a relative large 10$\%$ deviation from nominal efficiency to determine the analysis method's sensitivity to DOM efficiency error.

\begin{table}[h]
\caption[DOM Efficiency Systematic Datasets]{List of simulation sets used to estimate systematic error from possible differences in simulated and real DOM efficiency. The listed percentages give the dataset's deviation from the nominal values. These datasets are all derived from a single GENIE dataset produced with 120$\%$ DOM efficiency. The sets are then scaled down to their respective efficiencies by dropping the appropriate fraction of photon hits from the dataset events.\label{tab:eff_sets}}
\begin{center}
\begin{tabular}{cccc}
\toprule
\textbf{Simulator} & \textbf{Set Number} &\textbf{Spectrum} & \textbf{DOM Efficiency} \\
\midrule
GENIE & 800011 & E$^{-2}$ & Nominal  \\
GENIE & 800011 & E$^{-2}$ & 90$\%$ \\
GENIE & 800011 & E$^{-2}$ & 110$\%$ \\
\hline
\end{tabular}
\end{center}
\end{table}

The effective areas for the systematic efficiency sets are plotted in Figure \ref{fig:DOMSysEffArea}. As was the case for variation in ice parameters, the largest difference between the efficiency datasets is found at low energy. This is also very likely due to threshold events being disproportionately affected by changes in photon sensitivity. The trend in energy is clearly visible in the plot of the ratios of the systematic and nominal effective areas (Figure \ref{fig:DOMEffEnergyTrend}). The worst case scenario will also be used to determine how large of an effect we might expect from simulating DOM efficiency improperly. The effective area at lower energies shows an approximate 10$\%$ decrease from the nominal value for the dataset featuring 90$\%$ efficiency. Therefore, this 10$\%$ value is chosen as the contribution from possible error in DOM efficiency to the total systematic error.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.7\textwidth,keepaspectratio]{LowEnTransient_EffArea_SysGENIE_DomEfficiency.png}
  \end{center}
  \caption{Effective area at final event level as a function of energy for different possible DOM efficiency settings. At lower energies, the change in effective area becomes approximately proportional to the relative change in DOM efficiency.}
  \label{fig:DOMSysEffArea}
\end{figure}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.7\textwidth,keepaspectratio]{GENIE_EffectiveAreaRatioTrend_DomEff.png}
  \end{center}
  \caption{Ratio of systematic set effective areas to the nominal effective area. The plot above shows that the degree to which the analysis is affected by DOM efficiency is clearly energy dependent.}
  \label{fig:DOMEffEnergyTrend}
\end{figure}

If one considers the worst possible scenario, the errors in ice model $\sigma_{ice}$ and DOM efficiency $\sigma_{eff}$ show a degradation in the analysis effective area of 3$\%$ and 10$\%$ respectively. We combine these sources of error by adding the individual contributions in quadrature like so
\begin{equation}\label{eq:syserror}
\sigma_{sys}^2 = \sigma_{ice}^2 + \sigma_{eff}^2
\end{equation} 
This yields a total systematic error correction to the effective area of 10.44$\%$. This error is folded into the results of the analysis by adjusting the effective area used in limit calculations by -10.44$\%$. The effective area adjusted for systematic effects is plotted along with the nominal effective area in Figure \ref{fig:SysAreaWithNom}.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.7\textwidth,keepaspectratio]{LowEnTransient_EffArea_GENIE_WithSystematicAdjustedArea.png}
  \end{center}
  \caption{Effective area at final event level as a function of energy. The black line gives the analysis effective area after adjustment for systematic error while the nominal effective area is plotted as the dashed line.}
  \label{fig:SysAreaWithNom}
\end{figure}

\chapter{Analysis Method}

The analysis presented in this thesis makes use of both directional and timing information from the final level event dataset. The techniques that are used in this analysis have been applied to other IceCube event selections in a similar fashion. As of yet, these time-dependent searches focused on IceCube events have yet to find any time-dependent neutrino sources of significance higher than background expectations \cite{2012ApJ...744....1A}. A very thorough overview of the time-dependent likelihood analysis methods used in previous IceCube analyses is given by Braun, et al. \cite{2010APh....33..175B}. The method detailed in the following section is mostly the same, but there are a few minor modifications made to the process to improve performance on a low energy event selection.

\section{Unbinned Likelihood Method}
The identification of a statistically significant astrophysical signal amongst a high number of background events can be a difficult problem in astronomy. Likelihood based methods are commonly used to address this issue. Searches utilizing a likelihood method are able to provide a probabilistic interpretation of a signal hypothesis with respect to background fluctuations for the event dataset being examined. These searches will typically make use of timing, directional, and occasionally reconstructed energy information from events to find clustering indicative of a true astrophysical source. Whether or not individual events qualify as signal or background when testing a source hypothesis can differ depending on whether the search is 'binned' or 'unbinned'. For binned analyses, the signal hypothesis generates a set of coniditions that an event must meet to be considered as signal-like. This usually consists of selecting an area about the source location (a bin) that the reconstructed direction of the event must lie in. Events are then classified as either belonging to signal or being attributable to background. This results in all events having a binary status with respect to association with a hypothetical source. 

The unbinned method, which is the method chosen for this analysis, is typically more sensitive as it allows for events to be seen as both signal or background. In order to accomplish this, probability density functions (p.d.f.s) are constructed that are representative of the expected spatial, temporal, or energy distributions for both the signal and background possibilities. Thus, well-resolved events will strongly contribute to the likelihood calculation as signal- or background-like while more marginal events can make a contribution to either scenario with appropriate weighting instead of being sharply divided into either category. This makes the unbinned method much better suited for analysis of event samples with a wide range of resolutions.

The probability for seeing an event $i$ in our analysis given a time-dependent source hypothesis takes the following form:
\begin{equation}\label{eq:EventProb}
\mathcal{P}_i(|\mathbf{x}_i-\mathbf{x}_s|,n_s,t_i,t_o,\sigma_w,\sigma_i) = \frac{n_s}{n_{\mathrm{tot}}} \mathcal{S}_i + \left(1-\frac{n_s}{n_{\mathrm{tot}}}\right) \mathcal{B}_i
\end{equation}
where $S_i$ and $B_i$ are the signal and background p.d.f.s introduced below. The values of these PDFs depend on the reconstructed direction of the event \textbf{$x_i$}, the angular resolution of the event $\sigma_i$, the arrival time of the event $t_i$, the location of the hypothetical source \textbf{$x_s$}, the mean time of the source flare $t_o$, the duration of the flare $\sigma_w$, the total number of events in the dataset $n_tot$, and lastly the number of signal events $n_s$. The formulation of event-by-event probability given by \ref{eq:EventProb} provides a quantifiable method through which we can weigh various signal hypotheses for the dataset against the null hypothesis in which none of the events are the result of a signal source.

\subsection{Signal P.D.F.}
The signal p.d.f. describes the expected distribution in time and space for neutrino events that orginate from the source model being tested. The signal p.d.f. consists of both a spatial and temporal part and is constructed in this way
\begin{equation}
\mathcal{S}_i(|\mathbf{x}_i-\mathbf{x}_s|,t_i,t_o,\sigma_w,\sigma_i) = S_i(|\mathbf{x}_i-\mathbf{x}_s|,\sigma_i) \cdot T_i(t_i,t_o,\sigma_w)
\end{equation}
where
\begin{equation}
S_i(|\mathbf{x}_i-\mathbf{x}_s|,\sigma_i) = \frac{\kappa}{4\pi \sinh \kappa} \exp \left(\kappa \cos |\mathbf{x}_i-\mathbf{x_s}|\right)
\end{equation}
and
\begin{equation}
T_i(t_i,t_o,\sigma_w) = \frac{1}{\sqrt{2\pi}\sigma_w} \exp \left(-\frac{(t_i-t_o)^2}{2 \sigma_w^2}\right)
\end{equation}
The function representing the spatial term $S_i$ is the Kent-Fisher distribution \cite{1982}. The inclusion of the this distribution in the spatial p.d.f. represents a slight deviation from the standard construction of $S_i$ in IceCube analyses. It is analogous to the 2-dimensional Gaussian distribution on a flat surface, and for small values of $\sigma_{i}$ ($\lesssim 3^{\circ}$) the distributions are nearly identical. However, the Kent-Fisher distribution is properly normalized to the surface of a sphere. It therefore gives a better description of the spatial p.d.f. for events with larger uncertainties which are common in the lower energy sample used in this analysis. The concentration parameter $\kappa$ can be thought as the counterpart of the Gaussian sigma for the Kent-Fisher distribution, and it is determined by the event resolution ($\kappa = \sigma_{i}^{-2}$). The temporal term $T_i$ is simply a Gaussian with mean time of $t_o$ and a width of $\sigma_w$. Other source time profiles could be considered for this analysis, but testing has shown that the choice does not strongly affect the analysis sensitivity. Due to the untriggered nature of this analysis, the Gaussian profile is chosen to keep the method simple and relatively model independent. 
	
\subsection{Background P.D.F.}
The dataset being examined is heavily background dominated. This allows to simply derive our background p.d.f. directly from the final level dataset without having to make any assumptions about what the background should look like. Thus, the background p.d.f. looks like 
\begin{equation}
\mathcal{B}_i(\mathbf{x}_i,t_i) = P_{BkgDec}(\delta_i)\frac{P_{BkgAz}(\alpha_i)}{T}
\end{equation}
where $\delta_i$ and $\alpha_i$ are the event's reconstructed declination and detector azimuth respectively and $T$ is the total livetime of the analysis. The azimuthal and declination p.d.fs are taken from distributions of final level data as shown in Figure \ref{fig:BkgOnly_Zen_And_Azi}. By constructing the p.d.f. directly from the dataset we are able to fold in the declination dependence of both the muon and neutrino background as well as the difference in detector response. The IceCube detector is azimuthally symmetric, but the triangular lattice formed by its constituent strings do produce preferred corridors for background events to sneak through. This effect is fairly minor at the final event level, nonetheless it is also taken into consideration via the $P_{BkgAz}(\alpha_i)$ term. Lastly, the time dependence of the background is assumed to be flat. While there is seasonal variation in the atmospheric muon and neutrino rates, these modulations are not large at the final event level and the timescale of variation is much greater than the expected duration of neutrino emission from the target sources.

\begin{figure}
\centering
\subfigure[Final Level Event Azimuth Distribution]{\label{fig:alpha}\includegraphics[width=75mm]{BkgOnly_LES_SplineMPE_AzimuthDist_FinalLevelDist.png}}
\subfigure[Final Level Event Zenith Distribution]{\label{fig:beta}\includegraphics[width=75mm]{BkgOnly_LES_SplineMPE_ZenithDist_FinalLevelDist.png}}
\caption{Distirbution of final dataset events in zenith and azimuth in dector coordinates. These distributions are used to create the spatial terms in the background p.d.f. used in the likelihood calculation.} 
\label{fig:BkgOnly_Zen_And_Azi}
\end{figure}

\subsection{Likelihood Function}
In order to find the best fit for the source parameters $n_s$, $t_o$, $\sigma_w$ at a specified location \textbf{$x_s$}, an optimizable likelihood function is needed. This function is given by the product sum of all individual event probabilities from the dataset:
\begin{equation}\label{eq:LLH}
\mathcal{L}(\mathbf{x}_s,n_s,t_o,\sigma_w) = \prod \mathcal{P}_i(|\mathbf{x}_i-\mathbf{x}_s|,n_s,t_i,t_o,\sigma_w,\sigma_i)
\end{equation}
The value of this function will depend on all events within the dataset and will vary for different choices of the signal parameters $\mathbf{x}_s,n_s,t_o,\sigma_w$. Furthermore, we can maximize the ratio of the likelihood function with specific choices of signal terms to the value of the function under the null hypothesis ($n_s=0$) to yield a best fit to the data. The value of the likelihood ratio yields a test statistic $\lambda$ given by
\begin{equation}
\lambda = 2\log \left[\frac{\mathcal{L}(\mathbf{x}_s,n_s,t_o,\sigma_w)}{\mathcal{L}(n_s = 0)} \right]
\end{equation}
with $\mathcal{L}(n_s = 0)$ being the null hypothesis and $\mathcal{L}(\mathbf{x}_s,n_s,t_o,\sigma_w)$ as the signal hypothesis being tested. Maximization of $\lambda$ through variation of the flare parameters yields
\begin{equation}
\hat{\lambda} = 2\log \left[\frac{\mathcal{L}(\mathbf{x}_s,\hat{n}_s,\hat{t}_o,\hat{\sigma}_w)}{\mathcal{L}(n_s = 0)} \right]
\end{equation}
where $\mathcal{L}(\mathbf{x}_s,\hat{n}_s,\hat{t}_o,\hat{\sigma}_w)$ are the optimized flare parameters and $\hat{\lambda}$ is the maximized test statistic. 

The test statistic is defined as such so that the distribution of $\lambda$ values from datasets consisting of only background events are well-modeled by a $\chi^2$ distribution with degrees of freedom equivalent to the number of parameters being fitted. Given that the background distribution is $\chi^2$ distributed, Wilks's theorem can be used to estimate the probability or p-value of seeing a clustering of neutrino events with test statistic $\lambda$ \cite{wilks1938}. This p-value allows the method to reliably identify the most statistically significant neutrino flare in the dataset.

While this formulation of $\lambda$ has been shown to yield a $\chi^2$ distribution of background test statistics for time-independent searches in IceCube, it does not produce a distribution similar to $\chi^2$ when used in time-dependent searches \cite{2012ApJ...744....1A}. This is due to a bias towards shorter flare duration in the analysis method. For a dataset of a given duration, it is possible to divide the data into many more short flare events than long flares. This creates an effective trials factor for shorter flares, and so the definition of the test statistic must be changed in order to compensate for this effect:
\begin{equation}
\hat{\lambda} = 2\log \left[\frac{\sqrt{2\pi}\hat{\tau}_w}{T}\frac{\mathcal{L}(\mathbf{x}_s,\hat{n}_s,\hat{t}_o,\hat{\tau}_w)}{\mathcal{L}(n_s = 0)} \right]
\end{equation}
The introduction of the marginalization factor $sqrt{2\pi}\tau /T$ brings the background $\lambda$ distribution back into agreement with the appropriate $\chi^2$ distribution \cite{2012ApJ...744....1A}.

To summarize, this analysis uses a likelihood ratio method to find the best values for a signal-plus-background model of the dataset. This is accomplished through maximization of ratio of the likelihood function with best fit parameters to the value of the function under the null hypothesis to generate a test statistic. The test statistic serves as figure of merit that characterizes the degree to which the data is better explained by a signal hypothesis rather than the background only scenario. Because we are not selecting any specific locations to test for flaring, the test statistic must be maximized at each possible location in the sky. The details of this process are described in the following section.
\section{Sky Scan}

The analysis performed is not a triggered search, and therefore it is necessary to examine the entire solid angle domain of the analysis for any possible transient sources. The difficulty in rejecting background muons at lower energies limits the analysis to up-going and horizontal events ($< 5^{\circ}$ above the horizon). Because of IceCube's location at the South Pole, this results in a search over all right ascension in a declination band ranging from -$5^{\circ}$ to $90^{\circ}$. The search method discretizes the northern portion of the sky into many bins, and the coordinates of these bins serve as the location of a hypothetical flaring source to be tested. The fairly large median resolution of the event sample (see Fig. \ref{fig:EventRes}) allows the size of the search bins to be set to a relatively coarse 2$^{\circ}$ by 2$^{\circ}$. 

Maximization of the likelihood given by eq. \eqref{eq:LLH} is done at each location in the grid with $\mathbf{x_s}$ set to the location of the bin. This results in each bin having a best-fit neutrino flare with its own values of $\hat{n_s}$, $\hat{t}_o$, $\hat{\sigma}_w$, and test statistic $TS$. After this scan over the 2$^{\circ}$ by 2$^{\circ}$ is completed, a finer follow-up scan with 0.5$^{\circ}$ by 0.5$^{\circ}$ binning is performed on any coarse bins with $-\log_{10}(p_{val}) > 1.75$ where $p_{val}$ is the estimated pre-trials p-value of the maximized test statistic. Following the completion of the fine-scan, the best-fit flare from the bin with the most significant test statistic is returned as the final result of the search. The results of a scan over a scrambled dataset consisting of only background events is shown in Figure \ref{fig:NullTrialSkyMap}.
\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=1.0\textwidth,keepaspectratio]{NullMap996.png}
  \end{center}
  \caption{Sky map in celesital coordinates of pre-trials p-values for best-fit flares per bin for a randomized dataset. Random map generated by scrambling arrival times of real events. The black circle shows the location of the most significant flare found by the method. }
  \label{fig:NullTrialSkyMap}
\end{figure}

We calculate the sensitivity of this method by checking how well the search is able to pick out various levels of signal strength from background fluctuations. This procedure begins by selecting a point in the sky to perform our likelihood maximization. A background dataset is then generated by scrambling the time of arrival of the events. Due to the detector's location at the South Pole, this effectively scrambles the events in azimuth as well while keeping the declination distribution the same. The likelihood is then maximized for this set of scrambled data and the p-value obtained from the best fit is stored. This process is iterated many times to build a background p-value distribution. Injected signal events are now included in addition to the scrambled background data. Events are injected from an assumed source spectrum with a Poisson mean of $n_s$ that is increased until the desired fraction of p-values from signal injections (typically 90\%) beat the median p-value from the background only distribution. This is repeated for several different timescales in for a generic source with an $E^{-3}$ spectrum in Figure \ref{fig:SensE3}.
\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=.8\textwidth,keepaspectratio]{LowEnTransient_NEventDiscoveryPotentialANDSensitivity_E3.png}
  \end{center}
  \caption{Calculated event sensitivity of the analysis at Declination $\delta$=$16^{\circ}$ for an $E^{-3}$ spectrum.}
  \label{fig:SensE3}
\end{figure}

The discovery potential (which is also plotted in Figure \ref{fig:SensE3}) is calculated in a similar fashion. Rather than building a background p-value distribution, a threshold p-value is chosen. In this case, the value is set to the probability equivalent to that of a one-sided 5$\sigma$ deviation. Injections are performed until the mean value of injected $n_s$ results in flares whose recovered p-value exceeds the threshold p-value 50\% of the time. This does not, however, take into account any trials factors that arise from performing the scan over the whole northern sky. It is clear that the sensitivity of this method will suffer greatly for flares of longer duration. This limits the application of the method to sources with duration of approximately $10^4$s or shorter. However, given that the analysis was developed with short transient sources such as core-collapse SNe ($\Delta T \sim 1-100s$), this degradation in performance at longer timescales should not be an issue.

\section{Significance and Trials Factors}
In order to determine which bin has the most significant flare, we evaluate an estimated p-value based on the maximized test statistic $\lambda$ for that bin. The distribution of test statistic values for individual bins is assumed to be $\chi^2$ distributed, however the test statistic distribution of best-fit flares for searches over background only datasets is not known \textit{a priori} however. Therefore it is necessary to perform many iterations of the analysis method using scrambled versions of the dataset to determine the distribution of the test statistic with no real signal present in the data. The desired level of statistical significance determines the number of trials required, e.g. determining what value of the test statistic constitutes a 3$\sigma$ outlier would require approximately $10^4$ scramblings. The background test statistic distribution for such scramblings $2\times 10^4$ is shown in \ref{fig:NullScramblingDistribution}.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.98\textwidth,keepaspectratio]{TestStatisticDistribution_Null.png}
  \end{center}
  \caption{Distribution of maximized test statistic $\lambda$ for $2\times 10^4$ searches performed on randomized datasets. Dashed lines mark the location of one-sided $\sigma$ deviations.}
  \label{fig:NullScramblingDistribution}
\end{figure}

Once this distribution is adequately determined, the test statistic from any result we obtain from the analysis can be compared with the distribution we generated solely from background trials. This allows us to determine the probability of seeing a flare from just background fluctuations. This is sometimes referred to as the "p-value of the p-value" which is essentially the true p-value for the flare after properly accounting for all trials factors. This corrected p-value will be used when citing the significance of the final analysis result. 

The scrambled background trials also serve as a check for any biases in the analysis method with respect to best-fit parameters. The recovered flare parameters for these trials show no strong pull towards certain flare parameter values, though their is some mild declination dependence which is expected (the atmospheric background distribution is declination dependent). The distributions of best fit flare parameters for background trials can be seen in Figure \ref{fig:scramble_trials_parameters}. The symmetry of the detector is evident through the lack of preference in the azimuthal location of the best-fit flares. Additionally, the smooth distribution in recovered flare times reveals that the analysis method is able to avoid locking on to specific timescales.

\begin{figure}\label{fig:scramble_trials_parameters}
\centering
\subfigure[Right Acension]{\label{fig:a}\includegraphics[width=70mm]{RightAcensionDistribution_Null.png}}
\subfigure[Declination]{\label{fig:b}\includegraphics[width=70mm]{DeclinationDistribution_Null.png}}
\subfigure[Signal Strength $n_s$]{\label{fig:c}\includegraphics[width=70mm]{NSignalDistribution_Null.png}}
\subfigure[Flare Width $\sigma_w$]{\label{fig:d}\includegraphics[width=70mm]{SigmaDistribution_Null.png}}
\caption{Distribution of flare parameters for the most significant flares identified by analysis method in $2\times 10^4$ trials on scrambled background-only data. The values plotted are the location of the flare in R.A. (a) and Declination (b). Also, the distribution of flare strengths measured in number of signal events is shown in (c) while the best fit flare duration is given by (d).} 
\end{figure}



\chapter{Results}
Applying the defined analysis method on the unscrambled dataset yields a skymap of the pre-trials p-values derived from the maximized test statistic for each bin. This map is shown in Figure \ref{fig:RealSkyMap}. The black circle in Figure \ref{fig:RealSkyMap} shows the hottest spot after the completion of the sky scan. The best fit to flare parameters for this hot spot are listed in Table \ref{tab:best_fit_flare}. The lack of a high test statistic value in any adjacent bins in the sky map indicates that our hottest spot is dominated by a single highly resolved track event. Examination of the most signal-like events for this flare reveals that this is indeed the case. Figure \ref{fig:TopEvents} shows the spatial p.d.f. values for these events in addition to their time of arrival. Due to the event with the highest spatial p.d.f. value being well-localized ($<$1$^{\circ}$ estimated error), its signal-like contribution to the likelihood is limited solely to the bin it in which its reconstructed direction lies.

\begin{table}[h]
\caption[Best-fit signal parameters]{Best-fit values for flare location, duration, strength and time. The value listed for -log$_{10}(p)$ is the probability of seeing such a flare before applying the appropriate trials factor correction. The real probability of seeing a flare as or more significant as the one observed is given by the post-trials corrected $p$-value listed in the last row.\label{tab:best_fit_flare}}
\begin{center}
\begin{tabular}{cc}
  \toprule
 \textbf{Flare Parameter} &\textbf{ Best-fit Value} \\
\midrule
R.A. & 268.75$^{\circ}$ \\ 
Dec & 54.25$^{\circ}$ \\ 
$\hat{n}_s$ & 13.528 \\ 
$\hat{t}_0$ & 56107.8 MJD \\ 
$\hat{\sigma}_w$ & 5.89 days \\ 
-log$_{10}(p)$ & 4.1751 \\
Post-trials $p$ & 0.56 \\
\end{tabular}
\end{center}
\end{table}

%% SkyMap
\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=1.0\textwidth,keepaspectratio]{RealResultSkyMap.png}
  \end{center}
  \caption{Sky map of pre-trials p-values for best flares per bin. The black circle identifies the location of the most significant flare found at RA = 268.75$^\circ$ and Declination = 54.25$^\circ$.}
  \label{fig:RealSkyMap}
\end{figure}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.6\textwidth,keepaspectratio]{SpatialPDFValue_EventsArrivingNearFlareTime.png}
  \end{center}
  \caption{Plot above shows the signal spatial p.d.f. values for several of the most signal-like events for the best fit flare. The y-axis denotes the value of the spatial p.d.f. while the x-axis gives the time of arrival for the event in MJD. The blue gaussian is the time profile for the best fit flare found by the method.}
  \label{fig:TopEvents}
\end{figure}

The pre-trials significance of this flare was found to be about 3.82 $\sigma$. However, this flare represents the most significant flare found after examination of the entire sky, and so the true significance must factor in a trials penalty. The exact penalty that must be factored in is unknown \textit{apriori}, but it can be determined from background trials via the method described in section 9.3. The test statistic value for our result flare is compared to the distribution of test statistics from the most significant flares found in background-only trials in Figure \ref{fig:PostTrialsComparison}. Comparing the best-fit flare result from the analysis to this background distribution yields a highly background compatible trials-corrected $p$-value of 56$\%$. Despite the low significance of the best-fit flare, some cursory checks with high-energy astrophysical source catalogs were performed to see if any known sources may be associated with the found flare. The TeVCat catalog \cite{TeVCat}, which catalogs discovered TeV gamma ray sources, showed no known sources at or nearby the flare location. An additional check with the IAU list of known supernovae also revealed no known sources in temporal and spatial coincidence with the best-fit flare result \cite{SNeCatIAU}.

%% Post-trials p-value
\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.75\textwidth,keepaspectratio]{TestStatisticDistribution_WithResult.png}
  \end{center}
  \caption{Distribution of test statistic $TS =$ log$_{10}(\lambda)$ of most signficant flare found in 1,985 background trials. The test statistic value for the best fit flare on the unscrambled data set is given by the dashed line.}
  \label{fig:PostTrialsComparison}
\end{figure}

The flare found by the analysis method is entirely consistent with a dataset consisting of only background events suggesting no significant sources of neutrino emission for which the analysis is sensitive. In light of this null result, we can construct a limit on the neutrino flux of any unknown neutrino flare that occured during the observation period. Making this limit requires some assumptions about the shape of the neutrino spectrum. The generic source limit provided in this chapter will assume a simple E$^{-3}$ power-law neutrino spectrum from a source with a Gaussian time profile. A detailed examination of the neutrino emission model for the primary target source of this analysis, choked GRBs, is given in the following chapter.

The standard IceCube procedure in the event that the $p$-value of the result of the analysis is greater than 50$\%$ is to derive an upper limit from the calculated sensitivity of the analysis (which by definition corresponds to a $p$-value of 50$\%$). Figure \ref{fig:SensE3} shows the event sensitivity of the analysis for a neutrino source with a E$^{-3}$ spectrum. This plot details the number of neutrino events $n_s$ required for a source to be visible to the analysis method with 90$\%$ confidence. In order to calculate a limit on source neutrino flux, we must calculate the magnitude of the flux required to generate the number of events in the detector given by the event upper limit. The relationship between neutrino flux and number of observed events is given in Eq. \ref{eq:n_obs_from_flux}.
\begin{equation}\label{eq:n_obs_from_flux}
N_{obs} = \int dEA_{eff}(E)F_{\nu}(E)dE
\end{equation}
Integration of the analysis effective area $A_{eff}$ with an assumed neutrino flux $F_{\nu}$ yields event expectation in the detector. Setting $N_{obs}$ to our calculated event upper limit from Figure \ref{fig:SensE3} gives the required normalization constant for the neutrino flux. Once the normalization has been determined, the upper limit can be given in terms of time-integrated flux (Figure \ref{fig:time_int_flux_limit}). The degradation of the limit at longer flare widths is due to the increasing likelihood of background events to be found in the on-source region.
\begin{figure}\label{fig:time_int_flux_limit}
\centering
\subfigure[Event Upper Limit for E$^{-3}$ Spectrum]{\label{fig:eh}\includegraphics[width=100mm]{LowEnTransient_NEventDiscoveryPotentialANDSensitivity_E3.png}}
\subfigure[Time-Integrated Flux Upper Limit for E$^{-3}$ Spectrum]{\label{fig:bee}\includegraphics[width=100mm]{LowEnTransient_TIFluxSensitivity_E3_MergedSim_ActualFinalCut.png}}
\caption{(a) Event upper limit as a function of flare width for an E$^{-3}$ source flux. Event upper limit is defined as the number of signal events required for a source to be detectable by the analysis method with 90$\%$ confidence. (b) Upper limit on the total time-integrated neutrino flux for a neutrino flare with E$^{-3}$ power law emission. Flux upper limit derived directly from event upper limit and the analysis effective area.} 
\end{figure}


\chapter{Limits on Choked GRBs}
The upper limits described in the previous chapter provide an estimation of the sensitivity of the analysis method with respect to a generic neutrino source. If we assume a specific source model, however, the event upper limit given by the analysis result can be used to examine the detectability of the source under certain choices of model parameters. In this chapter we will examine the neutrino emission model for choked GRBs (see section 4.4) in light of this source class being the primary motivation for the development of this anlaysis.

The model for neutrino emission put forward by Razzaque, M\'{e}sz\'{a}ros and Waxman predicts a doubly broken power law with break energies set by hadronic and radiative cooling of the parent pions of the neutrinos \cite{2004PhRvL..93r1101R}. The model was extend by Ando and Beacom to include neutrinos from kaon decay resulting in a slighly more complicated spectral shape given the different break energies for the respective mesons \cite{2005PhRvL..95f1103A}. In the combined model, the neutrino spectrum shape and total fluence depend primarily on the energy E$_{j}$ and bulk lorentz factor $\Gamma_{b}$ of the relativistic jet in which the parent mesons are produced. For this reason, we examine the RMW/AB model in E$_{j}$--$\Gamma_{b}$ phase space to determine what choices of E$_{j}$ and $\Gamma_{b}$ would have been detectable by the analysis method if a choked GRB occurred during the livetime of the analysis.

\section{Parameter Dependent Upper Limit}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=1.0\textwidth,keepaspectratio]{UpperLimit_2DHisto_SysAdj.png}
  \end{center}
  \caption{guh}
  \label{fig:ParameterDep_UL}
\end{figure}

\section{Visibility Distance}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=1.0\textwidth,keepaspectratio]{DistanceLimit_2DHisto_WithContours_SysAdj.png}
  \end{center}
  \caption{guh}
  \label{fig:ParameterDepVisDist}
\end{figure}

\section{Volumetric Rate Limit on Choked GRBs}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=1.0\textwidth,keepaspectratio]{RateLimit_2DHisto_wContours_SysAdj.png}
  \end{center}
  \caption{guh}
  \label{fig:ParameterDepRateLimit}
\end{figure}

\chapter{Conclusion}

\appendix
\chapter{Appendix A -- Additional Event Selection Distributions}
This appendix contains additional details of event selction cuts from Chapter 7. Distributions of cut parameters for simulation and data belonging to both the low-energy stream (LES) and high-energy stream (HES) are given.

\section{Level 3 Cuts}
While the cuts used to eliminate noise events at this level were explicitly described, the other cuts applied at this data level were only referred to without defining them. Descriptions of these cuts are provided:
\begin{adjustwidth}{2.5em}{2.5em}
\setlength{\parindent}{0pt}
\textbf{Causal Veto PE} - 

\textbf{NAbove200PE} - 

\textbf{C2QR6} -

\textbf{VertexZ} -

\textbf{RTVeto250PE} -

\textbf{Charge Ratio} -

\end{adjustwidth}
\setlength{\parindent}{17.5pt}


\chapter{Appendix B -- Final Level Distributions}
This appendix contains distributions of select event parameters for both simulation and real events at the final selection level. The plots here show that the data is mostly well modeled by our neutrino simulation. There are some differences it absolute rate that are largely due to uncertainty in the normalization of the atmospheric neutrino spectrum. Definitions for the plotted parameters are also provided.
\begin{adjustwidth}{2.5em}{2.5em}
\setlength{\parindent}{0pt}
\textbf{N Channel} - N Channel or Nch refers to the number of DOMs receiving light from the event. This can be interpreted as a crude energy proxy as more energetic events will have a larger light yield.

\textbf{Detector Azimuth} - The detector azimuth for a given event is the best fit azimuthal direction from the track reconstruction method. A azimuth value of 0$^{\circ}$ corresponds to due East in local grid coordinates.

\textbf{Detector Zenith} - The detector zenith for a given event is the best fit zenith direction from the track reconstruction method. This value ranges from 0$^{\circ}$ to 180$^{\circ}$ with 0$^{\circ}$ representing events traveling downward from the Southern sky above the detector and 180$^{\circ}$ representing events coming upward through the Earth.

\textbf{Reduced Log-likelihood} - The reconstruction method used in this analysis makes use of a likelihood method to obtain a best fit to the event data. The method references a splined table of ice model parameters in addition to probabilities for certain DOMs in the array to generate hits from light emitted by a hypothetical particle track. Low values of reduced log-likelihood (rllh) indicate the track hypothesis has a good fit to the data.

\textbf{Finite Reco Z} - A separate reconstruction makes an attempt to fit a starting and stopping point to the best fit track of the event. This reconstruction also uses a likelihood method to arrive at an optimized solution. Finite Reco Z is the location of the starting vertex of the event in the detector z-coordinate. Events that start at lower depths in the ice are more likely to be upgoing neutrinos as opposed to downgoing atmospheric muons.
\end{adjustwidth}
The plotted distributions are also separated by whether they belong to the high-energy event selection stream (HES) or the low-energy stream (LES). This allows one to compare the data-simulation agreement between the two branches. Additionally, the simulation sets used for development of both branches are not the same which can lead to differing levels of agreement at the final level.

\begin{figure}[0.05]\label{fig:FL_nch_by_energy}
\centering
\subfigure[LES Nch-Energy Relation]{\label{fig:i}\includegraphics[width=75mm]{FinalLevel_Nch_Vs_Energy_NuMu_LESOnly.png}}
\subfigure[HES Nch-Energy Relation]{\label{fig:ii}\includegraphics[width=75mm]{FinalLevel_Nch_Vs_Energy_NuMu_HESOnly.png}}
\caption{NCh-Energy relation for simulated $\nu_\mu$ neutrino events at the final level. Nch is a shorthand term for the number of DOMs receiving light from an event. Subfigure (a) shows the correlation for events belonging to the Low-Energy Stream (LES) branch of the event selection while (b) shows the same for events belonging to the High-Energy Stream or HES branch.} 
\end{figure}

\begin{figure}\label{fig:final_level_distros}
\centering
\subfigure[Reconstructed Event Azimuth]{\label{fig:reco_azi}\includegraphics[width=75mm]{LES_SplineMPE_AzimuthDist_FinalLevelDist.png}}
\subfigure[Reconstructed Event Zenith]{\label{fig:reco_zen}\includegraphics[width=75mm]{LES_SplineMPE_ZenithDist_FinalLevelDist.png}}
\subfigure[Reconstruction Reduced Log-likelihood]{\label{fig:reco_rlogl}\includegraphics[width=75mm]{LES_BDTParam_SplineMPEMod_RLogL_FinalLevelDist.png}}
\subfigure[Reconstructed Event Vertex Detector Z-position]{\label{fig:reco_z}\includegraphics[width=75mm]{LES_BDTParam_FiniteReco_Z_FinalLevelDist.png}}
\caption{Distributions of  for simulation and real data at the final event level. Plots above for the Low-Energy Stream (\textbf{LES}) branch of the event sample.} 
\end{figure}

\begin{postliminary}
\bibliography{jdthesis}{}

\begin{vita}

\end{vita}
\end{postliminary}
\end{document}
