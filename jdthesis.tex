\documentclass{gatech-thesis}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{wrapfig}
\usepackage{changepage}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{url}
\usepackage{array}

\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

%%
%% This example is adapted from ucthesis.tex, a part of the
%% UCTHESIS class package...
%%
\title{Search for Neutrino Transients Using IceCube and DeepCore}
\author{Jacob D. Daughhetee}
\principaladviser{Dr. Ignacio Taboada}
\committeechair{Dr. Pablo Laguna}
\firstreader{Dr. A. Nepomuk Otte}
\secondreader{Dr. Sven Simon}[School of Earth and Atmospheric Science]
\thirdreader{Dr. John Wise}
%%\fourthreader{Professor }
\department{School of Physics}
\degree{Doctor of Philosophy}
\copyrighttrue
\copyrightyear{2015}
\submitdate{May 2015}
\approveddate{March 5, 2015}
\bibfiles{jdthesis}
%% The following are the defaults
%%    \titlepagetrue
%%    \signaturepagetrue
%%    \copyrightfalse
%%    \figurespagetrue
%%    \tablespagetrue
%%    \contentspagetrue
%%    \dedicationheadingfalse
\bibpagetrue
%%    \thesisproposalfalse
%%    \strictmarginstrue
\begin{document}
\bibliographystyle{gatech-thesis}
%%
\begin{preliminary}

\begin{dedication}
\null\vfil
{\large
\begin{center}
To all the unknown fluxes lurking below the limits
\end{center}}
\vfil\null
\end{dedication}

\begin{preface}

	This dissertation is based on data acquired with the IceCube Neutrino Observatory whose maintenance and operation is the result of an immense international collaborative effort. Due to the scale of the detector and the large variety of physics goals, it can be difficult to discern which aspects of an analysis are developed independently by the analyzer and which are the result of the efforts of other collaborators. The bulk of the work pertaining to experimental hardware, data acquisition, reconstruction algorithms, and simulation presented in this document can be attributed to the large contingent of IceCube collaborators. However, the development and refinement of the event selection starting at trigger level and subsequent analysis of the data are the original work of the author.

	Previous IceCube searches for astrophysical sources are primarily sensitive above a few hundred GeV with a focus on muon neutrino events having $\geq$1 TeV in energy. The Super Kamiokande experiment, the largest GeV scale neutrino detector, has also engaged in searches for astrophysical neutrinos, however these searches are generally limited to neutrino events at or below 10 GeV due to its size. Thus, the analysis presented in this document represents a first attempt at a search for short transient neutrino emission from astrophysical sources in the 30-300 GeV energy range. This is done through examination of an entirely new event sample focused on sub-TeV muon neutrino events obtained via selection criteria chosen to maximize sensitivity to soft-spectra transients. A likelihood analysis technique is applied to a final sample consisting of 22,040 events acquired between the dates of May 15th, 2012 and April, 30th 2013 to search for any temporal and spatial clustering indicative of a transient astrophysical neutrino source. A scan over the northern sky ($5^{\circ} < \delta < 90^{\circ}$) found a hottest spot at a right ascension of 268.75$^{\circ}$ and a declination of 54.25$^{\circ}$. After correcting for trials factors, the p-value of this result is found to be 56$\%$ and therefore completely consistent with a purely background dataset.
	
We examine what this null result implies for neutrino emission from a potential class of source object referred to as a choked gamma-ray bursts, a type of core-collapse supernovae that is capable of high-energy neutrino production in relativistic jets. The expected neutrino fluence at Earth for a wide range of jet model parameter values is calculated to determine how well the analysis result constrains the model. Ultimately, we find that the method is only sensitive to particularly energetic choices of jet parameters at distances beyond a few Mpc from Earth. However, the sensitivity of the method can be expected to improve considerably with future improvements in neutrino event reconstruction methods. If these improvements are made, the presented analysis method will begin to probe more likely values of choked GRB neutrino emission model parameters.

Although we present the results of this analysis in terms of its implications for a specific choked GRB neutrino emission model, there are many other possible ways in which this newly developed event sample could be explored. This includes the comparison of the data to known source lists whether those sources be gamma-ray bursts, active galactic nuclei, or supernovae. In particular, certain models of sub-photospheric neutrino emission from GRBs predict a flux of neutrinos precisely in the energy range of interest for this new event selection. This model could also be tested with a few modifications to the existing analysis method. Application of this sample to searches targeting other possible sources will allow for the confirmation or refutation of predicted transient neutrino emission.

A salient point of this work is that an entirely new neutrino event data set of 30-300 GeV muon neutrinos is now available for study. This sample yields roughly an order of magnitude increase in the effective area of IceCube for neutrinos below 100 GeV allowing the detector to search for astrophysical neutrino emission in a relatively unexplored region. Incorporation of this event sample into traditional IceCube searches will significantly enhance the detector's capabilities at lower energies. Additionally, further development of the event selection techniques and reconstruction methods will yield marked improvement in the performance of all analyses that make use of these currently under-utilized neutrino events.

\end{preface}

\begin{acknowledgements}
I want to thank my wife Leah and cat Laika who provided plenty of moral support. I would also like to acknowledge my fellow graduate student office mates whose constant distractions helped me retain my sanity.
\end{acknowledgements}
% print table of contents, figures and tables here.
\contents
% if you need a "List of Symbols or Abbreviations" look into
% gatech-thesis-gloss.sty.

\begin{summary}

% Long Comments
\long\def\/*#1*/{}
\/*
*/

Observations indicate that there is a correlation between long duration gamma-ray bursts (GRBs) and core-collapse supernovae (SNe).  The leading model for GRB production assumes that relativistic jets are generated by the core-collapse within the progenitor star.  Charged particles undergo Fermi-acceleration within internal shocks of these jets and subsequently give rise to gamma ray emission once the jets breach the surrounding stellar envelope.  Very few SNe result in the occurrence of GRBs, however, it has been suggested that a significant fraction of core-collapse SNe manage to produce mildly relativistic jets.  These jets are insufficiently energetic to break through the envelope and are effectively 'choked' resulting in a lack of observed gamma ray emission.  In both the failed and successful GRB scenario, neutrino production can occur if protons are accelerated in the internal shocks of these jets.  These neutrinos may be detectable by the IceCube neutrino observatory and its low energy extension DeepCore. This thesis presents the methods and results of a dedicated search for temporal and spatial clustering of neutrino events during the IceCube 2012 data season. Examination of 22,040 neutrino event candidates acquired over a detector livetime of 330 days revealed no statistically significant transient source of neutrino emission. Limits on the rate of choked GRBs in the nearby universe for possible values of neutrino emission model parameters are presented.


\end{summary}

\end{preliminary}
%%
\chapter{Introduction}
The expansion of traditional optical astronomy into wavelengths unobservable to the human eye revealed myriad phenomena previously unknown to science. Use of wavebands of light spanning several orders of magnitude allowed for the discovery of completely new astronomical sources. Additionally, it allowed for the study of inherently different physical processes within and around source objects. Yet, for all the vast advances in our understanding of the universe the opening up of the electromagnetic spectrum has brought us, it relies entirely upon the physical properties of its messenger particle, the photon.

Absorption of light, either by intervening matter or other background photons, limits the number and type of source objects optical astronomy can hope to either observe or characterize. In order to explore regions of high density as well as very high-energy processes, entirely different methods of observation are required. The limitations imposed by light-based astronomy have led to the dedicated investigation of other particles and phenomena as potential cosmic messengers. This rapidly developing field, often referred to as multi-messenger astronomy, attempts to explore physical regions inaccessible to standard astronomy through the use of the highest energy cosmic rays, gravitational radiation, and high-energy neutrinos. These channels provide a unique window into the universe albeit each with their own detection challenges.

The neutrino in particular provides many excellent properties for potential use as an astrophysical messenger. Due to its very low probability of interaction, it is able to provide information from some of the densest regions within the interiors of sources. Additionally, neutrinos are able to stream freely as they propagate from their origin without suffering absorption in intervening matter. Therefore any successfully extracted neutrino signal would provide unperturbed information about the physics of the source. These characteristics also make neutrinos exceptionally difficult to detect. Nonetheless, the possible insight into high-energy astrophysics neutrinos can provide has spurred the development of large-scale detectors sensitive to expected astrophysical fluxes.

One such detector, the IceCube Neutrino Observatory \cite{2006APh....26..155I}, was constructed specifically to search for high-energy neutrinos ($\geq$1 TeV) of astrophysical origin. The experiment has taken data continuously since 2005 in both partial (2005-2011) and fully constructed (2011-present) configurations. As the experiment has matured, many different types of analysis methods have been developed to look for neutrino signatures from specific astrophysical sources such as Gamma-ray Bursts (GRBs), Active Galactic Nuclei (AGN), and diffuse fluxes of neutrinos. These analyses have primarily focused on energetic ($\geq$1 TeV) muon tracks produced by muon neutrinos interacting within or outside of the detection volume. With the advent of DeepCore\cite{2012APh....35..615A}, the energy threshold of the combined detector has been lowered significantly allowing for the detection of neutrino events as low as 10 GeV. While the addition of DeepCore has already shown to be immensely useful for both the observation of neutrino oscillations and the sensitivity of indirect dark matter searches, there has been little incorporation of lower energy muon neutrino events into the traditional IceCube transient and steady-state point source searches.

Although the effective area of the IceCube-DeepCore detector at sub-TeV energies is significantly reduced, the use of traditional analysis methods on these lower energy events can provide an additional probe for possible neutrino sources in a lower energy regime. In the event that nearby neutrino sources are characterized by either soft-spectra or an energy cutoff, an analysis optimized to make use of low energy events can potentially be more sensitive than analyses using traditional IceCube data sample. The use of lower energy events is not without its drawbacks, however. The accuracy of directional reconstructions in IceCube depends heavily upon the number of light sensors that register photons from a given neutrino event. How many sensors are triggered is directly related to the energy deposited by the interacting neutrino resulting in lower energy events suffering from much worse resolution on average. One of the nearly irreducible backgrounds in IceCube analyses, the flux of atmospheric neutrinos, is strongly energy dependent as well. Due to the very steep spectrum of these neutrinos produced in cosmic ray air showers, any soft-spectrum astrophysical steady source will be exceedingly difficult to parse out from background.

When these difficulties are taken into consideration, it becomes readily apparent that an analysis designed to search for transient astrophysical neutrino sources is the optimal way to use a low energy muon neutrino sample in IceCube. Some potential sources include gamma-ray bursts (GRBs) \cite{2014arXiv1410.0679K}, core-collapse supernovae (SNe) with accompanying jets \cite{2004PhRvL..93r1101R}, and active galactic nuclei (AGN) \cite{2009APh....31..138B}. Whether these astrophysical events are accompanied by a substantial flux of energetic neutrinos is still unknown. Theorist predictions of neutrino emission from these sources indicate that a dedicated IceCube transient search may be sensitive for some values of model parameters. The aforementioned SNe harboring energetic jets are one of the more promising possible sources of neutrino emission. In a similar fashion to GRBs, these events produce neutrinos via internal collisions within the jet. However, unlike GRBs, the jets from these progenitors are insufficiently energetic to break out of the surrounding stellar envelope and are effectively `choked' off preventing the escape of gamma-rays. Due to the lack of strong gamma-ray emission, it is unknown what fraction of core-collapse SNe have jets, but it is likely that the rate of these events is much higher than that of fully-fledged GRBs.

The work presented in this thesis details the development and results of an IceCube-DeepCore analysis optimized to search for transient soft-spectra neutrino sources with a specific focus on neutrino emission from potential choked GRBs. This is accomplished through the modification and application of previously developed time-dependent point source search techniques to a set of neutrino events characteristically lower in energy than the typical IceCube sample. The following sections of this document will describe the acquisition of the data for this analysis, the selection criteria imposed on that data, and the analysis methods applied on the final set of events. In addition, pertinent background information pertaining to neutrino physics and methods in neutrino astronomy will be provided. Neutrino emission models for multiple source classes will also be discussed with a focus on the predicted neutrino emission from choked GRBs. Lastly, the details of the analysis result will be interpreted in light of its implications on the possible values of choked GRB model parameters.

\chapter{Neutrino Properties}
The neutrino is an electrically neutral particle that interacts with matter via the weak nuclear force and gravity. Its cross-section for interaction with ordinary matter is exceedingly small making the experimental detection of the neutrino a difficult task. It is classified as a lepton in the Standard Model, meaning that it is an elementary particle with $\frac{1}{2}$-integer spin (a fermion) and no strong force interaction. Neutrinos come in three variations with one corresponding to each of the three charged leptons present in the standard model: the electron, the muon, and the tau. The neutrino is the lightest of the  elementary fermions by a wide margin, and though observations indicate neutrinos are massive, currently only upper limits on the mass of individual neutrino types are known.

%% History
The first evidence for the existence of the neutrino came through the observation of the energy distribution of electrons emitted by nuclei undergoing $\beta$-decay. In 1930, Wolfgang Pauli postulated the existence of a light, electrically neutral particle as a solution to the problem of missing energy and momentum \cite{PauliLetter}. The idea was not unanimously well-received as he was ultimately suggesting a nigh impossible to detect particle as the solution to the conundrum. However, the idea of Pauli's hypothetical particle seemed much more palatable than the troubling alternative that energy or momentum may not be universally conserved. Confirmation of the existence of this proposed particle would not occur until 1956 through the detection of anti-electron neutrinos streaming from the nuclear fission reactors of the Savannah River Plant \cite{1956Sci...124..103C}.

Despite its discovery nearly six decades ago, a complete understanding of neutrino properties still eludes the physics community. The results of additional neutrino detection experiments revealed that neutrinos held some unsuspected peculiar properties. The most notable of these experiments was the detection of the flux of solar neutrinos via inverse beta decay in Homestake Mine by Ray Davis \cite{PhysRevLett.20.1205}. The experiment only measured one third of the expected neutrino flux predicted by the solar model. The conflict would eventually be resolved when it was determined that neutrinos undergo flavor oscillations as they propagate. This specific deviation of neutrino behavior from theoretical predictions in some sense encapsulates the trend of neutrino behavior defying expectations. As of today, many important properties of the neutrino are still undetermined such as the absolute value of the mass eigenstates, the possibility of the neutrino being its own antiparticle, ordering of the mass eigenstates (hierarchy problem), number of neutrino types, and the possibility of CP violation during oscillations. 

While these issues are extremely interesting in their own right, this section will only attempt describe the neutrino properties relevant to the analysis being presented. Thus, the primary topic of discussion will be the interaction modes of high energy neutrinos. In addition, the phenomenon of neutrino flavor oscillations will also be described in brief due to the important role flavor composition at Earth of a given source flux plays in the capability of detection by IceCube.

\section{Interactions in Matter}
One of the defining characteristics of the neutrino is the ability to stream through large distances of ordinary matter unperturbed. However, neutrinos will occasionally collide with normal matter through several different interactions of varying complexity. The energy of the neutrino and the composition of the target material determine the most likely mode of interaction. With that in mind, this section will only attempt to detail the neutrino-matter interaction of greatest importance in IceCube, i.e. the scattering of a neutrino with a target nucleon. While many other interaction processes can also occur (neutrino-electron scattering, inverse beta decay, coherent scattering with nuclei, etc.), the cross-section for these processes is far smaller than neutrino-nucleon scattering for neutrinos with energies of interest to IceCube and DeepCore ($E_{\nu} \geq$ 10 GeV).

\subsection{Neutrino-Nucleon Scattering}

The dominant mode of interaction for neutrinos detected by IceCube is through scattering with a nucleon of a hydrogen or oxygen atom within the ice. This interaction is carried out via the weak nuclear force and involves the exchange of either a $W^{+}$, $W^{-}$, or Z boson between the neutrino and the nucleon . Interactions in which a charged W boson is exchanged are referred to as charged-current (CC) and have the following form:
\begin{eqnarray}
\nu_{l} + N \rightarrow l + X
\end{eqnarray}
A neutrino of flavor $l$ will yield its counterpart charged lepton while the final state $X$ of nucleon $N$ will be dependent on the magnitude of the momentum exchange in the interaction and can range from ejection of a nucleon from the nucleus to total breakup and a particle shower. Neutral-current (NC) interactions occur via exchange of the chargeless Z boson and as such yield a different end state:
\begin{eqnarray}
\nu_{l} + N \rightarrow \nu_{l}' + X
\end{eqnarray}
Unlike CC interactions, only the energy imparted to the target nucleus will be observable as the outgoing neutrino $\nu_{l}'$ will carry some fraction of the energy. As is the case in the CC scenario, the end state for the nucleon in NC interactions will vary according to the energy deposited by the neutrino.

How deeply these interactions probe the internal structure of the target nucleon is ultimately dependent on energy of the incoming neutrino. For neutrinos below 20 GeV in energy, the exact manner in which the neutrino interacts with the target nucleon can be quite complicated as there are three possible scattering interaction modes. These modes include quasi-elastic scattering, resonance production, and deep inelastic scattering \cite{2012RvMP...84.1307F}. Nearly all interaction products from neutrino events detected by IceCube are the result of deep inelastic scattering. However, other modes of interaction can be much more prevalent in a DeepCore sample as the energy threshold for detection in DeepCore extends down to approximately 10 GeV. Figure \ref{fig:neutrino_scattering} shows the relative size of the cross-section for each of the aforementioned processes as a function of neutrino energy. The higher number of possible end states provided by quasi-elastic and resonance scattering make simulation and proper estimation of the cross-section for neutrino events in 1-20 GeV energy range a fairly complex ordeal. Quasi-elastic scattering and resonance production will only be briefly described as very few of the neutrino events present in the final event sample of the presented analysis interacted through either of these means.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.5\textwidth,keepaspectratio]{neutrino_nucelon_crosssections.png}
  \end{center}
  \caption[Neutrino-nucleon Cross-section by Energy]{Total inclusive cross-section is plotted above as a function of energy along with individual cross-sections for different interaction channels. Above a few tens of GeV, it is clear that deep inelastic scattering begins to dominate \cite{2012RvMP...84.1307F}.}
  \label{fig:neutrino_scattering}
\end{figure}

In the elastic (NC) or quasi-elastic (CC) scenario, an incoming neutrino will scatter off of the whole composite nucleon imparting some of its energy to the target. This is usually sufficient to eject the target nucleon and in some cases additional nucleons from the nucleus. The CC scattering is referred to as quasi-elastic due to the fact that it requires the conversion of the target proton(neutron) into a neutron(proton) to ensure charge conservation. This type of scattering does not probe the internal constituents of the nucleus (quarks and gluons collectively referred to as partons). The absolute cross-section, however, will depend heavily on nuclear properties of the target atom. Examination of Figure \ref{fig:neutrino_scattering} shows that even in a DeepCore dominated event sample this process will very rarely occur given the threshold for neutrino detection is approximately 10 GeV.

Above 1 GeV in neutrino energy, resonance production overtakes quasi-elastic scattering in relative contribution to the total interaction cross-section. In this form of inelastic scattering, the momentum transfer provided by the neutrino collision can excite the nucleon target. The decay of this excited state will generally involve the emission of an energetic meson (a particle consisting of a quark-anti-quark pair). At lower energies the emitted meson will usually be a single pion, however at higher energies multiple pions can be produced as well as strange mesons such as the kaon. An example muon neutrino resonance interaction takes the following form:

\begin{eqnarray}
\nu_{\mu} + N \rightarrow \mu^{-} + N^{*} \rightarrow \mu^{-} + N + \pi^{+}
\end{eqnarray}
In the scenario described above, the exited nucleon \textit{N} decays and emits a charged pion. Other end states yielding either charged or neutral pions are possible as well for either CC or NC interactions. An additional possibility is coherent scattering with the entirety of the nucleus. The momentum transfer in this case is rather low and results in little nuclear recoil and a heavily forward-scattered pion.

As the momentum transfer in the interaction increases, more exotic resonances can arise which in turn can result in the production of multiple pions or heavier mesons. The large number of possible end states for these interactions makes proper simulation of the process complicated. This can be significant for specialized analyses focusing on the lowest energy events in DeepCore as neutrinos interacting via resonance production actually make a sizable contribution to the event sample. What is ultimately important, however, is that the total light yield from these interactions is estimated correctly by the simulation used for analysis purposes. While elastic scattering and resonance production are significant for the lowest energy analyses or possible future low energy extensions for IceCube, these processes only add a minor contribution to the event sample used in the analysis presented in this thesis.

\subsection{Deep Inelastic Scattering}
The most important neutrino-matter interaction for IceCube is deep inelastic scattering. As Figure \ref{fig:neutrino_scattering} shows, it is far and away the dominant mode of interaction for neutrinos above the threshold energy for detection in IceCube ($\sim$100 GeV). At these energies, the wavelength of the incoming neutrino becomes sufficiently small enough to begin probing the internal parton structure of the target proton or neutron. The energy imparted to these internal components will break the nucleon apart. This breakup will result in the formation of a shower of short-lived hadrons (any particle composed of quarks) which will decay into more stable particles. The generation of these hadronic cascades during nucleon breakup is often referred to as hadronization of the nucleon's constituent quarks, and the formation of these particles is necessitated by quark confinement, a property of the strong nuclear interaction. The particles within this shower will suffer energy losses through either collisions or ionization of surrounding material until the energy is fully dissipated. A diagram detailing the deep inelastic scattering process for a muon neutrino is shown in Figure \ref{fig:dis_scattering}.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.7\textwidth,keepaspectratio]{dis.png}
  \end{center}
  \caption[Deep Inelastic Scattering Feynman Diagram]{Feynman diagram of a $\nu_{\mu}$ undergoing deep inelastic scattering with a nucleon via charged-current interaction. A large momentum transfer $q$ via exchange of a charged $W$ boson leads to breakup of the nucleon \cite{2012RvMP...84.1307F}.}
  \label{fig:dis_scattering}
\end{figure}

Like the scattering modes mentioned earlier, deep inelastic scattering can occur through either a CC or NC channel. The hadronic shower will occur regardless of the interaction channel, while an end-state lepton will only be present in the CC scenario. Neutrino experiments can infer the properties of the interacting neutrino through examination of the end state products. The three possible leptons (e$^-$, $\mu$, $\tau$) produced in CC interactions each exhibit different behavior as they interact in the detection medium allowing for flavor identification of the original neutrino. In the case of NC scattering, however, only the hadronic cascade is observable by the experiment. This not only prevents flavor identification of the neutrino, but it also makes estimation of the neutrino energy very difficult as some fraction of the total energy is carried away by an outgoing neutrino.

\begin{figure}
\centering
\subfigure[Neutrino DIS Cross-Section]{\label{fig:NeutrinoDIS_CS}\includegraphics[width=67mm]{DIS_CS_Neutrino.png}}
\subfigure[Anti-Neutrino DIS Cross-Section]{\label{fig:AntiNeutrinoDIS_CS}\includegraphics[width=70mm]{DIS_CS_AntiNeutrino.png}}
\caption[(Anti-)Neutrino Deep Inelastic Scattering Cross-Section]{Charged current (thin), neutral current (dashed), and total inclusive (solid) deep inelastic scattering cross-section as a function of neutrino (a) and anti-neutrino (b) energy \cite{Gandhi:1998ri}}
\label{fig:NeutrinoDIS_CrossSection}
\end{figure}

The cross-section for deep inelastic scattering increases with increasing neutrino energy as Figure \ref{fig:NeutrinoDIS_CrossSection} shows. While this increasing cross-section is generally favorable for detection purposes due to higher probability to interact in neutrino telescopes, a larger interaction cross-section at higher energies also leads to absorption of neutrinos within the Earth. Absorption is fairly infrequent for neutrinos below 10 TeV, but it becomes increasingly relevant for astrophysical neutrinos at the PeV or EeV scale \cite{2013arXiv1304.4891K}.

\subsection{Propagation of Interaction Products}
The products leftover from high-energy neutrino-nucleon interactions are generally very energetic as well. These particles will subsequently undergo many forms of energy loss and possibly decay as they travel through a detection medium. In a given interaction, the total light yield and its spatial extent are contingent on the products of the interaction. The products themselves are ultimately determined by both the flavor of the primary neutrino and whether the neutrino interaction was of the charged- or neutral-current variety. This section will detail the important energy loss mechanisms for interaction products and how these losses produce an observable signal in the IceCube detector. There are many modes of interaction for charged particles in materials, some of which are highly energy dependent and stochastic in nature. Because IceCube was designed specifically to detect and track high energy muons, the description of energy loss mechanisms will primarily focus on how these mechanisms pertain to muons. The same interactions will occur for other charged particles produced in neutrino interactions, however, these particles fail under normal circumstances to produce tracks long enough to be resolved by IceCube.

The primary energy loss mechanisms for charged leptons are ionization, bremsstrahlung, pair-production, and photo-nuclear interactions \cite{2001PhRvD..63i4020I}. These losses are described below:

\begin{adjustwidth}{2.5em}{2.5em}
\setlength{\parindent}{0pt}
\textbf{Ionization} - Ionization energy losses occur as the charged particle transfers energy to the electrons of intervening atoms and molecules. This energy loss is fairly continuous and is the dominant loss mode for sub-TeV muons in the ice \cite{2001ADNDT..78..183G}.

\textbf{Bremsstrahlung} - Energy losses via bremsstrahlung involve the deflection of the particle by another charged particle (typically a positively charged nucleus). This deflection results in the emission of a photon with energy equivalent to the kinetic energy lost in the interaction. These photons can often be energetic enough to produce a electromagnetic shower through electron-positron pair-production.

\textbf{Pair-production} - As the name might suggest, pair-production is the process in which an electron-positron pair is produced in the Coulomb field of an atomic nucleus \cite{2001ADNDT..78..183G}. If the energy imparted to the electron-positron pair is large enough, an electromagnetic shower can be produced. The process was originally investigated in detail by Bethe and Heitler \cite{BetheHeitler}.

\textbf{Photo-nuclear} - At high energies, charged leptons can undergo inelastic scattering with nuclei and in the process create hadron showers \cite{2003PhRvD..67c4027B}. The process is similar to the neutrino-nucleon scattering processes described earlier but with photon-exchange taking place rather than $W$ or $Z$ boson exchange.
\end{adjustwidth}
\setlength{\parindent}{17.5pt}
The relative contribution to total energy loss these interactions provide will vary with particle energy. The stopping power provided by these energy loss mechanisms is commonly parameterized so that the average range of the particle in a given medium can be more easily computed \cite{2001ADNDT..78..183G}:
\begin{equation}\label{eq:stopping_power}
-\frac{dE}{dX} = a(E) + b(E) E
\end{equation}
Here $a(E)$ represents the more or less continuous energy losses from ionization (electron stopping power) while the $b(E)$ term includes the contributions from the various radiative energy loss processes (bremsstrahlung, pair-production, and photo-nuclear). The units of stopping power $-\frac{dE}{dx}$ are MeV$\cdot$cm$^2 \cdot$g$^{-1}$. The relative strength of these processes with respect to stopping power can be seen in Figure \ref{fig:MuonStoppingPower}. By integrating the stopping power from Eq. \ref{eq:stopping_power}, we arrive at the following expression for particle range
\begin{equation}\label{eq:range}
R(E) = \int_{E_0}^{E}[a(E') + b(E')E']dE'
\end{equation}
with $E_0$ being the final energy of the particle \cite{2001ADNDT..78..183G}.
By assuming $a$ and $b$ are roughly constant in $E'$, Eq. \ref{eq:range} can be simplified into a expression yielding particle range from just the initial energy \cite{2001ADNDT..78..183G}:
\begin{equation}\label{eq:range_approx}
R(E) \approx (1/b)ln(1+\frac{E}{E_{c}})
\end{equation}
The term $E_{c}$ or critical energy is given by $E_{c}=\frac{a}{b}$ and it is the energy at which radiative and ionization energy losses become equivalent. The above approximation for particle range can be obtained from the assumption that the initial energy $E$ is very large and that energy losses are continuous. These assumptions must neglect the stochastic nature of many of these energy losses and therefore any range derived this way is more representative of an average range as the ranges of individual particles are subject to large fluctuations.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.9\textwidth,keepaspectratio]{MuonStoppingPower.png}
  \end{center}
  \caption[Muon Stopping Power]{Total stopping power (solid line) of muons in copper as a function of kinetic energy. Contributions from individual processes are shown as well as the energy ranges in which these processes are most dominant. \cite{2001ADNDT..78..183G}.}
  \label{fig:MuonStoppingPower}
\end{figure}

The range equation from Eq. \ref{eq:range_approx} can be used to estimate the range of an energetic muon in IceCube. Using a muon critical energy value of $E_{c}=675$ GeV in water and a value of $b=2.959 \cdot 10^{-6}$ g$^{-1}$cm$^{2}$ for radiative losses, we find a range of approximately 3 km for a 1 TeV muon (values for $E_{c}$ and $b$ obtained from \cite{2001ADNDT..78..183G}). The most important thing to note is that these losses determine both the brightness and length of muon tracks within the IceCube detector and ultimately how well their direction can be resolved. The physical dimensions of the IceCube detector were designed with the range of these muon tracks in mind. Interestingly, the effective area of the IceCube detector is greatly enhanced due to the ability of energetic muons to penetrate so deeply in matter.

\subsection{Cherenkov Emission}
While the previously described processes are most relevant for particle energy loss, the most import energy loss mechanism for particle detection in IceCube is the production of Cherenkov light. Cherenkov emission only makes a small contribution to total energy loss by charged particles in materials. It does however produce light in the visible spectrum yielding an easily detectable signal. This emission occurs when a charged particle travels through a dielectric medium at a greater speed than the phase velocity of light in that medium. The particle will polarize the surrounding material as it travels, however the particle will have moved on before enough time has elapsed for the material to relax resulting in a build up of wave fronts. A shock front is produced that takes the shape of a cone about the particle track. This light cone has an opening angle given by
\begin{equation}
\cos{\theta}=\frac{1}{n\beta}
\end{equation}
where $n$ is the index of refraction for the material and $\beta$ is the ratio of the particle velocity and the speed of light $c$. A diagram of Cherenkov light cone produced by a muon is shown in Figure \ref{fig:CherenkovDiagram}. If one takes a typical value for the refractive index in ice ($n=1.31$), an angle of roughly $40^{\circ}$ is obtained. Resolution of the Cherenkov cone of a single particle is a powerful particle identification technique for many particle detectors. However, only the Cherenkov light fronts of the most energetic and long-lived particle tracks are able to be resolved in IceCube due to the large separation distances between individual light sensors.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.65\textwidth,keepaspectratio]{CherenkovCartoon.png}
  \end{center}
  \caption[Diagram of Cherenkov Light Cone]{Diagram of a Cherenkov light cone produced by a muon traveling at speed $v=\beta c$ through a transparent medium with index of refraction $n$. The blue circles show points of light emission along the particle track. Since the muon travels faster than light in this medium, the light emission forms a shock-front analogous to a sonic boom.}
  \label{fig:CherenkovDiagram}
\end{figure}

While they may lack the long propagation lengths of muons, all other charged particles produced in neutrino-nucleon scattering interactions in the ice will also emit Cherenkov radiation as they propagate in the ice. This will generate a pool of light centered at the neutrino interaction vertex whose photon count is proportional to the energy of the interaction. Light produced in these particle showers and muon tracks is collected by IceCube via sensitive optical sensors located in the ice. The high scattering and absorption lengths of the glacial ice allow for accurate timing of neutrino interaction times and arrival directions. There are several modes of scattering of photons that can occur, however, which can result in degradation of the Cherenkov signal \cite{2013arXiv1301.5361I}. Detailed information on detector instrumentation, sensor response to the Cherenkov emission, and the topology of neutrino events in IceCube will be given in Chapters V and VI.


\section{Flavor Oscillations}
The process in which the lepton flavor of a given neutrino can change during its propagation is one of the particle's more intriguing properties. The unexpected observation of neutrino oscillations indicated that the neutrino, which was initially thought to be massless under the Standard Model, must actually have some non-zero mass after all. Recently, many experiments have studied this phenomenon with high precision in hopes of accurately determining the mass-splitting of the neutrino eigenstates as well as if neutrinos undergo charge-parity or CP violation. While the analysis being presented is in no way suited to studying the oscillation phenomenon, how the results are interpreted is very much dependent on how the modeled neutrino flavor ratio for a source evolves during travel from its origin to its arrival at Earth. Thus, this section will describe in short detail the oscillation process over astronomical baselines and through the Earth, the scenarios most applicable to IceCube.

The mixing of neutrino states arises due to the fact that the neutrino mass eigenstates do not directly correspond to the neutrino flavor eigenstates \cite{1978PhRvD..17.2369W}. While neutrinos are produced through weak interactions giving them a definite lepton flavor state, the mass of that neutrino is actually a superposition of three separate mass states. Likewise, the mass state of the neutrino is given by a superposition of the three lepton states. The phase of the mass states will evolve differently during propagation according to the mass differences between the mass eigenstates. Thus, when the neutrino interacts with matter again via the weak force it may be observed in a flavor state other than the its state during production. This mixing possibility can be described by a mixing matrix $U$:
\begin{equation}
\nu_{\alpha} = \sum_{i=1}^{3} U_{\alpha i} \nu_{i}
\end{equation}
where $\nu_{\alpha}$ are the lepton states ($e^-,\mu,\tau$) and $\nu_i$ are the mass states (1,2,3). Using the notation of Nunokawa et. al \cite{2008PrPNP..60..338N}, the matrix $U$ has the form
\begin{equation}
U = \left(\begin{array}{ccc}
1 & 0 & 0 \\
0 & c_{23} & s_{23} \\
0 & -s_{23} & c_{23}
\end{array}\right)
\left(\begin{array}{ccc}
c_{13} & 0 & s_{13}e^{i\delta} \\
0 & 1 & 0 \\
-s_{13}e^{i\delta} & 0 & c_{13}
\end{array}\right)
\left(\begin{array}{ccc}
c_{12} & s_{12} & 0 \\
-s_{12} & c_{12} & 0 \\
0 & 0 & 1
\end{array}\right)
\end{equation}
\begin{equation}
=
\left(\begin{array}{ccc}
c_{12}c_{13} & s_{12}c_{13} & s_{13}e^{i\delta} \\
-s_{12}c_{23}-c_{12}s_{23}s_{13}e^{i\delta} & c_{12}c_{23}-s_{12}s_{23}s_{13}e^{i\delta} & s_{23}c_{13}\\
s_{12}s_{23}-c_{12}c_{23}s_{13}e^{i\delta} & -c_{12}s_{23}-s_{12}c_{23}s_{13}e^{i\delta} & c_{23}c_{13} \\
\end{array}\right)
\end{equation}
In the above, $c_{ij}$ and $s_{ij}$ are shorthand notation for cos $ \theta_{ij}$ and sin $\theta_{ij}$ respectively where $\theta_{ij}$ is the mixing angle between states while $\delta$ is a phase term related to CP violation. By taking the inner product of the two states after application of the mixing matrix and applying the time evolution operator, one arrives at Eq. \ref{eq:oscprob} which gives the probability of transition of a neutrino of flavor $\alpha$ to one of flavor $\beta$ during propagation \cite{2008PrPNP..60..338N}.
\begin{equation}\label{eq:oscprob}
P(\nu_{\alpha}\rightarrow \nu_{\beta}) = \left| \sum_{i} U_{\alpha i}^* U_{\beta i} e^{-i\frac{m_{i}^2}{2E}L}\right| ^2
\end{equation}

The full-fledged oscillation probability is complicated, but the most important thing to takeaway from the expression given by Eq. \ref{eq:oscprob} is that the probability of observing the neutrino in a different state depends on the propagation length $L$ and the energy of the particle $E$. In many cases it is possible to only consider a two-flavor oscillation scenario as the third flavor may not participate strongly in the energies and distances being considered. This allows us to reduce the mixing matrix to a two dimensional form making the expression for oscillation probability much simpler \cite{1998PhRvL..81.1562F}:
\begin{equation}\label{eq:2doscprob}
P(\nu_{\alpha}\rightarrow \nu_{\beta}) = sin^22\theta sin^2\left(\frac{1.27\Delta m^2 (eV^2) L (km)}{E_\nu (GeV)}\right)
\end{equation}
This formulation will yield the probability of oscillation provided you know the mixing angle $\theta$ between $\alpha$ and $\beta$ and the mass-squared difference $\Delta m^2$ for the neutrino mass eigenstates. This approximation is commonly used by several experiments (e.g. IceCube \cite{2013PhRvL.111h1801A}) to make measurements on specific mixing angles. Using an accelerator produced neutrino beam with known energy, neutrino detectors can be situated at long distances from the beam source where the oscillation probability given by Eq. \ref{eq:2doscprob} is maximized. These long-baseline experiments provide the most precise measurements of neutrino oscillation parameters. The current best-fit values for these parameters are given in Table \ref{tab:osc_param} \cite{PhysRevD.89.093018}.
\begin{table}[h]
\caption[Best Fit Neutrino Oscillation Parameters]{Best fit values for oscillation mixing angles $\theta_{ij}$ and squared mass differences $\Delta m^{2}$. These values are for the normal mass hierarchy scenario wherein $m_1 < m_2 , m_3$. We define $\Delta m^{2}$ here as $m_{3}^2 - (m_1^2 + m_2^2)/2$ \cite{PhysRevD.89.093018}.\label{tab:osc_param}}
\begin{center}
\begin{tabular}{cc}
\toprule
\textbf{Parameter} &\textbf{ Best Fit}\\
\midrule
sin$^2 \theta_{12}$ & 0.308\\
sin$^2 \theta_{13}$ & 0.0234\\
sin$^2 \theta_{23}$ & 0.437\\
$\delta / \pi$ & 1.39 \\
$\Delta m^2$ & 2.43$\cdot 10^{-3}$ eV\\
\hline
\end{tabular}
\end{center}
\end{table}

The IceCube detector is not uniformly sensitive to all neutrino types. Therefore, it is important to estimate how the flavor ratio of a neutrino flux changes during its propagation from an astrophysical source to Earth. The manner in which a neutrino flux is generated at the source will determine the initial flavor flux ratio. The flavor ratio for a neutrino flux is usually given as a relative ratio of the individual flavor states to the total flux, e.g. (1:2:1) with the numbers in the parenthetical relating to the lepton flavors as ($\nu_e$:$\nu_{\mu}$:$\nu_{\tau}$). Some common neutrino production scenarios include muon-suppressed pion decay, pion and muon decay, and neutron decay which have flavor flux ratios of (0:1:0), (1:2:0), and (1:0:0) respectively. Neutrino production mechanisms will be described in more detail in Chapters 3 and 4, but for now we are just interested in how these flavor ratios evolve during propagation to Earth. 

The effect is visualized in Figure \ref{fig:baseline_osc}. Here we see that the initial ratio of (0:1:0) from muon-suppressed pion decay ultimate arrives at Earth with a ratio of approximately (1:2:2). Three-flavor oscillations tend to average out the ratio between the flavors, but there is still a distinct difference at Earth for the differing production mechanisms. When placing limits on the neutrino emission from a possible source in IceCube analyses, the choice of flavor ratio at Earth will influence the degree to which a flux from that source can be excluded. This is due to $\nu_{\mu}$ neutrinos being preferentially detected. In the event that astrophysical neutrino sources begin to be resolved, high statistics measurement of their flavor ratios should be able to identify what mode of neutrino production is at work at the source.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.75\textwidth,keepaspectratio]{AstroBaselineOsc.png}
  \end{center}
  \caption[Flavor Flux Evolution over Astronomical Baselines]{Neutrino flavor ratio space. Oscillation over astronomical baselines will result in a flavor ratio at Earth different than at source. If the flavor ratio for a given source can be measured it can help identify the process through which that source's neutrinos were produced \cite{2014arXiv1412.5106I}.}
  \label{fig:baseline_osc}
\end{figure}

\chapter{Neutrino Astronomy}
As the name suggests, the field of neutrino astronomy is concerned with the detection of neutrinos of extraterrestrial origin for the use of identifying and classifying astrophysical sources. In a similar fashion to the advances made through expansion of traditional optical astronomy to the high-energy regime of x-rays and gamma-rays, it is thought that the unique window provided by neutrino astronomy can facilitate tremendous increases in our understanding of the universe. However, the detection of astrophysical neutrino events presents a exceedingly difficult challenge with detection of an appreciable number of neutrinos from a single source requiring enormous instrumented volumes. Therefore, it is relevant to discuss the potential scientific rewards neutrino astronomy can provide.

\section{Motivation}
Neutrinos provide a novel and unexplored view into furthest reaches of the universe. The small cross-section for neutrino interactions in matter is somewhat of a double-edged sword. While it makes the detection of neutrinos immensely difficult, it also means that neutrinos will stream unperturbed through even the densest astrophysical media. The cross-section for interaction is still very low at even the highest energies. This allows neutrinos to provide information about processes at energies where other possible messengers, such as cosmic rays or photons, are attenuated significantly over astrophysical distances. The development of neutrino astronomy would therefore complement other high-energy astronomy methods quite nicely, creating a more complete picture of the high-energy universe.

The detection of a neutrino flux from a source and measurement of its spectrum would provide critical insight into the processes governing the densest and most energetic regions of that object. One the current problems in gamma-ray astronomy is determining whether gamma-ray emission from certain sources is the product of leptonic or hadronic acceleration \cite{2013ApJ...768...54B}, \cite{2008A&A...492..695B}. There is often a great deal of leeway in theoretical models so that neither scenario can be ruled out effectively. The detection or non-detection of a neutrino flux would reveal which scenario is at work for the source. The detection of high-energy neutrinos can also provide direct evidence of sources of ultra-high energy cosmic rays (UHECR). Because sites of cosmic ray production are almost guaranteed to be accompanied by neutrino production as well, the identification of high-energy neutrino sources would be a surefire way to identify the production sites for UHECRs \cite{1984ARA&A..22..425H}.

The complementary role neutrino astronomy plays in the wider field of multi-messenger astronomy perhaps best demonstrates the scientific merit of the neutrino as an astronomical tool. For example, the use of high-energy neutrino alerts as triggers for gamma ray telescopes could greatly enhance the ability of those instruments to detect transient phenomena. Additionally, it would allow for single neutrino events to contribute to the discovery of astrophysical sources. The physics potential of low-latency neutrino alert systems is all the more promising in light of the recent discovery by IceCube of a diffuse flux of neutrino events of astrophysical origin \cite{2013Sci...342E...1I}.

The complex phenomenology of high-energy astrophysics offers us many possible signals to use as potential cosmic messengers. The merits and pitfalls of neutrino astronomy have been discussed at length already, but there are many other kinds of emission available as well. Each of these messengers carries its own advantages and disadvantages. They all give a unique view into the high-energy universe, though, which makes coordination of efforts in all of these fields essential. The scientific output for each of these fields is greatly enhanced by development of the others. In this sense, the maturation of neutrino astronomy will not only yield interesting insights on its own, but it will also contribute greatly to the advancement of physics knowledge from gamma ray astronomy, cosmic ray studies, and gravitational wave detection.
\section{Cosmic Ray - Neutrino Connection}
The Earth's atmosphere is constantly bombarded with a flux of high-energy charged particles known as cosmic rays. Discovered in 1912 by Victor Hess \cite{1930NW.....18.1094H}, this flux of extraterrestrial radiation is composed of protons and ionized nuclei. This flux is fit quite well by a power law spectrum with an index $\gamma$ of 2.7 over a staggering eleven orders of magnitude in energy \cite{1990cup..book.....G}. Detection of the highest energy cosmic rays (E $\sim$10$^{20}$ eV) requires extremely large detection areas due to an event rate of about 1 particle per square kilometer per century.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.75\textwidth,keepaspectratio]{CosmicRaySpectrum.jpg}
  \end{center}
  \caption[Cosmic Ray Spectrum]{Cosmic ray energy spectrum at Earth as measured by several experiments \cite{2009PrPNP..63..293B}. The spectrum is relatively featureless over several orders of magnitude. There is an observed slight flattening of the spectrum at about 10$^{18}$ eV. This feature is dubbed the `ankle' and is widely considered to signal a transition between a galactic and extra-galactic component of the cosmic ray spectrum \cite{2012APh....39..129A}.}
  \label{fig:cosmicray_spec}
\end{figure}

Interactions of these cosmic rays with Earth's atmosphere results in the formation of particle showers. The high center of mass energy of the collision between primary cosmic ray and atmospheric target must be dissipated through the creation of many secondary particles. These particles will in turn undergo collisions of their own leading to an exponential increase in particle number. A schematic of one of these showers is shown in Figure \ref{fig:CRSketch}. 
\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.65\textwidth,keepaspectratio]{CRsketch.png}
  \end{center}
  \caption[Cosmic Ray Interaction Diagram]{Schematic diagram of a iron nuclei cosmic ray interacting with a target nitrogen atom in the atmosphere. The development of the shower is demonstrated by the illustration of secondary interactions caused by daughter particles of the initial interaction \cite{HiSPARC}.}
  \label{fig:CRSketch}
\end{figure}

The capabilities of the IceCube detector are heavily affected by the flux of cosmic rays at Earth. Cosmic ray air-showers will produce copious numbers of energetic muons capable of surviving the the trip from shower vertex to the surface of the Earth. The most energetic of these muons can penetrate the 1.5 km ice overburden to trigger the detector. Events of this type represent the dominate background in IceCube. The average trigger rate of the detector on these background cosmic ray muons is approximately 3 kHz. Cosmic ray air-showers also produce an irreducible background in the form of atmospheric neutrinos. Hadronic interactions in the air-shower will result in the production of many meson particles ($\pi$,$K$, etc.) \cite{1995PhR...258..173G}. These short-lived particles will primarily decay into muons and muon neutrinos. Additional neutrinos will be produced during the decay of the muon as well. At low energies, the spectral index for this neutrino flux will follow that of the primary cosmic rays. However, the index will steepen to value of 3.7 for energies above 100 GeV due to cooling of the parent mesons prior to decay \cite{1995PhR...258..173G}.
\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.65\textwidth,keepaspectratio]{atmospheric_nu_spectrum.png}
  \end{center}
  \caption[Atmospheric Neutrino Spectrum]{Plot of the $E^{2}$-weighted atmospheric neutrino spectrum as measured by several experiments. The conventional $\nu_{\mu}$ and $\nu_e$ components of the atmospheric spectrum originate from the $\pi$ and $K$ decay. The so-called prompt neutrino flux is the predicted contribution to the total neutrino flux from the decay of charmed mesons such as the $D$ particle \cite{2012arXiv1204.5379A}.}
  \label{fig:atmo_spec}
\end{figure}
Given the observed energies of cosmic rays at Earth, the production sources for these particles must exhibit physical conditions capable of significant particle acceleration. This requires acceleration regions to be relatively diffuse to prevent thermalization of particles. To determine the source class responsible for cosmic rays at energies indicative of galactic origin, one can examine the event rate, energy deposition, and particle acceleration capabilities of candidate sources. Use of these arguments has shown the environments surrounding supernovae to be the primary contributor to the galactic population of cosmic rays \cite{Ackermann15022013}. 

The origin of the highest energy cosmic rays still remains a mystery, however, and the prevailing notion is that they are likely produced in extragalactic sources. There are two arguments that motivate an extragalactic origin for these particles:
\begin{itemize}
\item 1) The Larmor radius for particles of energies greater than 10$^{20}$ eV in the galactic magnetic field is larger than the size of the galaxy \cite{1984ARA&A..22..425H}. If these particles were produced in the Milky way, their residence time would be quite short as they will not be contained. This suggests that these particles are actually arriving from other galaxies that harbor sources capable of acceleration to ultra-high energies.
\item 2) The lack of observed galactic sources capable of accelerating particles to these highest energies. A galactic origin of these cosmic rays would require the existence of galactic Eevatrons (astrophysical objects that accelerate particles to EeV energies). Explanation of this flux in terms of galactic sources would necessitate the identification of a galactic Eevatron.
\end{itemize}
The fact that these particles are most probably extra-galactic in origin has motivated the suggestion of sources featuring a cosmological distribution as the accelerators with GRBs and AGN regarded as the most likely culprits.

\subsection{Proton Acceleration}
The generation of the highest energy cosmic rays requires some form of non-thermal particle acceleration as these energies are nigh unobtainable in thermal distributions. Regions of acceleration must also not feature very high energy densities as this will allow for effective cooling of the accelerated particles. These stipulations on the possible acceleration regions indicate that observed high-energy cosmic rays likely originate from relatively diffuse plasmas in an environment featuring highly turbulent magnetic fields \cite{1984ARA&A..22..425H}.

The most widely accepted model of the acceleration process responsible for the generation of the cosmic ray flux is first-order Fermi acceleration \cite{1995PhR...258..173G}. This acceleration process requires the formation of a shock between two regions featuring different bulk velocities (see Figure \ref{fig:FA}) \cite{ParticleAcceleration}. Particles on the upstream side of the shock (the region of higher velocity $u_1$) will gain energy as they pass through the shock to the downstream region \cite{PhysRev.75.1169}. Magnetic instabilities in the vicinity of the shock can result in particles being kicked back into the upstream region with a probability given by Eq. \ref{eq:returnprob}.
\begin{equation}\label{eq:returnprob}
P_r(u_2) = \frac{(1-u_2)^2}{(1+2)^2} \approx 1-4u_2
\end{equation}
We obtain the result in Eq. \ref{eq:returnprob} by assuming that the bulk flow velocity in the downstream region $u_2 << 1$. After returning to the upstream region, magnetic fields can deflect the particle across the shock once again for another instance of energy gain. We can see that for small values of $u_2$ we can expect many particle crossings leading to significant particle acceleration.
\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.65\textwidth,keepaspectratio]{FermiAcceleration.png}
  \end{center}
  \caption[Fermi-acceleration Diagram]{A diagram of the first-order Fermi acceleration process. A relativistic shock is formed with the upstream region featuring bulk flow $u_1$ and downstream region with bulk flow $u_2$. Upstream particles traveling  across the shock will gain energy\cite{ParticleAcceleration}.}
  \label{fig:FA}
\end{figure}
The average energy gain during each shock crossing is given by Eq. \ref{eq:shock_the_monkey}
\begin{equation}\label{eq:shock_the_monkey}
E_{(k+1)} = \left(1 + \frac{4}{3}(u_1 - u_2) \right) E_k
\end{equation}
where $u_1$ and $u_2$ are the bulk flow velocities in the upstream and downstream regions and $E_k$ is the energy of the particle after the $k^{th}$ crossing. One can obtain an expected energy distribution for particles experiencing this acceleration process by combining the energy gain per crossing with the probability of return \cite{ParticleAcceleration}. The end result is the power law distribution in Eq. \ref{eq:fermi_plaw}
\begin{equation}\label{eq:fermi_plaw}
N(>E_k) = N_0 \left(\frac{E_k}{E_0}\right)^{-\gamma}
\end{equation}
where $E_k$ is the energy at a specified k$^{th}$ crossing, $N$ is the number of particles that achieve that energy or greater, $N_0$ is the number of particles in the distribution, $E_0$ is the initial particle energy, and $\gamma$ is the spectral index whose value is determined by the ratio of the upstream and downstream flow velocities like so $\gamma = \frac{3}{u_1/u_2-1}$.

In the strong shock limit limit ($u_1 >> u_2$), the  post-acceleration proton distribution will have the form of an $E^{-2}$ power law. The relativistic shocks present in sources of cosmic ray acceleration are generally assumed to be quite strong, and so the initial proton spectrum in cosmic ray accelerators is assumed to be $E^{-2}$. This proton distribution may be subject to cooling mechanisms during its time spent in the acceleration region, however. If dense radiation fields are present, then significant cooling of the protons can occur through photo-hadronic interactions \cite{1984ARA&A..22..425H}. Nonetheless, the fact that first-order Fermi acceleration can model the production of charged particles of cosmic ray energies reasonably well suggests that it provides a good approximation of the acceleration process responsible for cosmic ray production.

\subsection{Neutrino Production}
Sources that accelerate high-energy protons are also capable of producing a corresponding flux of neutrinos. Charged particles responsible for non-thermal photon emission are assumed to be accelerated in collision-less shocks. However, not all of these particles will manage to avoid particle collisions through the entirety of their acceleration period. The source environment in which the proton flux is accelerated will most likely be accompanied by an intense radiation field as well. Both photon-proton and proton-proton interactions can lead to the production of mesons (usually pions) which will ultimately decay into neutrinos.

Neutrino source fluxes that originate from meson decay will follow the spectrum of their parent mesons which in turn follows the proton spectrum. The processes for pion production in $pp$ and $p\gamma$ interactions are listed in Eqs. \ref{eq:pp}, \ref{eq:py1}, and \ref{eq:py2}
\begin{equation}\label{eq:pp}
pp \rightarrow \pi^+X
\end{equation}
\begin{equation}\label{eq:py1}
p\gamma \rightarrow \Delta^+ \rightarrow n \pi^+
\end{equation}
\begin{equation}\label{eq:py2}
p\gamma \rightarrow \Delta^+ \rightarrow p \pi^0
\end{equation}
If the details of proton acceleration at the source are known well, then it is simple to derive the expected neutrino flux from $pp$ and $p\gamma$. The decay chain for charged pions yields the neutrinos generated as a result of these photo-meson and proton-proton interactions.
\begin{equation}\label{eq:pion1}
\pi^+ \rightarrow \mu^+ + \nu_{\mu} \rightarrow e^+ + \nu_e + \bar{\nu_{\mu}} + \nu_{\mu}
\end{equation}
\begin{equation}\label{eq:pion2}
\pi^- \rightarrow \mu^- + \bar{\nu_{\mu}} \rightarrow e^- + \bar{\nu_e} + \nu_{\mu} + \bar{\nu_{\mu}} 
\end{equation}
The decay of the neutral pion will simply yield two photons, $\pi^0 \rightarrow \gamma\gamma$. The net result is the production of three neutrinos from a single $\Delta^+$ resonance interaction. Neutrino spectra from pion decay will therefore feature an initial flavor flux ratio of (1:2:0).

It is allowable for the charged pion to decay into an $e^+$ instead of a $\mu^+$. In fact, one would naively expect the branching ratio for the $\pi^+ \rightarrow e^+ + \nu_e$ channel to be much higher due to the low mass of the electron. However, weak force interactions are helicity dependent and neutrinos (anti-neutrinos) are only left-handed (right-handed). This will force the massive lepton in the pion decay to be emitted as left-handed (right-handed) as well. For massless particles the helicity term will depend entirely on the left- or right-handedness of the particle. Therefore, if either the muon or electron had no mass then the decays in Eqs. \ref{eq:pion1} and \ref{eq:pion2} would be forbidden. They do have mass, though, and the fact that the electron is less massive than the muon means that the decay mode $\pi^+ \rightarrow e^+ + \nu_e$ will be heavily suppressed.

One other possible mechanism for the production of an astrophysical neutrino flux is through decay of free neutrons \cite{2015APh....62...66B}. These models require the acceleration of neutrons to high energies which is accomplished through coupling with an accelerated proton population. Neutrons that proceed to escape the source region in addition to accelerated protons that are converted into neutrons via interactions will stream outward until they decay.
\begin{equation}\label{eq:neutron_nu}
n \rightarrow p + e^- + \bar{\nu_e} 
\end{equation}
Under this scenario, a neutrino flux is produced with a flavor flux ratio of (1:0:0). Some models of cosmic ray production in GRBs make predictions on this neutrino flux. However, current gamma ray observations and upper limits on neutrino emission from GRBs find this model to be disfavored \cite{2015APh....62...66B}.

In addition to neutrino production at cosmic ray acceleration sites, neutrinos can also be produced by interactions of cosmic rays during propagation. Cosmic ray collisions with intervening material may result in the production of pions which will of course decay and produce neutrinos \cite{2014arXiv1407.5223L}.  At extremely high energies (E$_{cr} \geq 10^{19.7}$ eV), cosmic rays will begin to interact with cosmic microwave background effectively making the universe opaque for particles above this threshold energy. This predicted cutoff in the cosmic ray energy spectrum is known as the Greisen-Zatsepin-Kuzmin or GZK limit \cite{2006astro.ph..7109H}. The process in which charged pions are produced is detailed in Eq. \ref{eq:GZK}.
\begin{equation}\label{eq:GZK}
p\gamma_{cmb} \rightarrow \Delta^+ \rightarrow n + \pi^+
\end{equation}
The charged pions produced at this cutoff will generate a flux of extremely high energy neutrinos with a distinct signature.

The HiRes experiment has detected the GZK limit via suppression of the cosmic ray flux at the highest of energies \cite{2008PhRvL.100j1101A}. This detection has led some physicists to label the predicted GZK neutrino flux as the one ``guaranteed" source in neutrino astronomy. There are some pessimistic composition models for the highest energy cosmic rays that may suppress this neutrino flux \cite{2011APh....34..620A}, but the matter is far from settled at this point. This has motivated the development of ultra high-energy neutrino detection techniques in hopes of observing this astrophysical signature.

\section{Detection Methods}

The primary method for the detection of high-energy neutrinos is through observation of Cherenkov light produced by interaction secondaries in a transparent medium (typically water or ice). Events can be detected either by surrounding a detection volume with light detectors (as is the case for Super-Kamiokande \cite{2003NIMPA.501..418F}) or by placing the detectors within the detection medium itself (e.g. IceCube, ANTARES \cite{2011NIMPA.656...11A}). The Cherenkov light emitted by charged particles in the detection medium is collected by the detectors. The time of arrival of photons at different sensors can then be used to reconstruct events to determine the direction from  which their parent neutrino primary arrived. In order to be sensitive to expected astrophysical fluxes, large instrumented volumes are required. The largest detectors therefore make use of natural media (ice cap, oceans, etc.) to avoid prohibitive instrumentation costs.

Detection with air Cherenkov telescopes is also a possible detection technique for extremely high energy tau neutrino events (10$^{15}$-10$^{18}$ eV) \cite{2013APh....41....7A}. The typical source for IACTs (Imaging Atmospheric Cherenkov Telescope) are particle showers produced via gamma-ray interactions in the atmosphere. These electromagnetic showers produce a pool of Cherenkov light at the ground level which can be used to reconstruct the energy and direction of the gamma ray primary \cite{1993ExA.....2..331A}. This method can also be applied to particle showers produced during the decay of long-lived $\tau$ leptons produced in charged current interactions of UHE $\nu_{\tau}$ in the atmosphere or limb of the Earth \cite{2013APh....41....7A}. The shape of the Cherenkov light pool in these interactions will be qualitatively different from the light generated via cosmic ray or gamma ray showers allowing these events to be readily identified. This method utilizes the atmosphere above the telescope as its detection volume. Such a large volume is required to potentially observe these rare ``earth-skimming" neutrino events.

Ultra high-energy neutrinos (10$^{18}$-10$^{21}$) can also be detected through the coherent radio emission produced by extremely energetic neutrino-induced particle showers in polarizable media. This coherent radio emission was predicted by Gurgen Askaryan in 1962 \cite{1962SovietJETPAskaryan}. The effect is related to the emission of Cherenkov radiation in dielectric media. Although the amplitude of the radio emission of particles within the shower is rather small, the bulk radio emission of all particles will be in phase leading to a coherent pulse of radio emission that can be observed from far away. This of course requires the target detection volume to be located in a radio quiet region. The ANITA experiment (Antarctic Impulsive Transient Antenna) monitors for this radio emission during surveys of the entire Antarctic icecap to search for ultra high-energy GZK neutrino events \cite{2010PhRvD..82b2004G}. The experiment makes use of eight horn antennae sensitive to sub-GHz frequencies. The detection equipment is attached to a high-altitude balloon capable of flights of about one month in duration. This experimental design allows ANITA to survey an enormous volume of antarctic ice in hopes of detecting the predicted faint GZK neutrino flux \cite{2006astro.ph..7109H}.

\chapter{Neutrino Sources}
While no individual extra-solar high-energy neutrino sources have yet to be discovered (with the exception of SN 1987A), there is strong theoretical support  and indirect observational evidence for their existence. The measurement of extremely high-energy cosmic rays implies that a flux of energetic neutrinos must also be present due to the intimate link between hadronic acceleration and neutrino production in high-energy astrophysical environments. Most importantly, the recent discovery of a flux of high-energy neutrinos of astrophysical origin by IceCube \cite{2013Sci...342E...1I} provides confirmation that there must be some sources of high-energy neutrinos although they are currently unresolved.

The energy range of the event selection for this analysis (10-10$^3$ GeV) severely limits the ability to resolve steady neutrino sources because of the high rate of background atmospheric neutrino events. This motivates the use of time-dependent search methods, and as such this section will only consider transient sources of high-energy neutrino emission. Transient sources of emission generally arise from cataclysmic astrophysical events such as massive compact object mergers, supernovae, or gamma-ray bursts. All of these events represent the release of massive amounts of gravitational energy over short time scales. The predicted neutrino emission from some these sources will be discussed in this section. The detection prospects for these sources in the scope of this analysis will be discussed as well.

\section{Active Galactic Nuclei}
The creation of UHECRs requires immensely energetic astrophysical systems capable of accelerating particles to very high energies prior to escape. Most massive galaxies are thought to host a supermassive black hole ($M_{BH} \geq 10^{6}$ $M_{\odot}$) at their center. Periods of intense accretion of matter onto these blacks holes can power relativistic jets of material perpendicular to the accretion disk. Variability in the jets can result in the formation of relativistic shocks which can accelerate charged particles to extremely high energies, and it is also possible for shocks located within the inner regions of accretion disk to accelerate these particles as well \cite{1996SSRv...75..341S}. Due to the massive release of gravitational energy during accretion, active galactic nuclei (AGN) have been proposed as a possible source for the highest energy cosmic rays \cite{1996SSRv...75..341S}.

The jets powered by AGN accretion will accelerate electrons to extremely high energies resulting in the production of synchrotron and inverse-Compton high-energy photon emission \cite{2008A&A...492..695B}. If any protons are present in the these jets, then they will likely be accelerated as well. The most probable acceleration process for these charged particles is first-order Fermi acceleration in internal jet shocks \cite{1995PhR...258..173G}. This results in a non-thermal proton energy distribution described by a $E^{-2}$ power law spectrum with some energy cutoff \cite{1995PhR...258..173G}. This flux of protons will necessarily undergo cooling processes through interaction with in the dense radiation fields of the environment or via self-collisions. The interactions that lead to charged pion production are listed below.
\begin{equation}
p\gamma \rightarrow \Delta^{+} \rightarrow n\pi^{+} \quad \text{(photo-meson production)}
\end{equation}
\begin{equation}
pp \rightarrow \pi^{+}X  \quad \text{(proton-proton collision)}
\end{equation}
The production of pions will give rise to a neutrino flux due to the pion decay chain described in subsection 3.2.2. 

Although it is possible for AGN to serve as steady sources of high-energy neutrinos, in the context of the presented analysis we are primarily interested in the possibility of flaring events from AGN in which there is a strong enhancement of neutrino production over short timescales \cite{1997ARA&A..35..445U}. If the duration of this flux enhancement is on the order of 10$^4$ s or less and the jet is pointed towards Earth, then it is possible that it could be detected by the untriggered time-dependent search method used in the presented analysis. Unfortunately, our search method lacks sensitivity at distances beyond $\sim 20$ Mpc, so the cosmological distribution of blazars (AGN with jet oriented towards Earth) does not bode well for prospective detection of an AGN flaring event. So while AGN may be considered a possible source for this analysis, detection would likely require some special circumstances and a high degree of serendipity.

\section{Core-Collapse Supernovae}
The lifespan and manner of death for a star is primarily determined by its mass. Sufficiently massive stars will often end violently in the form of a supernova. The interiors of stars formed with masses greater than 8 $M_{\odot}$ will form dense cores consisting of nuclei with high atomic number such as O,Ne,Mg, and even Fe \cite{2003astro.ph..1006H}. The cessation of nuclear fusion in these dense cores will render them unstable and eventually gravitational collapse will occur. This collapse will drive neutronization of protons ($p+e^{-} \rightarrow n + \nu_e$) in the core releasing a tremendous amount of energy (E$\sim 10^{53}$ erg) in form of a neutrino wind. The specific details of the physical process driving the supernova explosion are not fully known, but current models assume that the neutrino wind deposits some fraction of its energy into the surrounding stellar material creating a powerful shockwave \cite{2003astro.ph..1006H}. The initial mass of the star will determine if the core will collapse into a neutron star or directly into a black hole.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.5\textwidth,keepaspectratio]{NearbySNCatalogue.png}
  \end{center}
  \caption[Local SNe within 10 Mpc]{Observed SNe within 10 Mpc in the years 1999-2008 \cite{2011PhRvD..83l3008K}}
  \label{fig:local_ccsne}
\end{figure}

The detection of several neutrino events in temporal coincidence with supernova 1987A marked the first detection of an extra-solar neutrino source. Neutrino events were observed in three separate detectors a few hours prior to the optical observation \cite{1987PhRvL..58.1490H}, \cite{1987PhRvL..58.1494B}. Such an observation was possible due to the close proximity of the progenitor which was located within the Large Magellanic Cloud ($\sim$50 kpc from Earth). Detection of these events confirmed the theoretical prediction of an enormous liberation of energy in the form of neutrino emission during the collapse of the core of a massive star.

While the neutrino fluence produced during core-collapse of a massive star is quite large, the typical energy of these neutrinos is simply too low to be seen by the IceCube detector. However, an independent data acquisition system in IceCube does monitor for possible core-collapse SNe by looking for simultaneous rate increase in all IceCube DOMs indicative of the arrival of a soft neutrino flux. Our presented analysis method is in no way equipped to detect the neutrino flux produced during core-collapse. However, certain neutrino source classes for which this analysis may be sensitive are likely special cases of the core-collapse scenario, so it is therefore important to understand the rate at which these stellar explosions occur in the universe. 

Proper measurement of the local supernovae rate requires constant monitoring of the sky and meticulous cataloging in surveys. As the number and capability of telescopes has increased, the distance to which surveys can be considered complete has been extended considerably. Current estimates of the volumetric rate of core-collapse SNe place the value of the nearby rate somewhere between 6.3$\cdot10^{-5}$ and 1.4$\cdot10^{-4}$ SNe$\cdot$Mpc$^{-3}\cdot$yr$^{-1}$ \cite{2011MNRAS.412.1419L}, \cite{0004-637X-738-2-154}. The assumed fraction of core-collapse SNe that are capable of producing neutrino emission of interest to our analysis can be used to estimate the rate at which more interesting core-collapse events will occur. This in turn gives an estimation of the probability of such an event occurring at a detectable distance during our observation period.

\section{Gamma-ray Bursts}
Gamma-ray Bursts (GRBs) are characterized by extraordinarily bright emission of gamma rays over timescales ranging from 0.1-10$^3$ s. The isotropic distribution of detected bursts suggest that they occur at cosmological distances and are not of galactic origin. If one assumes that the emission from these sources is isotropic, then the luminosity required for the observed fluxes at Earth is on the order 10$^{52}$ erg$\cdot$s$^{-1}$ \cite{2004RvMP...76.1143P}. This enormous energy output suggests that GRB emission must be significantly beamed with Earth lying in the line of sight. The large energy budget of GRBs also suggests that they may be possible candidates for the sources of the highest energy cosmic rays. If this is indeed the case, then it stands to reason that high-energy neutrino emission could also be present during GRB events.

This possibility has been investigated with many configurations of the IceCube detector, and there has yet to be a confirmed significant detection of neutrino events in correlation with detected GRBs \cite{2011PhRvL.106n1101A}, \cite{2012Natur.484..351I}. The limits imposed by the non-detection of coincident neutrino events in these searches has severely limited the allowable parameter space for theoretical GRB neutrino emission models. Because GRBs typically occur at cosmological distances, it is unlikely that any GRBs detected during the observation period of this analysis will be detectable. Because this analysis is untriggered it is still within the realm of possibility that neutrino emission from a GRB could be found with our search method. This analysis may even have superior detection prospects with respect to standard IceCube searches if the neutrino emission from GRBs is much softer than the conventional prediction. 

\subsection{Fireball Model}
The most widely accepted model of long-duration GRBs is the `fireball' model developed by Rees and M\`{e}sz\`{a}ros \cite{1992MNRAS.258P..41R}. In this model a central engine, such as the compact object produced during core-collapse, powers the production of highly relativistic jets during rapid accretion. These jets will be oriented along the spin axis of the engine. Strong magnetic fields in these jets will confine a plasma consisting of leptons, photons, and possibly baryons. The kinetic energy imparted to this plasma will result in the jet material reaching Lorentz boost factors on the order of a few hundred \cite{1992MNRAS.258P..41R}. The accelerated plasma will burrow through the intervening stellar material until breakout is reached. After the jet has escaped the stellar envelope, expansion and cooling allow the fireball to become optically thin and gamma rays produced in the jet can escape.

Accretion at the central engine is expected to be highly variable, resulting in the formation of sub-shells of material within the jet featuring different bulk Lorentz factors \cite{2014arXiv1410.0679K}. These shells will inevitably collide resulting in the formation of several internal shocks within the jet. Charged particles downstream of the shock front may be accelerated through first-order Fermi acceleration. In this manner, a non-thermal power law spectrum of electrons is generated. Synchrotron emission from these electrons will in turn produce the observed non-thermal gamma ray emission \cite{1996ApJ...461L..37B}. If protons are also present in the jet, then it is expected that they will also be accelerated in the relativistic shocks. However, it is not known how much baryon loading will occur in these jets.

We can expect a neutrino flux from these jets in the event that a sizable flux of accelerated protons is produced. The production scenario is similar to that described for AGN in section 4.1. Proton-$\gamma$ and proton-proton interactions will lead to the creation of both charged and neutral pions. The decay chain for the $\pi^+$ is listed below.
\begin{equation}\label{eq:pidecay}
\pi^{+} \rightarrow \mu^{+} + \nu_{\mu}
\end{equation}
The $\mu^+$ will subsequently decay,
\begin{equation}
\mu^+ \rightarrow e^+ + \nu_e + \bar{\nu_{\mu}}
\end{equation}
resulting in the generation of a neutrino flux with a flavor ratio ($\nu_e$:$\nu_{\mu}$:$\nu_{\tau}$) of (1:2:0). Higher energy mesons such as the kaon may also be produced in pp and p$\gamma$ interactions. These mesons will also decay to muons generating muon neutrinos in the process. However, the flux at lower energies will be dominated by neutrinos of pionic origin. The observable neutrino flux can be changed considerably if efficient cooling of the parent mesons is allowed to take place prior to their decay. Depending on the jet environment, cooling of the mesons can occur through hadronic interactions or via synchrotron energy losses \cite{2005PhRvL..95f1103A}. This can lead to ``muon-suppression" and will result in a final neutrino flavor flux ratio of (0:1:0). Ultimately, the shape of the neutrino spectrum will follow that of the protons as well as the photon spectrum for neutrinos produced via photo-meson production.

\subsection{Subphotospheric Model}
An alternative model of neutrino emission proposed by Murase attributes the observed prompt gamma ray emission in GRBs to sub-photospheric gamma rays \cite{2013PhRvL.111m1102M}. This model examines the role of hadronuclear interactions (pp, pn, and nn) during the early stages of development of the jet. During the initial acceleration of protons and neutrons in the jet, it is possible for the two species to become decoupled \cite{PhysRevLett.85.1362}. This will result in protons achieving higher Lorentz boost factor values than their neutron counterparts. This will lead to inelastic collisions as the protons overtake the slower neutrons which will result in the production of mesons and muons. The subsequent decay of these particles will of course produce neutrinos.

The bulk of this predicted quasi-thermal neutrino flux lies at energies ranging from 10-100 GeV. This is much lower in energy than the predicted TeV neutrinos from the fireball model. The estimated neutrino fluence for this model is given in Figure \ref{fig:subphotospheric_nus}. This possible neutrino production mechanism would therefore be an excellent potential source for the presented analysis due to its primary focus on neutrinos in the energy range of 30-300 GeV. Nonetheless, detection of such a flux is only feasible for very powerful GRBs with very low redshift values (z$<0.1$). Thus, detection of this GRB neutrino flux is unlikely but still intriguing.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.85\textwidth,keepaspectratio]{SubPhotoFluence.png}
  \end{center}
  \caption[GRB Neutrino Fluence under Sub-photospheric Model]{Energy fluence of $\nu_{\mu}$ and $\bar{\nu}_{\nu}$ from high-luminosity GRB at a redshift of z=0.1 in the sub-photospheric emission model \cite{2013PhRvL.111m1102M}.}
  \label{fig:subphotospheric_nus}
\end{figure}

\section{Choked Gamma-ray Bursts}
The core-collapse of a massive star is thought to be the mostly likely engine for powering the jets responsible for gamma-ray emission from long duration GRBs \cite{2004RvMP...76.1143P}. Proposed models for GRBs posit that gamma ray emission is produced via Fermi-acceleration of either electrons or protons in relativistic shocks within the energetic jets \cite{2004IJMPA..19.2385Z}. Whether these jets are prevalent in a large fraction of core-collapse supernovae is unknown, but it does suggest that a correlation between long duration GRBs and supernovae should exist. In fact, some instances of GRB association with SNe have been observed \cite{2006ARA&A..44..507W}, \cite{2011AN....332..434M} \cite{2003astro.ph..1006H}. However, only a small fraction ($\leq 10^{-3}$) of observed supernovae are known to be associated with GRBs \cite{2003ApJ...599..408B}. 

This has led some to speculate that jet production during core-collapse might occur in a higher fraction of SNe than the observed GRB-SNe fraction would indicate. Rather than achieving breakout like the jets in the GRB scenario, the softer jets of these SNe may never breach the surrounding stellar envelope and will be effectively `choked' off (see Figure \ref{fig:chokedjet}). This may be due to either the jets being insufficiently energetic or the surrounding envelope of material being far more massive than what is typical in GRB progenitors. However, if these jets produce neutrinos in a similar manner as the jets of GRBs, it may be possible to observe such a source through its neutrino emission despite the fact that the optical signature remains hidden. In this scenario, a continuum class of objects would exist in which GRBs represent CC-SNe harboring the most energetic of jets.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.85\textwidth,keepaspectratio]{ChokedJet.png}
  \end{center}
  \caption[Choked GRB Jet Diagram]{Simple diagram showing the choked jet structure. The jet powered by the central engine will accelerate particles in internal shocks. However, the jet energy is dissipated before it can burrow through the surrounding stellar material.}
  \label{fig:chokedjet}
\end{figure}

The suspected frequency of CC-SNe in the universe is much higher than that of GRBs \cite{0004-637X-738-2-154}, \cite{2004RvMP...76.1143P}. Thus, it is possible that choked GRBs may occur at a much higher rate than GRBs in the nearby universe making choked GRBs a prime target for time-dependent neutrino analyses. Of course this would require that the neutrino production in the soft jets of choked GRBs is capable of producing neutrinos of energies detectable in IceCube and DeepCore.

A model of slow jets in CC-SNe developed by Razzaque, M\'{e}sz\'{a}ros and Waxman describes the neutrino emission for this potential source class \cite{2004PhRvL..93r1101R}. Further development of this model by Ando and Beacom incorporates the contribution to the neutrino flux from kaon decay \cite{2005PhRvL..95f1103A}. We will hereafter refer to this model as RMW/AB. The neutrino flux predicted by the RMW/AB model is soft compared to the expectation from full-fledged GRBs. Due to breaks in the energy spectrum at energies below IceCube's optimal energy range, it is possible that a DeepCore analysis provides the best opportunity to observe neutrino emission from these jets.

\subsection{Neutrino Emission Model}
The neutrino emission model in the choked GRB scenario shares many similarities with the GRB fireball model described in section 4.3. Jet formation is expected during rapid accretion onto the newly formed compact object after core-collapse. The typical jet energy in the RMW/AB formulation is set to $E_j=10^{51.5}$. The expected bulk Lorentz boost factor $\Gamma_b$ for the jet is given a mildly relativistic value of 3 with a rather broad opening angle $\theta_j \sim \Gamma_b^{-1}=0.3$. The variability timescale of the  central engine should be similar to that of the GRB scenario and it is set to $t_v = 0.1$s.

The variability in accretion onto the central engine will result in the production of shells of material in the jet with differing boost factors. This will lead to the development of many relativistic shocks which will accelerate any downstream charged particles via first-order Fermi acceleration \cite{2005PhRvL..95f1103A}. Protons within the jet will initially be accelerated to an $E^{-2}$ spectrum, but they will also be subject to two cooling mechanisms:
\begin{itemize}
\item 1) Hadronic Cooling - The $p\gamma$ and $pp$ interactions that are responsible for the generation of high-energy neutrinos will also cool the accelerated protons as they interact. The efficiency of this cooling will depend on the photon and proton densities respectively. The threshold energy for $p\gamma \rightarrow \Delta^+$ production is given by
\begin{equation}
E_p' = \frac{0.3\text{ GeV$^2$}}{E_{\gamma}'}
\end{equation}
where $E_p'$ and $E_{\gamma}'$ are the proton and photon energies in the comoving frame of the jet.
\item 2) Radiative Cooling - The magnetic fields that accelerated the protons will also force energy losses due to proton synchrotron emission. The cooling timescale for this process will depend primarily on the energy of the proton and the strength of the magnetic field. The accelerated protons will also lose energy via Bethe-Heitler pair-production in the dense thermal photon radiation field. The efficiency of this cooling process increases with proton energy, and it will therefore be the mechanism responsible for the determining the maximum proton energy.
\end{itemize}
Factoring in these cooling mechanism yields an initial $E^{-2}$ proton spectrum with spectral breaks at the energies where the hadronic and radiative cooling processes begin to take effect.

The shape of this proton spectrum will be reflected in the neutrino spectrum as well. The neutrino production method will be the same as it was for GRBs. Mesons will be produced in $pp$ and $p\gamma$ interactions and will subsequently decay to produce leptons and neutrinos (see Eq. \ref{eq:pidecay}). These mesons will also undergo hadronic and radiative cooling prior to decay. The radiative cooling mechanisms are particularly efficient for muons which will lead to a suppression of neutrinos originating from muon decay. Thus, only the muon neutrinos produced during $\pi$ and $K$ decays will contribute significantly to the neutrino flux. Therefore the initial flavor flux ratio for the neutrino emission from the jet will be (0:1:0).

\begin{table}[h]
\caption[Neutrino Emission Model Parameters]{This tables summarizes the parameters for the neutrino emission model for soft jets in core-collapse SNe. We will use the notation from Ando and Beacom \cite{2005PhRvL..95f1103A}.\label{tab:chkgrb_params}}
\begin{center}
\begin{tabular}{l C{5cm} rc}
  \toprule
 \textbf{Parameter} &\textbf{Description} &\textbf{Canonical Value} & \textbf{Dependence}\\
\midrule
$E_j$ & Total jet kinetic energy & 10$^{51.5}$ & -- \\ 
$\Gamma_b$ & Bulk Lorentz boost factor in jet & 3.0 & -- \\ 
$D$ & Source distance & -- & -- \\ 
$B_{\pi(K)}$ & Branching ratio for $\nu_{\mu}$ production in pion (kaon) decay & 1 (0.6) & -- \\ 
$<n>_{\pi(K)}$ & Multiplicity of pions (kaons) in pp interactions & 1(0.1) & -- \\ 
$\theta_j$ & Jet opening angle & 0.3 & $\Gamma_b^{-1}$ \\
$E_{p,min}$ & minimum proton energy & 10 GeV & -- \\ 
$E_{p,max}$ & maximum proton energy & 7$\cdot 10^4$ GeV & -- \\ 
$E_{\nu,b}^{\pi(1)}$ ($E_{\nu,b}^{K(1)}$)& hadronic cooling break energy & 30 (200) GeV & $\varpropto E_j^{-1}\Gamma_b^{5}$ \\ 
$E_{\nu,b}^{\pi(2)}$ ($E_{\nu,b}^{K(2)}$)& radiative cooling break energy & 100 (20,000) GeV & $\varpropto \Gamma_b$\\ 
$E_{\nu,max}^{\pi}$ ($E_{\nu,max}^{K}$)& maximum neutrino energy & 10500 (21,000) GeV & $\varpropto \Gamma_b$\\ 
\hline
\end{tabular}
\end{center}
\end{table}
The neutrino spectrum is given by a doubly broken power law in Eq. \ref{eq:chkgrb_spec} with break energies given by the hadronic and radiative break energies $E_{\nu,b}^{\pi(1)}$ ($E_{\nu,b}^{K(1)}$) and  $E_{\nu,b}^{\pi(2)}$ ($E_{\nu,b}^{K(2)}$). An expression for the neutrino fluence $F_{\nu}$ as a function of jet parameters can be found in Eq. \ref{eq:nuflu}. Definitions of all the parameters and their dependence on jet energy $E_j$ and Lorentz boost factor $\Gamma_b$ are provided in Table \ref{tab:chkgrb_params}. Using Eqs. \ref{eq:chkgrb_spec} and \ref{eq:nuflu}, we can calculate the expected neutrino fluence at Earth for a sample choked GRB event. The expected fluence for a burst located 10 Mpc away with canonical values of the jet parameters is plotted in Figure \ref{fig:ref_chkgrb_flux}.

\begin{equation}\label{eq:chkgrb_spec}
\frac{d\Phi_\nu}{dE}=F_\nu\left\{\begin{array}{cc}
E^{-2} & E > E_{\nu}^{(1)} \\ 
E_{\nu}^{(1)}E^{-3} & E_{\nu}^{(1)}< E < E_{\nu}^{(2)} \\ 
E_{\nu}^{(1)}E_{\nu}^{(2)}E^{-4} & E_{\nu}^{(2)}< E < E_{max}
\end{array}\right.
\end{equation}
\begin{equation}\label{eq:nuflu}
F_{\nu} = \frac{<n>_{\pi(K)}B_{\pi(K)}}{8} \cdot \frac{E_j \Gamma_b^2}{2 \pi D^2 \text{ln}(E_{p,max}', E_{p,min}')}
\end{equation}
The normalization and shape of the flux is highly sensitive to the choice of jet parameters. Any deviation from the canonical values can result in a large difference in event expectation at Earth. If the nominal values on the jet parameters are pessimistic, it is conceivable that IceCube analyses would be sensitive to this emission up to fairly large distances. Visualization of the strong dependence of the neutrino fluence on $\Gamma_b$ is shown in Figure \ref{fig:fluxplot_multiplejetparams}.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.85\textwidth,keepaspectratio]{FluxPlot_Canonical_RespectiveFluenceNorm.png}
  \end{center}
  \caption[Choked GRB Flux at Earth by Pion and Kaon Contribution]{Estimated $E^{2}$-weighted neutrino flux at Earth for a choked GRB at a reference distance of 10 Mpc with canonical model parameters ($\Gamma_{b}=3$, $E_{jet}=3\cdot 10^{51}$ erg). At lower energies, the flux is dominated by neutrinos of pionic origin (dashed line). Neutrinos produced in kaon decays have a harder spectrum and will dominate at higher energies.}
  \label{fig:ref_chkgrb_flux}
\end{figure}

The soft spectrum predicted by the RMW/AB model motivates the use of a DeepCore based event selection instead of a standard IceCube sample. Our event selection choices therefore focus on isolating $\nu_{\mu}$ events in the 30-300 GeV energy range. The identification of neutrinos from jets in a gamma ray dark SNe would be a dramatic confirmation of the suspected GRB-SNe connection. Great strides in our understanding of core-collapse phenomena and the production of GRBs will be made if neutrino analyses can begin to observe this jetted neutrino emission.
%% Flux plot

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.85\textwidth,keepaspectratio]{ChkGRBFluxPlot.png}
  \end{center}
  \caption[Choked GRB Flux at Earth for different bulk Lorentz factor $\Gamma_b$]{Estimated $E^{2}$-weighted neutrino flux at Earth for a choked GRB at a reference distance of 10 Mpc for several choices of bulk Lorentz factor $\Gamma_b$.}
  \label{fig:fluxplot_multiplejetparams}
\end{figure}

\subsection{Model Limits on SN2008D}
The X-ray telescope of the \textit{SWIFT} satellite detected a bright flash indicating a transient event during observation of NGC 2770 on January, 9, 2008. Followup observations showed that the \textit{SWIFT} source was a core-collapse supernova of type Ib \cite{2008Natur.453..469S}. At a distance of only 27 Mpc, this supernova provided an opportunity for the IceCube detector to search for any corresponding high-energy neutrino emission. There is no clear evidence, however, that SN2008D had an aspherical explosion which would suggest the production of energetic jets similar to those described by the RMW/AB model \cite{2008Natur.453..469S}. Nonetheless, a search for high-energy neutrino emission was carried out using the 22-string partial configuration of IceCube \cite{2011A&A...527A..28I}.

This search established three separate search windows of varying spatial extent and duration to be examined for potential signal muon neutrino events. Emission time windows of 10$^2$ s, 10$^3$ s, and 10$^4$ s were chosen with respective angular acceptance regions of 6.2$^{\circ}$, 2.6$^{\circ}$, and 1.5$^{\circ}$ \cite{2011A&A...527A..28I}. Examination of the data revealed no neutrino events present after cuts in any of the defined search windows, which was entirely consistent with the event expectations from signal and background \cite{2011A&A...527A..28I}. If one assumes that SN2008D did produce soft jets, the null result enables limits on the possible values of jet parameters to be set for different emission timescales. The results from this IceCube analysis are shown in Figure \ref{fig:SN2008D}. These results must also assume that any jet produced in SN2008D was oriented towards the Earth. This first attempt to place limits on the soft-jet neutrino emission model would provide the basis for the investigation into choked GRB jet parameters undertaken by the analysis presented in this thesis.
\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.65\textwidth,keepaspectratio]{SN2008D_ICLimits.png}
  \end{center}
  \caption[IceCube-22 Limits on SN2008D]{Limits on the bulk Lorentz factor $\Gamma_b$ and total jet energy $E_j$ for SN2008D from the IceCube-22 dedicated analysis. The shaded regions give the parameter space ruled out at the 90$\%$ confidence level for emission timescale $\tau$.}
  \label{fig:SN2008D}
\end{figure}
The presented analysis will examine a similar $\Gamma_b$-$E_j$ parameter space as the one shown in Figure \ref{fig:SN2008D}. This search will not be triggered by a specific core-collapse event, however, so the search method will be considerably different than defined window method used in the previous IceCube analysis of SN2008D. The lack of an external trigger allows for this search to examine the whole Northern sky in addition to a wide range of possible emission timescales. The details of this search method will be given in Chapter 9.

\chapter{Detector}
\section{IceCube and IceTop}
The IceCube Neutrino Observatory is a km$^{3}$-scale neutrino detector located deep within the glacial ice of the Antarctic ice sheet at the geographical South Pole. This location provides IceCube with a pristine detection medium in addition to mechanical support for the entirety of the array. The detector consists of 5,160 light sensors known as digital optical modules (DOMs) which are distributed along 86 cables (referred to as strings) that supply power and provide communication to the surface. Each cable is instrumented with 60 DOMs spaced 17 meters apart starting at 1,450 meters below the surface and terminating at 2,450 meters below. An inter-string spacing of 125 meters on average results in a total instrumented volume of approximately 1 km$^{3}$. Figure \ref{fig:icecube} provides a schematic illustrating the detector geometry.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=1.0\textwidth,keepaspectratio]{ArrayWSeasonsLabels.pdf}
  \end{center}
  \caption[IceCube Diagram]{Diagram of the IceCube Neutrino Observatory (Courtesy of the IceCube Collaboration).}
  \label{fig:icecube}
\end{figure}

Installation of the IceCube strings took place over several years and required the use of a specialized hot-water drill. In the deployment process, the hot-water drill is used to bore through the ice leaving a water-filled column in which the string and its attached DOMs are lowered. The water column subsequently freezes the cable and all DOMs in place rendering them completely inaccessible from the surface. The deployment of the first IceCube string occurred on January 29th, 2005. The remaining strings were deployed over the next five summer seasons resulting in data seasons of different detector shapes and size. The final string was deployed on December 18, 2010 giving IceCube its ultimate 86-string configuration.

%%% IceTop

In addition to the detectors installed deep in the ice, there are also 81 detector stations (each station consisting of two tanks) at the surface. These tanks, which utilize two of the same light-sensing DOMs as IceCube, comprise the IceTop surface array. The DOMs in these tanks, which are also frozen in place, look for Cherenkov radiation produced by cosmic ray air shower secondaries in the tank ice. By examining the arrival time of charged particles from the shower front, the direction of cosmic rays incident at Earth can be determined. The spatial extent of the shower as well as the total charge deposition in the tank PMTs allow for accurate estimation of the energy of the primary cosmic ray. Data produced from IceTop is used to study cosmic ray composition, spectra, and anisotropy.

Due to the spatial relation of both IceTop and IceCube, they are able to complement the capabilities of each other quite nicely. IceTop's primary purpose is to study air shower physics, but it also serves as a veto for downgoing atmospheric muons and neutrinos in IceCube. This is particularly useful in the search for highly energetic neutrinos of astrophysical origin such as the events reported in \cite{2013Sci...342E...1I} and \cite{2014PhRvL.113j1101A}. Any downgoing event found by these searches that is accompanied by a causually connected air shower signal in IceTop is immediately identified as atmospheric in origin. Alternatively, the background muons detected in IceCube can be used for more detailed study of air-shower composition and energy in IceTop analyses. For more detailed information on the physics goals and detection capabilities of IceTop, see \cite{2013NIMPA.700..188A}.


\section{DeepCore}

DeepCore \cite{2012APh....35..615A} is a sub-detector deployed in tandem with IceCube between 2009 and 2010 that was primarily designed to lower the energy threshold of IceCube. The array consists of eight infill strings located in the center of the IceCube detector in addition to twelve central standard IceCube strings. This configuration gives DeepCore three surrounding layers of IceCube strings to use as an active veto for the primary background of atmospheric muons (see \ref{fig:DeepCoreSchematic}). In order to improve detector response to lower energy neutrinos, $\mathcal{O}$(10-100 GeV), the infill strings of DeepCore have a much closer inter-string separation of 42 m and have 50 DOMs spaced 7 m apart deployed deep in the ice between 2,100 m and 2,450 m. This denser instrumentation allows for better timing and spatial resolution of charged secondaries produced in neutrino interactions. Additional sensitivity to lower energies is gained through the use of the newer Hamamatsu R7081MOD model PMT in the infill string DOMs as opposed to the standard Hamamatsu R7081-02 used in IceCube. This model boasts higher quantum-efficiency in the photocathode for photons at typical Cherenkov wavelengths ($\lambda \sim 400$ nm). In-ice measurements of the high quantum-efficiency (HQE) DOMs showed a 35$\%$ increase in sensitivity to Cherenkov light with respect to the standard IceCube DOMs \cite{2012APh....35..615A}.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.98\textwidth,keepaspectratio]{BoreholeLaserDustLogging.png}
  \end{center}
  \caption[Borehole Dust Concentration Measurements]{Borehole laser measurements of dust concentrations in the ice as a function of depth. Measurements for several IceCube strings are displayed. Higher values on the y-axis denote higher dust concentrations. The "dust layer" features quite prominently at a depth of 2100m \cite{2013JGlac..59.1117.}.}
  \label{fig:DustLogger}
\end{figure}

The depth selected for deployment of the DeepCore DOMs was determined via examination of the ice properties previously mapped by both the Anatarctic Muon and Neutrino Detector Array (AMANDA) \cite{2006JGRD..11113203A} and pre-existing IceCube configurations \cite{2013JGlac..59.1117.}. These investigations into the optical properties of the ice revealed that the deepest ice (depths $\geq$ 2,100 m) had superior optical qualities with respect to the ice closer to the surface. Additionally, it was determined that a layer of high dust concentration in which light is scattered and absorbed to a much higher degree exists at a depth of 2,000-2,100 m (see Figure \ref{fig:DustLogger}). The eight infill strings also have a section of 10 DOMs with 10 m spacing located just above this dust layer. These DOMs form a veto cap to further increase the detection probability and rejection of directly down-going muons. Figure \ref{fig:DeepCoreSchematic} shows the distribution of the DeepCore DOMs and the spacing and orientation of the DeepCore strings with respect to IceCube as a whole.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.7\textwidth,keepaspectratio]{IC86EDC_DeepCoreDiagram.pdf}
  \end{center}
  \caption[DeepCore Schematic]{Top down and side-view diagram of DeepCore. The side-view shows the difference in DOM distribution for the infill strings and their relation to the dust layer \cite{2012APh....35..615A}.}
  \label{fig:DeepCoreSchematic}
\end{figure}

%%% Physics in DeepCore
The primary physics goal of the DeepCore installation is to provide increased sensitivity for indirect dark matter searches by improving the IceCube detectors ability to resolve sub-100 GeV neutrino events. In this regard, it has been quite successful in establishing limits on the cross-sections of many WIMP (Weakly Interacting Massive Particle) dark matter models with the Sun \cite{2013PhRvL.110m1302A} and Milky Way \cite{2011PhRvD..84b2004A} as possible sources. The lowering of the detector's energy threshold has also made neutrino oscillation parameter measurements possible due to the high statistics provided by atmospheric neutrino events \cite{2013PhRvL.111h1801A}. Most importantly for the analysis presented in this thesis, however, is the improvement in effective area and resolution DeepCore provides for 30-150 GeV muon neutrinos. As this thesis will demonstrate, including these neutrino events into previously established IceCube point source analysis methods greatly improves IceCube's capability to discover transient events with soft spectra.
\section{Neutrino Events in IceCube}

In order to isolate the sparse neutrino events from the abundance of background cosmic ray muons, it is necessary to fully understand the nature of the detector response to neutrinos and neutrino secondaries interacting within the detector. Neutrinos that are sufficiently energetic to be detected by IceCube will undergo deep inelastic scattering with a nucleon target (see for more information on this process see section \textbf{2.1.1}). The nature of the boson exchange will determine if this process is of the charged-current (CC) or neutral-current (NC) variety. The hit topology of a given neutrino event in IceCube will depend upon the flavor of the neutrino ($\nu_{e}$, $\nu_{\mu}$, $\nu_{\tau}$) as well as the channel through which it interacts with a target nucleon in the ice.
\subsection{Cascades}
In NC interactions of all flavors, a hadronic cascade is produced which yields a roughly isotropic distribution of light. Any spatial extent in the hadronic cascade particles will be much smaller than the DOM separation distance. Thus, the Cherenkov emission from these particles will appear to be a point source of light within the detector. This results in a spherical pattern of DOMs that register light from this type of interaction. The radius of DOMs which are able to detect light from the cascade is determined by the total energy deposited in the ice by the neutrino primary. Events with this hit pattern are referred to as cascades.  The spherical hit pattern produced by these types of interaction will be the same regardless of the neutrino flavor. An example event display for this type of interaction can be seen in Figure \ref{fig:cascade}.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=1.0\textwidth,keepaspectratio]{hese_cascade_event.png}
  \end{center}
  \caption[Sample Cascade Event]{A high-energy cascade event in IceCube with deposited energy of $210\pm^{29.0}_{25.8}$ TeV \cite{2013Sci...342E...1I}. The colored spheres represent DOMs that have registered light during the event. The size of the spheres are indicative of the total light received by the PMT on that DOM. The color denotes the timing of the hit with red corresponding to earlier times and blue corresponding to later times.}
  \label{fig:cascade}
\end{figure}

Whereas the detector response for NC interactions is flavor independent, the event topology in CC interactions is determined primarily by the lepton flavor of the neutrino. In addition to a hadronic cascade, the CC interaction will also yield an energetic lepton corresponding to the flavor of the interacting neutrino. In the case of $\nu_{e}$ and $\nu_{\tau}$ CC interactions, the resulting hit pattern in IceCube will take the form of a cascade in a similar manner to the NC interactions. While the source of Cherenkov emission is no longer point-like, the length of electron and tau particle tracks are much shorter than the inter-DOM separation distance. Some marginal pointing can be achieved for these events, however, since the light produced in the hadronic and electromagnetic cascades in these events is not totally symmetric. For sufficiently energetic $\nu_{\tau}$ events in IceCube, more exotic signatures are possible. These arise from the increased lifetime of the outgoing $\tau$ lepton resulting in two separate light-producing cascades that can be resolved separately either in space or time. As of the writing of this thesis, no events of this type have been observed in IceCube. While the pointing provided by cascade-like events is rather poor, the energy of events of this type that are fully contained in the detector can be reconstructed with good accuracy. 

\subsection{Muon Tracks}
IceCube is designed specifically to be sensitive $\nu_{\mu}$ CC interactions due to superior pointing provided by long-lived muon tracks in the ice. Daughter muons from $\nu_{\mu}$ CC interactions can travel distances ranging from ~300 m ($E_{\nu_{\mu}}\sim 100$ GeV) to several kilometers ($E_{\nu_{\mu}}\geq 1$ TeV) \cite{2001PhRvD..63i4020I}. As these muons travel through the ice, they produce light in electromagnetic showers through both ionization and stochastic radiation losses. Because the muon is traveling faster than the speed of light in the ice (index of refraction $n_{ice} \sim 1.3$), the Cherenkov light generated about the muon track will form a cone which is ultimately aligned with the original neutrino direction. This results in a linear hit pattern in IceCube DOMs, providing a clear signal with good directional information. Muon tracks with the highest contained length in the detector provide the best resolution due to their long lever arm and low kinematic angular difference with respect to the parent neutrino. This allows muon tracks to serve as the primary event type for astronomical purposes. An example of a high-energy track event is shown in Figure \ref{fig:track}.

%% Add muon kinematic angle
\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=1.0\textwidth,keepaspectratio]{hese_track_event.png}
  \end{center}
  \caption[Sample Track Event]{A high-energy track event in IceCube with deposited energy of $71.4 \pm 9.0$ TeV \cite{2013Sci...342E...1I}.}
  \label{fig:track}
\end{figure}


\chapter{Data Acquisition}
Maintaining smooth and efficient data acquisition for a detector consisting of such a large number of sensors presents a formidable challenge. Reconstructing physics events within the detector requires accurate timing of signals received by individual sensors coupled with a high degree of synchronization among all detection elements. In this section, a succinct description of the detection of the light-yield from particle interactions in the ice and the subsequent processing of that data is given. The reader interested in a much more thorough account is encouraged to consult the summary by Abbasi et al. \cite{2009NIMPA.601..294A}. 

\section{The Digital Optical Module}
The essential component of the IceCube detector is the DOM. Each of these sensor units contains a Hamamatsu R7081-02 25 cm photo-multiplier tube (PMT), attached digitizing electronics, and LED flashers all housed within a glass pressure vessel \cite{2006NIMPA.567..214H}. A penetrator cable breaches the pressure vessel to connect the DOM electronics to the supporting string cable enabling DOM-to-DOM as well as DOM-to-surface communications. Figure \ref{fig:domscheme} provides an illustration of the DOM structure and its constituent components while Figure \ref{fig:dompic} gives a picture of a fully assembled DOM in its harness with breakout cable. Absolute quantum-efficiency measurements were made for all DOMs prior to deployment in the ice. In order to estimate how the efficiency might change after the water column in which the DOMs were deployed freezes, studies on the efficiency of DOMs at typical in-ice temperatures were performed in labs at IceCube member institutions. The inclusion of LED flashers at UV wavelength allows the DOM to simulate Cherenkov signals for the purpose of calibrating neighboring DOMs. These LEDs are also used to perform studies on the bulk ice properties near the DOM as well as the optical properties of the ice in the re-frozen water column in which the DOM is located.

\begin{figure}
\centering
\subfigure[DOM Schematic]{\label{fig:domscheme}\includegraphics[width=70mm]{DomSchematic.png}}
\subfigure[Assembled DOM Photo]{\label{fig:dompic}\includegraphics[width=60mm]{LabDOM.pdf}}
\caption[DOM Schematic and Photo]{(a) Schematic detailing DOM structure \cite{2009NIMPA.601..294A}. (b) A fully assembled DOM supported by a cable harness.} 
\label{fig:DomPics}
\end{figure}
The operational lifetime of IceCube is tied directly to the survival of the DOMs in the ice. The inability to access these modules necessitated a design with a high probability of survival under intense pressures and cold operating temperatures. The design has so far proven to be quite robust; the DOM survival rate since first installation until the 2013-2014 season is an impressive 98\%. The majority of DOM failures occurred during the freeze-in period of their deployment where the pressures acting on the glass vessel are strongest \cite{2009NIMPA.601..294A}. These failures are likely the result of stress on the breakout cable and its connection. Very few DOMs have suffered from failure of main board electronics components meaning any DOMs that survive through freeze-in will likely have a long lifetime.

\section{Hit Generation}

All data acquisition begins with the registering and processing of photon hits in individual DOMs. The PMT of the DOM is configured so that the photocathode (which converts photons received by the PMT into electrons) is kept grounded while the anode is held at positive high-voltage.  Cherenkov photons from nearby charged secondaries are detected when they intercept the photocathode of the PMT on the underside of the DOM. This generates a small current pulse in the PMT which is subsequently amplified and sent to the main board of the DOM for digitization. After the pulse has been amplified, a local coincidence (LC) signal is sent from the DOM to its nearest and next nearest neighboring DOMs. In the event that a LC signal is then received from one of these neighboring DOMs, it is fed to an ATWD (Analog to Waveform Digitizer) which samples the input pulse 128 times at a rate of 40 Mhz. The capture and digitization process takes 29 $\mu$s, so the main board is equipped with two ATWDs that can run in parallel to minimize the amount of dead time in the DOM \cite{2009NIMPA.601..294A}. If the pulse received from the PMT is longer than the ATWD readout time, an ADC (Analog to Digital Converter) is also present to receive and digitize the signal. Additionally, pulses that fail to trigger LC with other DOMs will still undergo digitization via the ADC rather than the ATWD. The pulses produced by the ADC have much coarser binning in time and therefore poorer resolution on timing of the the pulse.

After digitization the pulses are sent to a FPGA (field-programmable gate array) which handles local coincidence triggering logic, generation of hits, and storing of hit information. This integrated circuit will readout the output from the digitizers and store the information until the hit information is ready to be communicated to the surface. Hits that satisfy the local coincidence criteria are known as HLC (hard local coincidence) while isolated hits that fail to show coincidence in other DOMs are referred to as SLC (soft local coincidence). The designation between HLC and SLC will determine how hits are handled further down the data processing pipeline. While some analyses prefer to work solely in the realm of HLC hit information to minimize the contribution of background effects, many reconstruction algorithms and low-energy analyses will favor the inclusion of SLC hits as they may represent true physics hits from a fainter light source.

Collection of hit information from the DOMs is controlled by DOMHub computers located in the IceCube Laboratory (ICL) at the surface (see Figure \ref{fig:icl}). Each DOMHub machine is responsible for all 60 DOMs on a single IceCube string. The DOMHub computers are equipped with eight DOM readout (DOR) cards each of which is capable of handling communications with up to eight DOMs. The DOR cards signal the run state for the DOM, maintain time synchronization, send software updates, and monitors for any software or hardware failures \cite{2009NIMPA.601..294A}. The entire system is kept synchronized by a master clock updated by GPS and set to UTC. All of these components come together to ensure that all hit times measured by DOMs are reliable without any drift in relative time between separate DOMs or DOMHubs.



\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=1.0\textwidth,keepaspectratio]{ICL.jpg}
  \end{center}
  \caption[IceCube Laboratory Photo]{The IceCube Laboratory at South Pole. DOMHub computers located within the ICL communicate with all DOMs on a respective string. The cables carrying power and information for each DOM arrive at the ICL and enter the building from either the left or right tower near the ceiling (Photo credit: J. Daughhetee).}
  \label{fig:icl}
\end{figure}

\section{Triggering and Event Building}

The stream of DOM hit information arriving in the ICL must be parsed into physics events before any meaningful data analysis can be performed. During the average snapshot of the detector over a short time period ($\sim 10 \mu$s), there will typically only be DOM hits triggered by noise within the detector. In order to select out only interesting events, the IceCube data acquisition system (DAQ) examines the hit information continuously until a certain hit pattern 'triggers' the system to readout the data and construct a physics event. These triggers generally search for a clustering of events coincident in time that are consistent with a particle track or shower. Due to the diversity of analyses in IceCube, there are several triggers optimized for specific physics events. Only the three triggers taken as input for this analysis will be described, though. They can be summarized as follows:

\begin{adjustwidth}{2.5em}{2.5em}
\setlength{\parindent}{0pt}
\textbf{Simple Majority Trigger (8)} - This is the most commonly used trigger in IceCube analyses. This trigger requires that at least 8 DOMs record an HLC hit within a time window of 5 $\mu$s. When the trigger condition is reached, data during a time window defined by -4$\mu$s to +6$\mu$s with respect to the trigger firing time is readout by the DAQ and recorded as an event.

\textbf{Cylinder Trigger (4)} - Instead of solely using a multiplicity requirement, the cylinder trigger attempts to isolate events that show some clustering in space. It defines a cylinder about a DOM with a height of 75m and a radius of 175m. This cylinder encompasses a vertical section of five DOMs on the central string in addition to the nearest neighboring strings. The trigger condition is satisfied if there are 4 HLC hits within this defined volume in a time span of 1$\mu$s. The DAQ will then readout data from over a time window like that used for SMT8 events (-4$\mu$s,+6$\mu$s).

\textbf{DeepCore Simple Majority Trigger (3)} - This trigger works in a similar fashion to the SMT8 trigger. It has a much lower HLC hit threshold (3), but it only looks for HLC hits in DOMs below the dust layer on strings that comprise DeepCore. Additionally, the time window is reduced from 5$\mu$s to only 2.5$\mu$s. The readout window for the DAQ is a bit larger than that for SMT8 (-6$\mu$s,6$\mu$s). The high-quantum efficiency of PMTs in DeepCore DOMs combined with the lower HLC hit threshold result in far more noise-induced triggers than SMT8.
\end{adjustwidth}
\setlength{\parindent}{17.5pt}

The total trigger rate for the IceCube detector is about 9 kHz with the SMT8, DCSMT3, and Cylinder Triggers firing at inclusive rates of 2.3 kHz, 280 Hz, and 4 kHz respectively \cite{I3Live}. Many events will satisfy multiple triggers and/or the same trigger multiple times. The DAQ system will merge all concurrent satisfied triggers into a single event physics event which ultimately yields a triggered event rate of about 3 kHz \cite{I3Live}. This trigger rate produces an enormous volume of data of which only a small portion is transferred to the north via satellite. All triggered events are written to tape, however, and this data is eventually transferred from Antarctica to storage at the IceCube data warehouse. These physics events are the input for all IceCube analyses and the beginning of the analysis chain.

\chapter{Event Selection}
A quick comparison between the rate at which atmospheric neutrinos trigger the IceCube and DeepCore detectors ($\sim$ 10 mHz) and the overall event rate ($\sim 3$ kHz) shows that the data generated by IceCube is very strongly dominated by background. This background is almost entirely due to energetic muons produced in cosmic ray air showers passing through the detector from above. Due to the large range of physics capabilities of the detector, many different filters exist to reduce the data volume and select out events of interest to specific analyses. Event selection for IceCube analyses generally consists of selecting the appropriate filter(s) for the desired signal followed by application of several iterations of cuts optimized to reduce background to an acceptable level while maintaining efficiency with respect to signal events.

The efficacy of these cuts on real neutrino signal is estimated through the use of large amounts of Monte Carlo (MC) simulation of both neutrinos and background muons. An adapted version of the air-shower simulation code CORSIKA (COsmic Ray SImulations for KAscade) is used to generate datasets of background downgoing  muons produced in cosmic ray air-showers \cite{1998cmcc.book.....H}.  Neutrino event simulation is provided by two different event generators named GENIE and Neutrino Generator. The Neutrino Generator or NUGEN is the primary neutrino event simulation code used in IceCube analyses. The NUGEN code is a version of the ANIS neutrino simulation that is modified to be used specifically with IceCube software \cite{2005CoPhC.172..203G}. This code considers many relevant factors in neutrino propagation and interaction including absorption in the Earth, the rock/ice interface below the detector, and neutral current regeneration. However, NUGEN only considers deep inelastic scattering events in the IceCube detector. Other scattering processes become relevant at energies below $\sim$100 GeV, so at these energies and below the accuracy of cross-sections provided by NUGEN become suspect. For this reason we also include neutrino simulation from GENIE (\textbf{G}enerates \textbf{E}vents for \textbf{N}eutrino \textbf{I}nteraction \textbf{E}xperiments) \cite{2010NIMPA.614...87A}. This generator provides more accurate simulation at lower energies due to its consideration of other scattering processes in addition to deep inelastic scattering.

All simulation datasets used during the development of the analysis cuts are listed in Table  \ref{tab:event_sel_sim}. In addition to cut development, GENIE sets 1314, 1414 and NUGEN set 10090 are also used to simulate neutrino signal events for analysis method testing and results calculations. The two neutrino simulation generators are used to predict neutrino event rates for separate energy ranges with GENIE governing predictions between 4-190 GeV and NUGEN simulation covering everything $\geq$190 GeV. The use of different types of neutrino simulation may raise some concern about disagreement in event rate predictions. To ensure a smooth transition at 190 GeV, we normalize the NUGEN simulation so that it is in agreement with GENIE predictions near the transition energy. Background muon simulation is handled primarily by CORSIKA set 9622. Set 9255 provides an additional high-energy component that makes a minor contribution to predicted event rates.
\begin{table}[h]
\caption[Ice Properties Systematic Datasets]{List of simulation sets used in the development of data selection cuts and analysis sensitivity. Background simulation is provided by CORSIKA which simulates the muons produced in cosmic ray air-showers. NUGEN and GENIE neutrino datasets estimate atmospheric neutrino rates as well as the sensitivity of the analysis to neutrino sources. \label{tab:event_sel_sim}}
\begin{center}
\begin{tabular}{ccccc}
\toprule
\textbf{Simulator} & \textbf{Set Number} &\textbf{Type} & \textbf{Spectrum} & \textbf{Energy Range}\\
\midrule
CORSIKA & 9622 & Cosmic Ray Primaries & E$^{-2.6}$ & 600-10$^{5}$ GeV \\
CORSIKA & 9255 & Cosmic Ray Primaries & E$^{-2.0}$ & 10$^{5}$-10$^{11}$ GeV \\
NUGEN & 10090 & $\nu_{\mu}$ + $\bar{\nu_{\mu}}$ & E$^{-2}$ & 10-10$^9$ GeV \\
NUGEN & 10193 & $\nu_{e}$ + $\bar{\nu_{e}}$ & E$^{-2}$ & 10-10$^9$ GeV \\
GENIE & 1314 & $\bar{\nu_{\mu}}$ & E$^{-2.5}$ & 4-190 GeV \\
GENIE & 1414 & $\nu_{\mu}$ & E$^{-2.5}$ & 4-190 GeV \\
GENIE & 1114 & $\bar{\nu_{e}}$ & E$^{-2.5}$ & 4-190 GeV \\
GENIE & 1214 & $\nu_{e}$ & E$^{-2.5}$ & 4-190 GeV \\
\hline
\end{tabular}
\end{center}
\end{table}
The lack of a unified neutrino simulation for this analysis poses some minor inconveniences. This issue will likely be addressed in future iterations of this analysis as the collaboration begins production of GENIE simulation sets encompassing much larger energy ranges. The result for this search will depend on this current GENIE simulation, however,  which is known to have some physical inaccuracies. Sets 1314 and 1414 make some approximations in hadronic propagation and light yield that are known to not be correct. These approximations ultimately lead to a slightly higher than expected event rate for neutrino events below $\sim$50 GeV. These factors may possibly make a non-negligible difference in the final sensitivity of the analysis.

\section{Low-energy Channel}

Because of the primary focus of this analysis on a lower-energy event selection, the DeepCore-dominated low-energy filter stream is taken as input. Selecting only events which pass this filter reduces the trigger-level data rate of 3 kHz to a much more manageable 40 Hz. The low-energy filter attempts to select a relatively background free sample by defining a detection volume about DeepCore that does not extend to edge of the detector. This allows optical sensors outside of the detection volume to serve as dedicated downgoing muon detectors. Events that have hits on DOMs outside the defined detection volume that are causally connected with the hits inside the volume are able to be identified as background muons. A schematic representation of this filtering algorithm is shown in Figure \ref{fig:DCVetoSketch}. 

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=.5\textwidth,keepaspectratio]{DeepCoreVeto.jpg}
  \end{center}
  \caption[DeepCore Veto Algorithm]{Diagram of a downgoing muon traveling through IceCube into DeepCore. The colored circles indicate DOMs triggered by the muon with red representing earlier times and blue representing later times. The times of DOM hits in a defined veto region are compared to the time and location of the center of gravity (COG) of fiducial volume DOM hits in DeepCore. Events that are found to have more than one veto region DOM hit causally connected with the COG in DeepCore are filtered out as likely downgoing cosmic ray muons \cite{2012APh....35..615A}.}
  \label{fig:DCVetoSketch}
\end{figure}

This filter actually consists of two separate streams which are differentiated by the definition of which DOMs comprise the detection (or fiducial) volume and which DOMs are considered a part of the veto region. Inclusion of this additional branch using the relaxed veto allows for increased acceptance of higher-energy upgoing muon neutrino events that may otherwise be cut by the more stringent definition. Figure \ref{fig:TwoLayerDrawing} shows the fiducial boundaries of the standard and expanded branches of the DeepCore filter. The exclusive acceptance rate for the standard and relaxed veto filters is 17.25 Hz and 23.3 Hz respectively. The end result is a reduction in the trigger level data rate of nearly two orders of magnitude yielding a much more manageable data sample on which more advanced background reduction techniques can be applied.
\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=.5\textwidth,keepaspectratio]{TwoLayerDC.png}
  \end{center}
  \caption[Fiducial Volume Definitions]{String location plot detailing the boundaries of the standard (blue shaded region) and expanded (orange shaded region) DeepCore volumes. DOMs belonging to strings inside the boundary and whose vertical position in the detector lies below the dust layer are considered to be inside the fiducial volume.}
  \label{fig:TwoLayerDrawing}
\end{figure}

\section{Analysis Specific Cuts}
After reducing the IceCube data stream to specific filters, the selection criteria imposed by differing analyses will begin to diverge in order to optimize sensitivity to their respective target signal. For this analysis, the data given by the low-energy filter is put through a series of cuts optimized to preserve upgoing and contained track-like events from muon neutrino interactions. What event traits to cut on and to what degree is decided through examination of many cut choices on a sample of simulated muon neutrino events and background simulation. The end result is a selection of event cuts that can be grouped into two categories. The first category consists of cuts derived from detector information in the veto region while the second focuses on event quality and reconstruction characteristics. Finally, a cut developed through machine learning techniques and optimized with respect to analysis sensitivity is applied to achieve a final level data set ready for analysis.

Each iteration of data selection criteria are referred to as `levels'. In IceCube, the output of the various physics filters is known as Level 1 or L1 data. This L1 data is subject to additional processing involving more CPU intensive reconstructions. This post-processed data is referred to as L2, and it is at this level that specific analyzers in IceCube will begin to impose their own selection criteria. We will continue use of this nomenclature to describe the various steps involved in the event selection process specific to this analysis (L3, L4, etc.). Table \ref{tab:event_rates} located in section 7.5 of this chapter provides a level by level summary of the data and MC simulation rates.

Lastly, the event selection process is divided into two catergories based on which branch of the DeepCore filter the event comes from. Events passing the standard filter definition which utilizes a full three surrounding layers of IceCube strings as active veto are referred to as low-energy stream or LES events. Events passing the branch which uses an expanded fiducial volume are referred to as high-energy stream or HES events. As the names suggest, the average energies of events belonging to the HES are higher than those found in the LES branch. The two event streams will largely make use of the same cuts, but there are instances in which the treatment of the two event samples is quite different. These differences will be specifically mentioned.

\subsection{Level 3}
Despite the best efforts of the low-energy filter, downgoing muons created in cosmic-ray air showers still represent the most dominant background at this level. Another large contributor to the event rate at this level is the presence of events triggered by correlated noise in DeepCore DOMs. These noise events consist of a handful of DOMs ($\sim$3-5) in the central part of the detector that all happen to have noise-induced pulses over a time period shorter than the DeepCore trigger window (2.5 $\mu s$). Because these events are almost entirely localized within central DeepCore, these events are only present in the LES portion of the event selection. Due to a lack of proper simulation of these events\footnote{Newer simulation in IceCube now accurately reproduces these correlated noise events.}, it is important to remove them from event processing as early as possible.

The first set of cuts used in L3 focus on isolating and removing noise-induced events. These events generally have very low values of NChannel, a term for the number of DOMs registering light during an event. Additionally, the temporal correlation between the pulses received on these DOMs is not very strong. These characteristics motivate the following event cuts:
\begin{adjustwidth}{2.5em}{2.5em}
\setlength{\parindent}{0pt}
\textbf{MicroHits Cut} - This cut examines a 250 ns time period about the event trigger time to ensure that there are still some hits in spatial and temporal coincidence. The long time window of the DeepCore trigger, 2.5 $\mu$s, can allow noise-induced DOM hits to build up to the required 3 HLC threshold. By cutting on the number of DOM hits and total deposited charge in the DOM PMTs during this 250 ns time period, the number of noise triggered events is significantly reduced.

\textbf{Noise Engine Cut} - The Noise Engine is a modified version of an algorithm designed to identify track-like events in the detector. Its ability to identify tracks has been repurposed so that the algorithm can isolate events that show no track-like characteristics whatsoever. Events whose hits show little to no correlation consistent with a particle track or shower are more than likely noise-induced. The Noise Engine issues the event a pass/fail status.
\end{adjustwidth}
\setlength{\parindent}{17.5pt}
Distribution of hits and charge from the MicroHits cut are shown in Figure \ref{fig:MicroHitAndCharge}. The excess of data with respect to CORSIKA simulation caused by noise triggers is clearly visible.

\begin{figure}
\centering
\subfigure[DOM Hits from MicroHits Cut]{\label{fig:micro1}\includegraphics[width=70mm]{StdDC_MicroHits.png}}
\subfigure[PMT Charge from MicroHits Cut]{\label{fig:micro2}\includegraphics[width=70mm]{StdDC_MicroCharge.png}}
\caption[L3 MicroHits and MicroCharge Distributions]{Plots above show the distributions for number of DOMs hit (a) and total charge deposited (b) for simulation and data. The large excess of data (black) with respect to CORSIKA (red) simulation is explained by the presence of unsimulated noise events.} 
\label{fig:MicroHitAndCharge}
\end{figure}

In addition to these cuts focused on noise events, several cuts designed to eliminate downgoing muons are also employed at this level. These cuts make use of hit information from DOMs belonging to the veto region to search for any indication the event being a downgoing muon. These cuts will typically examine the DOM hits in each event with little or no hit cleaning in order to minimize the chances of leaving out possible veto information. This will also cause a small fraction of potential signal events to be removed from the analysis due to coincident noise hits in the veto region. These cuts include the reapplication of the DeepCore filter with no hit cleaning, a cut on the number of hits allowed in the upper regions of the detector, a cut on events having correlated clusters of veto region hits, examination of the time profile of light deposition in the detector, a cut on the reconstructed vertex position, and finally a cut on the ratio of hits in the veto region to hits in the fiducial region. The exact details of these cuts and plots showing the distributions for simulation and data can be found in Appendix A.

The cuts applied to the HES portion of the event selection are nearly the same. A slightly more stringent cut value is chosen for several of the aforementioned cuts due to the smaller veto region for this branch. Noise events are nearly non-existent in this branch of the event selection, so the MicroHits and Noise Engine cuts are not applied. Not all of the listed veto cuts are used for the HES branch either. Due to the longer muon tracks for $\nu_{\mu}$ signal events in the HES branch, the cut on the hit ratio between the veto and fiducial regions and the cut on the light deposition time profiles are not applied. Events that are part of the LES branch that only fail these two track-energy dependent cuts are transferred to the HES branch instead of being dropped by the analysis in hopes of retaining potentially interesting events. Distributions of cut parameters for HES simulation and data events are also given in Appendix A.

\subsection{Level 4}
The data volume is reduced substantially at this level by removing all events that are reconstructed as downgoing  in the detector (i.e., coming from the Southern sky). The cuts described in the previous level were designed to be used in many low-energy analyses and as such they exclude zenith dependent cuts. This analysis makes no attempt to use downgoing events due to difficulties in eliminating the muon background enabling us to impose a cut on the reconstructed direction. Another round of cuts utilizing veto information is imposed at this level as well. Lastly, a minimum threshold of DOM hits per event is set. The details of these cuts are listed below.
\begin{adjustwidth}{2.5em}{2.5em}
\setlength{\parindent}{0pt}
\textbf{Reconstruction Cut} - This cut marks the first use of information from event reconstruction algorithms in the event selection. The reconstruction used at this stage consists of a six iteration likelihood method called SPE6. The likelihood algorithm of this method assumes that pulses of light received by DOMs during the event arise from single photon hits. A likelihood value is generated by taking the product sum of the probabilities for DOMs being triggered or not triggered by light from a hypothesis track. This likelihood is then maximized to give a best fit to the hypothesis muon track. Six iterations are performed to ensure that the fit does not lock on to a local minimum in the likelihood space. The zenith direction from this reconstruction serves as the cut parameter, and any events whose reconstructed direction is more than 5$^{\circ}$ above the horizon is cut from the analysis. This cut is also applied to the reconstructed zenith provided by the simpler LineFit reconstruction algorithm which acts as a seed for the SPE6 reconstruction.

\textbf{Hit Threshold Cut} - Even after application of cuts specifically crafted to remove noise events in Level 3, there are still some noise-triggered events present at this data level. These events all have low NChannel (number of DOMs registering light in the cleaned hit series) values and feature a random distribution in reconstructed direction. Signal events that also happen to have low values of NChannel are unable to be reconstructed reliably and are generally of poor quality. For this reason and minimum NChannel threshold of 10 is set to remove these events from further processing.

\textbf{Veto Cuts} - There is still some veto hit information that has yet to be utilized in the event selection process. These cuts make use of slightly different hit cleaning techniques in an attempt to retain possible non-noise hits from the veto region. The first cut applies the same hit cleaning technique normally used in the analysis, but with looser cleaning settings. The number of veto region hits that occur prior to the event trigger are counted, and events showing two or more such hits are removed. Yet another application of the original DeepCore filtering algorithm is also applied. This time an extremely loose hit cleaning is performed to maximize the possibility of finding hits in the veto region caused by incoming muons. A cut on the number of veto hits is applied.
\end{adjustwidth}
\setlength{\parindent}{17.5pt}

Most of these cuts are also present in the HES L4 event selection with exception being the additional application of the DeepCore filter algorithm. The HES branch also makes use of a cut on track quality and two separate cuts designed to eliminate coincident muon events. The track quality cut examines the space-angle difference between the LineFit and SPE6 reconstructed directions for events. Events whose reconstructions disagree on the track direction by more than 30$^{\circ}$ are removed. When two muons enter the detector simultaneously, the reconstruction methods can be fooled into reconstructing the event as a single upgoing track rather than two separate events. These coincident muon events are removed by splitting the event into two separate hit maps to search for individual tracks. Two different event splittings are used to try and identify these coincident events.

This suite of cuts results in a reduction of the data rate by more than two orders of magnitude (2.16 Hz $\rightarrow$ 15.2 mHz). The distribution of reconstructed zenith in Figure \ref{fig:SPE6Recos} shows the ability of these reconstruction based cuts to separate background events. At this point the data rate is still an order of magnitude larger than the atmospheric neutrino rate predicted by MC simulation, so there is still a considerable amount of background events remaining. Distributions for both the LES and HES cut parameters are given in Appendix A.

\begin{figure}
\centering
\subfigure[Reconstructed Zenith for LES Events]{\label{fig:res1}\includegraphics[width=70mm]{L4_LES_SPE6ZenithDist.png}}
\subfigure[Reconstructed Zenith for HES Events]{\label{fig:res2}\includegraphics[width=70mm]{L4_HES_SPE6ZenithDist.png}}
\caption[L4 SPE6 Zenith Reconstruction]{Normalized distributions of reconstructed zenith from an 6-iteration SPE fit to nominal DOM hit map. The distributions for LES (a) and HES (b) events show that the bulk of the data at this level consists of downgoing muons. The cut region is given by the shaded areas in the plots above.} 
\label{fig:SPE6Recos}
\end{figure}

\subsection{Level 5}
The selection criteria for this level are solely focused on removing the remaining obvious downgoing muon events. These events have managed to evade previous attempts at removal due to peculiarities in their cleaned DOM hit pattern. Using different hit cleaning settings can sometimes eliminate noise hits that are capable of confusing the reconstruction algorithms. Reapplying the reconstruction methods on these new DOM hit patterns can result in the proper identification of events as downgoing muons. The reconstruction cuts using different hit cleanings are described below.
\begin{adjustwidth}{2.5em}{2.5em}
\setlength{\parindent}{0pt}
\textbf{TightSRT SPE6 Zenith Cut (LES Only)} - The prevalence of noise hits in DeepCore DOMs can often lead reconstructions astray resulting in poor best-fit track hypotheses. The event displays of many of the remaining events at this cut level reveal that they are clearly background muons. However, noise hits present in the DOM hit map used for reconstruction can result in mis-reconstruction of these events. By using a hit cleaning algorithm with tighter settings, we can create a hit map that is less likely to include these noise this. Feeding this map to the reconstruction method will often reveal the true downgoing nature of these previously mis-reconstructed events. Events whose zenith is above 5$^{\circ}$ are removed.

\textbf{SPE2 with Early Hits Removed Zenith Cut} - This cut functions similarly to the TightSRT SPE6 just described. Instead of using new hit cleaning settings, the first two hits from the nominal hit map are dropped. A common class of background event at this level consists of events mis-reconstructed as upgoing due to the presence early hits in the DeepCore region of the detector. In order to incorporate these spurious hits into the fit, the reconstruction method will attempt an upgoing track hypothesis that intersects with the downgoing hit pattern produced by a muon higher up in the detector. A cut is made on the reconstructed zenith of the event after exclusion of the first two DOM hits. Events whose zenith is above 5$^{\circ}$ are removed.
\end{adjustwidth}
\setlength{\parindent}{17.5pt}
These simple cuts result in a reduction of the data rate by a factor of two without incurring substantial $\nu_{\mu}$ signal losses. Distributions of the reconstructed zenith for events from the LES and HES branches are plotted in Figure \ref{fig:NoEarlySPE2}. After these cuts have been made, most of the obvious downgoing muons present in the event sample have been identified and removed. There are still many cosmic ray muon events remaining, but these events have managed to sneak through the outer layers of the detector without depositing much light. Elimination of these background events requires the more advanced techniques described in Level 6.

\begin{figure}
\centering
\subfigure[LES SPE2 Zenith Distribution with Early Hits Removed]{\label{fig:les_spe2}\includegraphics[width=70mm]{LES_NoEarlySPE2ZenithDistribution_L5.png}}
\subfigure[HES SPE2 Zenith Distribution with Early Hits Removed]{\label{fig:hes_spe2}\includegraphics[width=70mm]{HES_NoEarlySPE2ZenithDistribution_L5.png}}
\caption[L5 SPE2 Early Removed Zenith Reconstruction]{Distribution of reconstructed zenith from an 2-iteration SPE fit to a DOM hit map with the earliest two DOM hits removed. The distributions for LES (a) and HES (b) events show that a large population of mis-reconstructed downgoing muons are able to be identified through this method. The shaded are in subplot (a) shows the cut region; this region is the same for subplot (b) although it is not displayed.}
\label{fig:NoEarlySPE2}
\end{figure}
\subsection{Level 6}
Rather than making simple straight cuts on event parameters, the last step in the event selection process makes use of machine learning techniques. The background that still remains at this level is not as easily separated from signal events due to large overlap in the distributions of their event parameters. We can however use the differences in the shapes of these distributions to form a multi-dimensional cut that is optimized for signal retention. Once this cut has been applied, a final straight cut is imposed that ensures event quality for the final level dataset.
\subsubsection{Machine Learning Cut}
The last remaining separation power provided by cut parameter distributions is not easily extracted via simple straight cuts. In order to maximize separation of signal and background, we make use of a machine learning technique known as a boosted decision tree or BDT. Construction of this decision tree requires input signal and background samples for training. The shapes of the BDT input parameters for these two samples are then used to guide the training of the tree.

Building of the decision tree begins through examination of one of the selection variable distributions and choosing a cut value on that variable that leads to the best separation of signal and background \cite{MachineLearning}. This splits the data into signal-like and background-like samples that are referred to as branches. The selection variables are then re-examined and whichever variable provides the best separation power is then used to separate both branches into signal-like and background-like components thus creating a subset of branches. This process is continued on each branch until the sample of that branch has either been completely separated into branches consisting of only signal or background events or there are simply too few events remaining for cuts to be meaningful \cite{MachineLearning}. The terminal nodes of these branches are called ``leaves". Leaves having signal purity greater than 50$\%$ are designated as signal leaves while those having less purity are designated as background leaves. Each of these leaves is then assigned a weight based on the purity it achieves in classification of the training sample.

During the initial creation of the tree all events are given the same weight when evaluating classification branches. Once the tree has been set, any events that are wrongly classified by the tree will then be re-weighted or ``boosted" for the generation of a new decision tree \cite{MachineLearning}. This process is iterated to refine the initial decision tree and to prune branches that are ineffective in correctly identifying signal or background events. The process ends once there are no more misidentified events or a preset limit on number of trees is reached. The end result is the creation of a highly effective sorting tree that gives individual events a score based on its leaf assignment. The tree is then set to work sorting events that were not used during training to check for any indication of oversampling of specific events from the training sample. 

The input parameters for the BDT used at this level are all derived from various reconstruction techniques. The parameters are a reflection of reconstruction quality, track shape and position, and track coincidence with veto region hits:
\begin{adjustwidth}{2.5em}{2.5em}
\setlength{\parindent}{0pt}
\textbf{Finite Reco Z} - The Finite Reco algorithm attempts to fit a starting and stopping point to the reconstructed muon track. Neutrino events present at this level are the result of interactions either inside or below the detector volume. As such, the best fit of event vertex from Finite Reco will typically be located in lower portions of the detector. Background muons may not produce light in the veto regions, but the best fit for the starting vertex of these events will often lie at higher positions in the detector z-coordinate.

\textbf{Finite Reco R} - This parameter is just the distance of the best fit Finite Reco vertex from the central string of DeepCore, string 36. The vertex of neutrino events are more likely to be clustered near the center part of the detector while background muons will tend to have a best fit vertex that lies farther away.

\textbf{Track Veto Charge} - This parameter is obtained through a brute force reconstruction method that searches for possible veto hits. First, the downgoing sky is binned in regions of equal area. Then, a hypothesis track originating from each bin direction is fit to the event vertex in the central detector. All tracks are checked for any DOM hits in the outer region of the detector that are spatially and temporally coincident with the hypothesis track. The track featuring the highest total deposited charge in veto region PMTs is then recorded. This charge value is used as a separtion parameter.

\textbf{Direct Hits} - This parameter serves as a reconstruction quality estimate. The number of direct hits in a reconstruction is given by the number of PMT pulses in DOMs that are caused by unscattered photons from the muon track. A PMT hit is considered ``direct" if the time of the PMT pulse $t_i$ is not significantly later than the expected time of arrival of an unscattered photon from the muon track $t_{dir}$. The definition of a direct hit in this analysis is a hit whose time residual lies in the range $t_{res} = t_{i} - t_{dir} =$ [-25 ns, 150 ns].

\textbf{Reconstruction Reduced Log Likelihood} - The final level reconstruction makes use of a likelihood method to determine the best fit to the data. The highest quality reconstructions will have the lowest values of reduced log likelihood (rllh) which corresponds to a track hypothesis in good agreement with the data. Any background muon events at this level must be mis-reconstructed due to previous cuts on downgoing events. Therefore their reconstruction is necessarily wrong and is unlikely to have a low rllh value indicative of a good fit.

\textbf{Average DOM-Weighted Track Distance} - This parameter deals directly with the details of the reconstruction track geometry. The average distance between DOM hits used in the reconstruction and the track fit is calculated. This average is weighted by the amount of light received by each DOM to minimize noise-hit contribution. Poorly reconstructed tracks will usually have a larger average DOM-to-track distance.
\end{adjustwidth}
\setlength{\parindent}{17.5pt}
These variables are used by the BDT to achieve maximum separation between signal and background samples. The signal sample used to train the BDT consists of well-resolved (space angle $\Delta \Psi_{\nu-reco} \leq 5^{\circ}$) $\nu_{\mu}$ simulation events from GENIE sets 1314 and 1414. Real data is used as the background sample as it still consists primarily of background cosmic ray muons. A separate BDT using NUGEN simulation for signal training is used for events belonging to the HES branch of the event selection. This decision tree also makes use of some of the same selection parameters including reduced log likelihood, average DOM-weighted track distance, and direct hits. An additional parameter known as Direct Length is also used. This parameter is simply the best fit reconstructed event track length using only direct photon hits. The other selection parameters previously described are only used in the creation of the LES branch BDT.

The score distributions for data and simulated events for both branches of the event selection are plotted in Figure \ref{fig:BDTScoreDistros}.
\begin{figure}
\centering
\subfigure[LES L6 BDT Score Distribution]{\label{fig:bdt1}\includegraphics[width=70mm]{LES_BDTScoreDist_NormalizedRates_WC9255_And_H3AC9622_L6.png}}
\subfigure[HES L6 BDT Score Distribution]{\label{fig:bdt2}\includegraphics[width=70mm]{HES_BDTScoreDist_Rates_L6.png}}
\caption[L6 BDT Score Distribution]{Distribution of L6 BDT scores for the LES (a) and HES (b) event selection branches. Data/MC agreement is not particuarly great in the background dominated regions. This is especially the case for the HES sample due to low CORSIKA statistics. Because the background for the analysis is directly measured from the data sample rather than simulated, this is not too troubling of an issue. The signal region of both distributions shows relatively good data-MC agreement. The slight overestimation of the signal rate in the LES branch is likely due to the overestimation of hadronic light yield at low neutrino energies in GENIE sets 1314 and 1414.}
\label{fig:BDTScoreDistros}
\end{figure}
The plots demonstrate the separation power provided by the BDTs. One can obtain an event sample of the desired neutrino purity by simply making a singular cut on the BDT score. For the final sample, cut values are chosen at 0.0 and -0.01 for the LES and HES branches respectively. These cuts yield an event sample in which atmospheric neutrinos comprise the majority of events.

\subsubsection{Quality Cuts}
Application of the BDT cut yields a manageable dataset that should be fairly neutrino pure. Some of the events that remain, however, are of suspect quality and are likely not useful to include in the analysis. By imposing a cut on event quality, we can eliminate some of these events without harming the sensitivity of our analysis. The quality parameter that is cut on is an estimation of the resolution of the event known as paraboloid sigma $\sigma_{para}$. This quantity is derived from the final level event reconstruction (see 7.4), and $\sigma_{para}$ itself is described in detail in section 7.5. For the discussion at hand, $\sigma_{para}$ can simply be understood as a measure of the event resolution.

Events having a value of $\sigma_{para}$ greater than 45$^{\circ}$ are removed from the final sample. The choice of a cut at 45$^{\circ}$ stems from a particular feature present in the analysis code. Overall, this quality cut reduces the data rate by about $10\%$. The events removed by this cut generally produce very few hits within the detector, and therefore their true direction is incredibly difficult to constrain.

One could argue that the timing information provided by such events could still prove useful in identifying potential neutrino flares. However, certain intricacies of the analysis code prevent these events from contributing to the signal hypothesis. During the analysis of final level events, the reconstructed direction of an event will be checked against a hypothetical source location to determine if the event could be associated with that source. This process involves the comparison of two probability density functions (p.d.f.s) which describe the event distributions expected from signal and background respectively \footnote{The mathematical details of these functions are provided in the discussion of the analysis method in chapter VIII.}. In order to decrease computation time, only events whose signal-to-background ($S/B$) ratio is greater than a specified threshold will actually contribute to the likelihood calculation. This effect is visualized in Figure \ref{fig:QualityCutParameterSpace}. The contours indicate different choices for this $S/B$ threshold, and only events with values of spatial separation ($\Delta \Psi$) and resolution ($\sigma_{para}$) that lie within the contour are able to contribute to a signal hypothesis. In practice we have chosen a $S/B$ threshold value of 8 which imposes a restriction on the maximal allowed value of $\sigma_{para}$ at 45$^{\circ}$.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=.65\textwidth,keepaspectratio]{SignalOverBackground_ContourMap.png}
  \end{center}
  \caption[Signal over Background PDF ratio by Event Resolution and Angular Error]{Contour plot showing the ratio of the signal and background p.d.f. values for different choices of event-source separtion ($\Delta \Psi$) and event resolution ($\sigma_{res}$). Parameter choices that lie within the contours have a signal-to-background ratio large enough to be included in the analysis should that contour value of $S/B$ be chosen as the threshold.}
  \label{fig:QualityCutParameterSpace}
\end{figure}

\section{Event Reconstruction}
An accurate reconstruction of neutrino events in the data sample is critical for optimal performance of any pointing analysis. During the event selection process several iterations of reconstructions are performed so that downgoing muons from cosmic rays can easily be identified. During data selection at lower levels, simpler reconstruction algorithms are often used to prevent a prohibitive amount of required CPU time for processing. These reconstructions then serve as the seed for more advanced techniques used at the analysis level. In this section the seed reconstructions for the final level will be discussed in addition to a detailed description of the final reconstruction whose results are used in the analysis.

Reconstruction of a physics event within the IceCube detector depends on the geometry of the DOMs registering light from the event, the time of arrival for photons at those DOMs, and the total amount of light received at the individual DOMs. The final reconstruction used on events in this analysis makes use of a likelihood approach to derive a best fit muon track to the DOM hit pattern. This is accomplished by calculating the probability of each DOM hit in the event originating from photons from a hypothesis muon track. The probability of DOMs seeing \textit{no} light from that track is calculated as well for all DOMs that fail to register any hits. A likelihood function that depends on track location and direction in addition to the location and timing of DOM hits is constructed from the product sum of all the hit/no-hit probabilities. The form of this function is given in Eq. \ref{eq:RecoLLH}.
\begin{equation}
\label{eq:RecoLLH}
\mathcal{L} = \prod_{i}^{N_{hits}} P_{hit}(t_{res},d_i,\Psi_i,A_i) \cdot \prod_{N_{hits}+1}^{N_{DOM}} P_{no-hit}(d_i,\Psi_i)
\end{equation}
where the hit probability $P$ depends on the observed time delay in arrival of a photon from the track $t_{res}$, the distance between the muon track and the DOM $d_i$, the angle between the PMT and track Cherenkov cone $\Psi_i$, and the amplitude of the pulse generated in the DOM $A_i$. The timing residual $t_{res}$ is simply the difference in the arrival of the photon with respect to the time one would expect from an unscattered photon. If the track fit is accurate, then events with $t_{res} \approx 0$ are likely unscattered during propagation and are referred to as direct hits.

In order to acquire a good fit to the actual data, the reconstruction method maximizes the value of the likelihood function under different choices of track position and direction. Maximization of this function leads to a best fit on the track parameters and therefore an estimated direction and location of the secondary muon produced in the muon neutrino interaction. The value of the likelihood for the best fit can be used as an estimation of the goodness-of-fit for the reconstruction. Usually the reduced log-likelihood value, -log$_{10}(\mathcal{L})$, of events is compared to isolate which events are best reconstructed.

The reconstruction used at the final level functions somewhat differently from the likelihood based reconstructions used at earlier levels. Whereas earlier methods assumed that the pulses generated in DOMs are the product of single photon hits (SPE), the final level method assumes DOM pulses can originate from any multiplicity of photon hits (MPE). By dropping the single photon assumption, finer resolution of the track parameter likelihood space is achieved leading to an improvement in the average event resolution \cite{2004NIMPA.524..169A}.

Previous reconstructions made use of the empirically motivated Pandel function to estimate the probability of seeing a hit in a DOM at distance $d$ from the hypothesis track with time delay $t_res$ \cite{2004NIMPA.524..169A}. The function has the form
\begin{equation}
\label{eq:Pandel}
P(t_{res}) = \frac{1}{N(d)}\frac{\tau^{-d/\lambda}\cdot t_{res}^{(d/(\lambda-1))}}{\Gamma(d/\lambda)} \cdot e^{-(t_{res}(\tau ^{-1}+\frac{c_{med}}{\lambda _a})+\frac{d}{\lambda _a})}
\end{equation}
with
\begin{equation}
N(d) = e^{-d/\lambda _a} \cdot \left(1+\frac{\tau \cdot c_{med}}{\lambda _a}\right)^{-d/\lambda}
\end{equation}
where $\Gamma$ is the Gamma function, $c_{med}$ is the speed of light in the medium, $\lambda _a$ is the ice absorption length, and $N(d)$ is a normalization factor. The $\tau$ and $\lambda$ terms are free parameters that are functions of spatial parameters. These terms are determined through Monte Carlo simulation to achieve a good fit to the measured data. This function provides a decent estimate of the actual distribution of arrival times for photons arriving at the DOM from a source located at a distance. 

Rather than use this function to generate hit probabilities, the final level reconstruction makes use of the best estimate of ice properties within the detector. This is done through the use of These tables are produced via direct photon propagation simulation resulting in binned values of estimated light yield from a source in the ice. Depth in the ice, zenith angle dependence, and spatial orientation between observer and light source are factored into the tables. To prevent complications from binning effects, spline interpolation is applied to the ice tables to ensure smooth variation of optical properties in the ice. Probability for DOM hits is then calculated by comparing the observed light pulse to the expectation from the splined photon tables. By accurately modeling the ice properties, the resolution of the likelihood method is improved.
\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=.45\textwidth,keepaspectratio]{RecoChain.png}
  \end{center}
  \caption[Reconstruction Chain]{Reconstruction chain used for this analysis. Less cpu-intensive reconstruction techniques are used to seed more advanced methods used at the final event level. The directional reconstruction provided by SplineMPE16 is what is ultimately used in analysis of the data.}
  \label{fig:recochain}
\end{figure}
The final reconstruction uses an MPE likelihood method and splined photon tables in place of the Pandel function. The likelihood fit is iterated 16 times to obtain a best fit track to the data. The entire reconstruction chain is shown in Figure \ref{fig:recochain}. This reconstruction provides good resolution for the more energetic events in the data sample due to the brightness and length of their muon tracks. The event display for a well-reconstructed muon neutrino event found in the final sample can be seen in Figure \ref{fig:HESEventWithReco}. The best fit reconstructed track is given by the red arrow.

Resolution on the primary neutrino is less than 10$^{\circ}$ for events of energies $\geq 50$GeV. Reconstruction performance continues to improve at higher neutrino energies and will asymptotically approach 1$^{\circ}$ resolution for energies greater 250 GeV. At lower primary neutrino energies, the ability of the method to correctly identify the neutrino direction suffers considerably (see Figure \ref{fig:EventRes}). Since these reconstruction methods were primarily designed for energetic muon tracks, it is not surprising that they are not quite as effective for the short stubby muon tracks produced by low energy events. The improvement of reconstruction techniques at low energies is currently an active area of development in IceCube analyses. It is likely that future versions of this point source analysis will obtain far greater levels of sensitivity via improvement of the resolution of low energy neutrino events.
\begin{figure}
\centering
\subfigure[Neutrino Resolution by Energy]{\label{fig:res1}\includegraphics[width=100mm]{FinalLevel_NeutrinoResolutionByEnergy_JustGENIE.png}}
\subfigure[Neutrino Resolution by Number of DOMs]{\label{fig:res2}\includegraphics[width=100mm]{GENIE_FinalLevel_NeutrinoResolutionByNch.png}}
\caption[Final Level Event Resolution]{Median event resolution as a function of primary neutrino energy (a) and as a function of DOMs receiving light in the event (b). These distributions are for events belonging to the low-energy branch (LES) of the event selection.}
\label{fig:EventRes}
\end{figure}
\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=1.0\textwidth,keepaspectratio]{HES_Upgoing_WithReco.png}
  \end{center}
  \caption[Final Level Reconstructed Event Display]{Event display for a final level neutrino track event with reconstructed track. This event is fairly energetic with respect to the usual events in the final sample. The red arrow shows the best fit from the 16-iteration SplineMPE reconstruction. The dust layer is clearly visible by the lack of DOMs registering light in the middle of the event track.}
  \label{fig:HESEventWithReco}
\end{figure}

\section{Error Estimation}
Given a reconstruction for an event, it is essential to have an accurate estimation of the possible error of that reconstruction. While it is impossible to know this error on an event-by-event basis, we can examine a large set of simulated events to obtain a statistical estimation of how well an individual event will be resolved. The reconstruction method used for events in this analysis utilizes a likelihood technique in which the directional parameter space is scanned over to optimize the fit. Events that are well-reconstructed will generally have a likelihood space with a strong minimum well centered about the true direction of the muon. On the other hand, the likelihood space of poorly resolved events is usually characterized by a very broad minimum in which there is no strong preference for a certain direction.

The shape of the likelihood space from event reconstruction correlates quite strongly with the resolution of the event. We fit this likelihood space through the Paraboloid fitter developed by Till Neunh\"{o}ffer \cite{2006APh....25..220N} which attempts to fit an error ellipse to the likelihood space. To obtain a fit to the shape of the likelihood space for directional parameters only, the position and time of the particle track in the detector is removed from the likelihood calculation so that the dimensionality of the likelihood space is reduced to a two-dimensional surface (see Figure \ref{fig:pcoord1}) \cite{2006APh....25..220N}. This two-dimensional space should exhibit a minimum well about the best fit reconstruction direction. If one assumes that shape of this well is well approximated by a two-dimensional paraobloid surface with Gaussian curavature, a confidence ellipse can be fit to the likelihood space whose boundaries are given by the 1-$\sigma$ width of the Gaussian \cite{2006APh....25..220N}. Once a fit is achieved, the resolution of the event can be estimated from the width of the major and minor axes of this ellipse. An example of such an ellipse is given in Figure \ref{fig:pcoord2}. It should be mentioned that this process can be extended to the spatial dimensions of the likelihood fit as well to obtain a fit on the error of particle vertex location, but this functionality is not used in this particular analysis.
\begin{figure}
\centering
\subfigure[Reconstruction Track Coordinate System]{\label{fig:pcoord1}\includegraphics[width=70mm]{TrackCoordinateSys.png}}
\subfigure[Paraboloid Sigma Definitions]{\label{fig:pcoord2}\includegraphics[width=70mm]{ParabSigma.png}}
\caption[Reconstruction Track Coordinate System and Paraboloid Sigma Definition]{(a) The coordinate system for muon tracks in reconstruction algorithms. (b) Definition of the paraboloid sigmas derived from the reconstruction likelihood space. Width of the major and minor axes of the potential well are given by $\sigma_{1,2}$ while $\sigma_{\theta , \phi}$ give the width of the likelihood space in reconstruction directional coordinates \cite{2006APh....25..220N}.}
\label{fig:ParaCoord}
\end{figure}

Although the estimated width of the likelihood minimum can differ greatly in the $\phi$ and $\theta$ directions, it is usually desirable for the sake of simplicity to obtain only a single error estimation parameter. This is done by adding the widths of the major and minor axes of the 1-$\sigma$ ellipsoid in quadrature to obtain a single value $\sigma_{para}$.
\begin{equation}
\sigma_{para} = \frac{\sqrt{\sigma_{1}^2 + \sigma_{2}^2}}{\sqrt{2}}
\end{equation}
By doing so, we effectively approximate the likelihood minimum fit by an ellipsoidal Gaussian with two widths as a circular minimum fit by a radially symmetric Gaussian with width $\sigma_{para}$. This single value is then used to estimate the angular error of the event. The confidence that the true direction lies in the angular phase space encompassed by this 1-$\sigma_{para}$ region corresponds to the containment of the 1-$\sigma$ region of a two-dimensional Gaussian, i.e. 39$\%$.

In practice, the error prediction given by $\sigma_{para}$ tends to underestimate the true error on the neutrino direction and needs to be rescaled. The degree to which the $\sigma_{para}$ underestimates the true error is energy dependent as well. This is referred to as paraboloid pull and is defined as the ratio of the paraboloid error estimation $\sigma_{para}$ and true neutrino error $\Delta\Psi$. Using signal neutrino simulation, we can examine the paraboloid pull distribution for our final level sample. The pull as a function of NChannel (an energy proxy) is shown in Figures \ref{fig:pull1} and \ref{fig:pull2} for the LES and HES branches respectively. The median pull values for each NChannel bin are given by the white circles. The change in median pull value as a function of NChannel is then fit by a fourth degree polynomial providing a parameterization of the paraboloid pull for both branches of the event selection.
\begin{figure}
\centering
\subfigure[LES Paraboloid Pull]{\label{fig:pull1}\includegraphics[width=70mm]{Sample2_Nch_Vs_ParaboloidPull_LESNuMuGENIE_UnWeighted_NeutrinoError.png}}
\subfigure[HES Paraboloid Pull]{\label{fig:pull2}\includegraphics[width=70mm]{Sample2_Nch_Vs_ParaboloidPull_HESNuMuNugen_UnWeighted_MuonError.png}}
\subfigure[LES Corrected Paraboloid Pull]{\label{fig:pull3}\includegraphics[width=70mm]{Sample2_Nch_Vs_Corrected_ParaboloidPull_NuMu_UnWeighted_MuonError.png}}
\subfigure[HES Corrected Paraboloid Pull]{\label{fig:pull4}\includegraphics[width=70mm]{Sample2_Nch_Vs_ParaboloidPull_HESNuMuNugen_UnWeighted_MuonError_CORRECTED.png}}
\caption[Paraboloid Pull Distributions]{Paraboloid pull distributions for LES (a) and HES (b) event branches as a function of NChannel (energy proxy). The white circles give the median pull value for each NChannel bin. A four degree polynomial is fit to the median values so that events can have their $\sigma_{para}$ values adjusted by their respective NChannel. Distributions (c) and (d) show the pull for the corrected paraboloid values. Adjusting by NChannel leads shows agreement between the median pull value and the true error in neutrino direction.}
\label{fig:ParaboloidPull}
\end{figure}

Using this parameterization of the paraboloid pull, the paraboloid sigma for each event is then adjusted so that the median pull value for each NChannel bin is approximately unity. This brings the error estimation in line with the actual neutrino error predicted from Monte Carlo simulation. The corrected pull distributions are shown in Figures \ref{fig:pull3} and \ref{fig:pull4}. The dispersion of pull values for the HES branch is reasonably low. Thus, the corrected paraboloid value should provide a good estimate of the point spread function or PSF for these events. However, the pull distribution for the LES branch shows that there is a large degree of dispersion in pull values for low NChannel events. Many of these events lack well-behaved likelihood spaces due to scarcity of hits in the detector. Ultimately, this results in non-optimal agreement between the PSF expected from paraboloid and the true PSF for these events. Comparison of the PSF model used during analysis and the true PSF for simulated neutrino events is shown in Figure \ref{fig:PSFModel}.
\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.8\textwidth,keepaspectratio]{CorrectSigmaVsKentDistribution_Multi_Nu.png}
  \end{center}
  \caption[Event PSF Modeling]{The histograms in the plot above show the distribution in angular error for final level events in a given corrected $\sigma_{para}$ range. The PSF is modeled by the Kent Fisher distribution \cite{1982}, which is essentially a two-dimensional Gaussian normalized to a sphere. The peaks of the actual error distribution tend to be closer to the true direction than the PSF model expects. This is due to the long tails of the error distributions driving the pull correction to higher values. Use of a more sophisticated PSF in future iterations of this analysis will potentially lead to greater sensitivity.}
  \label{fig:PSFModel}
\end{figure}

\section{Final Level Data}
After all cuts have been applied, we are left with a sample of 22,040 events during the observation period with an expected neutrino purity of about 90$\%$. The bulk of these events are neutrinos of atmospheric origin and they represent an irreducible background for the analysis. While the sample does still contain some background cosmic ray muon events, these events do not greatly affect the sensitivity of our time-dependent analysis at the emission timescales expected from our target sources. As we are not trying to make any precision measurements on the atmospheric neutrino spectrum or oscillation parameters, we are able to be a bit loose with our cuts so that signal retention is maximized. The level-by-level data rates expected from simulation as well as the actual data rates are listed in Table \ref{tab:event_rates}. Distributions for various event parameters including reconstructed azimuth, zenith, and event vertex location are shown in Appendix B. Figure \ref{fig:LESEventFinal} shows an event display of a muon track from a typical final level neutrino event in the LES branch of the event selection. 

\begin{table}[h]
\caption[Final level data rate.]{Summary of event rates at each selection level. The amtospheric muon and neutrino rates are estimated through the use of Monte Carlo simulation (MC). The discrepancy between the data rate and summed MC rates at pre-final levels (Filter,L3,L4,L5) can largely be attributed to unsimulated correlated noise events in the detector in addition to a small contribution from $\nu_e$ events. Many early level data cuts focus on these noise events, which are properly handled in newer simulation datasets.\label{tab:event_rates}}
\begin{center}
\begin{tabular}{cccc}
  \hline
 \textbf{Event Type} &\textbf{ Cosmic Ray $\mu$} &\textbf{ Atmospheric $\nu_{\mu}$} &\textbf{ Collected Data}\\
\hline
Filter Level & 38.6 Hz & 7.8 mHz & 40.5 Hz \\ 
Level 3 & 1.79 Hz & 5.58 mHz & 2.16 Hz \\ 
Level 4 & 7.65 mHz & 1.64 mHz & 15.2 mHz \\ 
Level 5 & 2.5 mHz & 1.35 mHz & 6.3 mHz \\ \hline
\textbf{L6 -- Final Level}     & \textbf{0.065 mHz} & \textbf{0.81 mHz} & \textbf{0.774 mHz} \\ \hline
\end{tabular}
\end{center}
\end{table}

We estimate the sensitivity of this event selection to potential neutrino source fluxes by calculating a neutrino effective area through the use of our muon neutrino Monte Carlo simulation. Events are propagated through the detector and forced to interact to prevent simulation of non-interacting events. These events are then given a weight derived from their probability of interaction, the spectrum by which they were generated, the total number of events generated, and the geometrical area over which they were generated. The simulation events that remain after all data selection criteria have been applied are then binned in energy. The weighted number of final events in each can then be compared to the number initially generated to determine what fraction would actually be detected. Taking the product of the geometrical area over which events were generated and the fraction of events that are detected at final level yields a neutrino ``effective" area for the detector. The effective are for the presented analysis after all cuts is shown in Figure \ref{fig:EffAreaFinal}.
\begin{figure}\label{fig:EffAreaFinal}
\centering
\subfigure[Analysis Effective Area NUGEN + GENIE]{\label{fig:goo}\includegraphics[width=75mm]{LowEnTransient_EffArea_GENIE_Nugen.png}}
\subfigure[Analysis Effective Area with just GENIE]{\label{fig:boo}\includegraphics[width=75mm]{LowEnTransient_EffArea_GENIE_DiffBin.png}}
\caption[Final Level Neutrino Effective Area]{(a) Muon neutrino effective area after all event selection cuts have been applied. The green line is given by the low-energy GENIE simulation while the black corresponds to the effective area predicted by NUGEN. The red dashed line shows the effective are for a traditional IceCube point source analysis. The presented selection method results in a higher effective are for sub-100 GeV neutrino events. (b) Close-up of the effective area at lower energies.} 
\end{figure}

Comparison of the effective area provided by this selection to previous point source selections shows an improvement at the lowest of detectable energies in IceCube. While the presented method may not be as sensitive as previous searches for harder astrophysical sources ($F_{\nu}\sim$E$^{-2}$), we can expect this method to outperform the standard selection for transient sources featuring soft spectra or spectra with a low energy cutoff. Therefore this selection complements IceCube's previous efforts quite nicely as it allows us to increase our sensitivity to a broader range of possible sources. 

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=1.0\textwidth,keepaspectratio]{LESEventForThesis.png}
  \end{center}
  \caption[Final Level Event Display]{Event display for a final level neutrino track event originating in DeepCore. The colored spheres represent DOMs that have registered a hit during the event. The size of the spheres are indicative of the total light received by the PMT on that DOM. The color denotes the timing of the hit with red corresponding to earlier times and blue corresponding to later times.}
  \label{fig:LESEventFinal}
\end{figure}

\chapter{Systematic Effects}
There are many systematic uncertainties that can affect the interpretation of the results of this analysis. The primary contributors to this uncertainty are the \textit{in situ} scattering and absorption properties of the ice medium and the absolute quantum efficiency of the PMTs within the DOMs. While there are other errors that could be considered here, e.g. the absolute neutrino cross-section, the contribution they provide to the overall error is negligible.

\section{Ice Properties}
The optical properties of the subsurface ice at the South Pole represent the most difficult systematic effect to adequately measure. This is largely due to the fact that this detection medium is inaccessible from the surface making direct measurement of the optical properties impossible without taking an ice core sample. The scattering and absorption lengths in the ice greatly impact how light will propagate from interaction secondaries. Increased scattering can delay the arrival of photons to DOMs giving a larger spread in possible event interaction times. Assuming an incorrect absorption length can be problematic as well as it leads to inaccurate estimation of the total energy deposited by the neutrino event in addition to incorrect estimation of the threshold energy of the analysis. If one hopes to reconstruct neutrino events with high enough accuracy for pointing, then it is necessary to develop a detailed and accurate model of the ice in which the detector is located.

Precise modeling of the polar ice is an ongoing task in the IceCube collaboration. Several iterations of models have reduced what was once a severe systematic problem to a relatively mild contribution to total systematic error. Current ice models consider several factors including depth dependence, tilt of ice layers, direction of glacial flow, and grain size distribution \cite{2013arXiv1301.5361I}. The absolute values of the optical properties such as absorption and scattering length are not known, however. Therefore we generate simulation that assumes different values for these parameters to estimate how large of an impact any error in the model will make at the final level of the analysis. To determine the degree to which this uncertainty affects our analysis, three datasets with differing values of absorption and scattering lengths are examined at the final level. The specifics of these datasets are listed in \ref{tab:ice_sets}.

\begin{table}[h]
\caption[Ice Properties Systematic Datasets]{List of simulation sets used to estimate systematic error from improper modeling of the ice properites. The listed percentages give the dataset's deviation from the nominal values. Variation in the absorption and scattering lengths lead to fairly minor changes in the analysis effective area with the strongest effects seen at lower energies.\label{tab:ice_sets}}
\begin{center}
\begin{tabular}{ccccc}
\toprule
\textbf{Simulator} & \textbf{Set Number} &\textbf{Spectrum} & \textbf{Scattering} & \textbf{Absorption}\\
\midrule
NUGEN & 10039 & E$^{-2}$ & -- & +10$\%$ \\
NUGEN & 10040 & E$^{-2}$ & +10$\%$ & -- \\
NUGEN & 10041 & E$^{-2}$ & -7.1$\%$ & -7.1$\%$ \\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.7\textwidth,keepaspectratio]{LowEnTransient_EffArea_SysNugen_IceEffect_300GeVTrunc.png}
  \end{center}
  \caption[Ice Systematic Effective Area Comparison]{Effective area at final event level as a function of energy for different possible ice properties. Increased absorption and scattering lead to very little degradation of the neutrino effective area at even the lowest energies. The subplot below the effective are shows the bin-by-bin ratio of the systematic effective area to that of the nominal effective area.}
  \label{fig:IceSysEffArea}
\end{figure}
Examination of the effective areas plotted in Figure \ref{fig:IceSysEffArea} indicate that the variation of ice model parameters does not lead to a particularly large change in the final level effective area. While this source of systematic error had previously been quite dominant in IceCube analyses, the development of more sophisticated and accurate ice models has done much to alleviate this uncertainty. It is evident that the change in effective area between systematic sets is energy dependent, however. This is likely due to the fact that lower energy events will produce fewer hits in the detector. While the lack of a few hits due to higher absorption or scattering is unlikely to prevent an energetic event from triggering the detector, losing even a single photon hit can result in a threshold event lacking a sufficiently bright signal to pass through all selection criteria. For this analysis we will take a conservative approach to our systematic error by assuming the worst case scenario from the systematic studies. In this case, we will assume the contribution to systematic error provided by inaccurate modeling of the ice to be a 3$\%$ effect. This value corresponds to the degree to which the effective area is reduced at the lowest energies of the analysis under the scenario in which ice absorption is increased by 10$\%$.

\section{DOM Quantum Efficiency}
The absolute value of the quantum-efficiency of in-ice DOMs is not entirely constrained. Measurements of the quantum-efficiency of all DOMs were taken prior to deployment, but the deployment process and subsequent years spent in the ice may have changed the sensitivity of each DOM differently. Because DOM efficiency relates directly to the ability of the DOMs to measure light, incorrect estimate of this parameter will lead to comparable error in any results derived from the analysis. There have been attempts within the collaboration to quantify how well the efficiency is known through dedicated study of minimally ionizing muons from the data. These studies have allowed us to limit the possible range of values the true efficiency could take.

In our consideration of DOM efficiency as a source of error for this analysis, simulation datasets with varying levels of DOM efficiency are used. As was done for the ice systematic sets, the final level effective area for the analysis using the different DOM efficiency sets will be studied. Unlike the ice systematic sets, the efficiency sets listed in Table \ref{tab:eff_sets} were not generated independently. All sets originate from a single parent dataset generated with DOM efficiency set to 120$\%$ of the nominal value. This parent dataset is only processed to the DOM hit level prior to any triggering or filtering. To generate the listed datasets at their respective efficiencies, a fraction of the photons present in the parent dataset are dropped from each event in accordance with the desired efficiency. We consider a relative large 10$\%$ deviation from nominal efficiency to determine the analysis method's sensitivity to DOM efficiency error.

\begin{table}[h]
\caption[DOM Efficiency Systematic Datasets]{List of simulation sets used to estimate systematic error from possible differences in simulated and real DOM efficiency. The listed percentages give the dataset's deviation from the nominal values. These datasets are all derived from a single GENIE dataset produced with 120$\%$ DOM efficiency. The sets are then scaled down to their respective efficiencies by dropping the appropriate fraction of photon hits from the dataset events.\label{tab:eff_sets}}
\begin{center}
\begin{tabular}{cccc}
\toprule
\textbf{Simulator} & \textbf{Set Number} &\textbf{Spectrum} & \textbf{DOM Efficiency} \\
\midrule
GENIE & 800011 & E$^{-2}$ & Nominal  \\
GENIE & 800011 & E$^{-2}$ & 90$\%$ \\
GENIE & 800011 & E$^{-2}$ & 110$\%$ \\
\hline
\end{tabular}
\end{center}
\end{table}

The effective areas for the systematic efficiency sets are plotted in Figure \ref{fig:DOMSysEffArea}. As was the case for variation in ice parameters, the largest difference between the efficiency datasets is found at low energy. This is also most likely due to threshold events being disproportionately affected by changes in photon sensitivity. The trend in energy is clearly visible in the plot of the ratios of the systematic and nominal effective areas (Figure \ref{fig:DOMEffEnergyTrend}). The worst case scenario will also be used to determine how large of an effect we might expect from simulating DOM efficiency improperly. The effective area at lower energies shows an approximate 10$\%$ decrease from the nominal value for the dataset featuring 90$\%$ efficiency. Therefore, this 10$\%$ value is chosen as the contribution from possible error in DOM efficiency to the total systematic error.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.7\textwidth,keepaspectratio]{LowEnTransient_EffArea_SysGENIE_DomEfficiency.png}
  \end{center}
  \caption[DOM Efficiency Systematic Effective Area Comparison]{Effective area at final event level as a function of energy for different possible DOM efficiency settings. At lower energies, the change in effective area becomes approximately proportional to the relative change in DOM efficiency. The subplot below the effective are shows the bin-by-bin ratio of the systematic effective area to that of the nominal effective area.}
  \label{fig:DOMSysEffArea}
\end{figure}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.7\textwidth,keepaspectratio]{GENIE_EffectiveAreaRatioTrend_DomEff.png}
  \end{center}
  \caption[DOM Efficiency Systematic Effective Area Ratio]{Ratio of systematic set effective areas to the nominal effective area. The plot above shows that the degree to which the analysis is affected by DOM efficiency is clearly energy dependent.}
  \label{fig:DOMEffEnergyTrend}
\end{figure}

If one considers the worst possible scenario, the errors in ice model $\sigma_{ice}$ and DOM efficiency $\sigma_{eff}$ show a degradation in the analysis effective area of 3$\%$ and 10$\%$ respectively. We combine these sources of error by adding the individual contributions in quadrature like so
\begin{equation}\label{eq:syserror}
\sigma_{sys}^2 = \sigma_{ice}^2 + \sigma_{eff}^2
\end{equation} 
This yields a total systematic error correction to the effective area of 10.44$\%$. This error is folded into the results of the analysis by adjusting the effective area used in limit calculations by -10.44$\%$. The effective area adjusted for systematic effects is plotted along with the nominal effective area in Figure \ref{fig:SysAreaWithNom}.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.7\textwidth,keepaspectratio]{LowEnTransient_EffArea_GENIE_WithSystematicAdjustedArea.png}
  \end{center}
  \caption[Final Level Neutrino Effective Area with Systematic Error Adjustment]{Effective area at final event level as a function of energy. The black line gives the analysis effective area after adjustment for systematic error while the nominal effective area is plotted as the dashed line.}
  \label{fig:SysAreaWithNom}
\end{figure}

\chapter{Analysis Method}

The analysis presented in this thesis makes use of both directional and timing information from the final level event dataset. The techniques that are used in this analysis have been applied to other IceCube event selections in a similar fashion. As of yet, these time-dependent searches focused on standard IceCube events have yet to find any neutrino sources with higher significance than what is expected from background fluctuations \cite{2012ApJ...744....1A}. A very thorough overview of the likelihood analysis methods used in previous time-dependent IceCube analyses is given by Braun, et al. \cite{2010APh....33..175B}. The method detailed in the following section is mostly the same, but there are a few minor modifications made to the process to improve performance on a low energy event selection.

\section{Unbinned Likelihood Method}
The identification of a statistically significant astrophysical signal hidden among a high number of background events is a difficult problem in astronomy. Likelihood based methods are commonly used to address this issue. Searches utilizing a likelihood method are able to provide a probabilistic interpretation of a signal hypothesis with respect to background fluctuations for the event dataset being examined. These searches will typically make use of timing, directional, and occasionally reconstructed energy information from events to find clustering indicative of a true astrophysical source. Whether or not individual events qualify as signal or background when testing a source hypothesis can differ depending on whether the search is `binned' or `unbinned'. For binned analyses, the signal hypothesis generates a set of conditions that an event must meet to be considered as signal-like. This usually consists of selecting an area about the source location (a bin) that the reconstructed direction of the event must lie in. Events are then classified as either belonging to signal or being attributable to background. This results in all events having a binary status with respect to association with a hypothetical source. 

The unbinned method, which is the method chosen for this analysis, is typically more sensitive as it allows for events to treated as both signal and background. In order to accomplish this, probability density functions (p.d.f.s) are constructed that are representative of the expected spatial, temporal, or energy distributions for both signal and background events. Thus, well-resolved events will strongly contribute to the likelihood calculation as signal- or background-like while more marginal events can make a contribution to either scenario with appropriate weighting instead of being sharply divided into either category. This makes the unbinned method much better suited for analysis of event samples with a wide range of resolutions.

The probability for seeing an event $i$ in our analysis given a time-dependent source hypothesis takes the following form:
\begin{equation}\label{eq:EventProb}
\mathcal{P}_i(|\mathbf{x}_i-\mathbf{x}_s|,n_s,t_i,t_o,\sigma_w,\sigma_i) = \frac{n_s}{n_{\mathrm{tot}}} \mathcal{S}_i + \left(1-\frac{n_s}{n_{\mathrm{tot}}}\right) \mathcal{B}_i
\end{equation}
where $S_i$ and $B_i$ are the signal and background p.d.f.s introduced below. The values of these p.d.f.s depend on the reconstructed direction of the event $\mathbf{x}_i$, the angular resolution of the event $\sigma_i$, the arrival time of the event $t_i$, the location of the hypothetical source $\mathbf{x}_s$, the mean time of the source flare $t_o$, the duration of the flare $\sigma_w$, the total number of events in the dataset $n_{tot}$, and lastly the number of signal events $n_s$. The formulation of event-by-event probability given by Eq. \ref{eq:EventProb} provides a quantifiable method through which we can weigh various signal hypotheses for the dataset against the null hypothesis in which none of the events are the result of a signal source.

\subsection{Signal P.D.F.}
The signal p.d.f. describes the expected distribution in time and space for neutrino events that originate from the source model being tested. The signal p.d.f. consists of both a spatial and temporal part and is constructed in this way
\begin{equation}
\mathcal{S}_i(|\mathbf{x}_i-\mathbf{x}_s|,t_i,t_o,\sigma_w,\sigma_i) = S_i(|\mathbf{x}_i-\mathbf{x}_s|,\sigma_i) \cdot T_i(t_i,t_o,\sigma_w)
\end{equation}
where
\begin{equation}
S_i(|\mathbf{x}_i-\mathbf{x}_s|,\sigma_i) = \frac{\kappa}{4\pi \sinh \kappa} \exp \left(\kappa \cos |\mathbf{x}_i-\mathbf{x_s}|\right)
\end{equation}
and
\begin{equation}
T_i(t_i,t_o,\sigma_w) = \frac{1}{\sqrt{2\pi}\sigma_w} \exp \left(-\frac{(t_i-t_o)^2}{2 \sigma_w^2}\right)
\end{equation}
The function representing the spatial term $S_i$ is the Kent-Fisher distribution \cite{1982}. The inclusion of the this distribution in the spatial p.d.f. represents a slight deviation from the standard construction of $S_i$ in IceCube analyses. It is analogous to the two-dimensional Gaussian distribution on a flat surface, and for small values of $\sigma_{i}$ ($\lesssim 3^{\circ}$) the distributions are nearly identical. However, the Kent-Fisher distribution is properly normalized to the surface of a sphere. It therefore gives a better description of the spatial p.d.f. for events with larger uncertainties which are common in the lower energy sample used in this analysis. The concentration parameter $\kappa$ can be understood as the counterpart of the Gaussian sigma for the Kent-Fisher distribution, and it is determined by the event resolution ($\kappa = \sigma_{i}^{-2}$). The temporal term $T_i$ is simply a Gaussian with mean time of $t_o$ and a width of $\sigma_w$. Other source time profiles could be considered for this analysis, but testing has shown that the choice does not strongly affect the analysis sensitivity. Due to the untriggered nature of this analysis, the Gaussian profile is chosen to keep the method simple and relatively model independent. 
	
\subsection{Background P.D.F.}
The dataset being examined is heavily background dominated. This allows us to simply derive our background p.d.f. directly from the final level dataset without having to make any assumptions about what the background should look like. Thus, the background p.d.f. looks like 
\begin{equation}
\mathcal{B}_i(\mathbf{x}_i,t_i) = P_{BkgDec}(\delta_i)\frac{P_{BkgAz}(\alpha_i)}{T}
\end{equation}
where $\delta_i$ and $\alpha_i$ are the event's reconstructed declination and detector azimuth respectively and $T$ is the total livetime of the analysis. The azimuthal and declination p.d.fs are taken from distributions of final level data as shown in Figure \ref{fig:BkgOnly_Zen_And_Azi}. By constructing the p.d.f. directly from the dataset we are able to fold in the declination dependence of both the muon and neutrino background as well as the difference in detector response for certain declination bands. The IceCube detector is azimuthally symmetric, but the triangular lattice formed by its constituent strings do produce preferred corridors for background events to sneak through. This effect is fairly minor at the final event level, nonetheless it is also taken into consideration via the $P_{BkgAz}(\alpha_i)$ term. Lastly, the time dependence of the background is assumed to be flat. While there is seasonal variation in the atmospheric muon and neutrino rates, these modulations are not large at the final event level and the timescale of variation is much greater than the expected duration of neutrino emission from the target sources.

\begin{figure}
\centering
\subfigure[Final Level Event Azimuth Distribution]{\label{fig:alpha}\includegraphics[width=75mm]{BkgOnly_LES_SplineMPE_AzimuthDist_FinalLevelDist.png}}
\subfigure[Final Level Event Zenith Distribution]{\label{fig:beta}\includegraphics[width=75mm]{BkgOnly_LES_SplineMPE_ZenithDist_FinalLevelDist.png}}
\caption[Background Zenith and Azimuth Distributions]{Distribution of final dataset events in zenith and azimuth in detector coordinates. These distributions are used to create the spatial terms in the background p.d.f. used in the likelihood calculation.} 
\label{fig:BkgOnly_Zen_And_Azi}
\end{figure}

\subsection{Likelihood Function}
In order to find the best fit for the source parameters $n_s$, $t_o$, $\sigma_w$ at a specified location $\mathbf{x}_s$, an optimizable likelihood function is needed. This function is given by the product sum of all individual event probabilities from the dataset:
\begin{equation}\label{eq:LLH}
\mathcal{L}(\mathbf{x}_s,n_s,t_o,\sigma_w) = \prod \mathcal{P}_i(|\mathbf{x}_i-\mathbf{x}_s|,n_s,t_i,t_o,\sigma_w,\sigma_i)
\end{equation}
The value of this function will depend on both the properties of events within the dataset and the values chosen for the signal parameters $\mathbf{x}_s,n_s,t_o,\sigma_w$. Evaluating the value of the likelihood function with $n_s=0$ corresponds to a purely background hypothesis for the dataset in which there are no events from flaring sources. Specific signal hypotheses for the dataset will have their own values of $\mathbf{x}_s$, $n_s$, $t_o$, and $\sigma_w$ which will potentially yield higher values of the likelihood function $\mathcal{L}$ depending on how well the hypothesis fits the dataset. One can evaluate the merit of two different hypotheses for the dataset by performing a likelihood ratio test in which the ratio of likelihood function values for different hypotheses is evaluated to produce a test statistic $\lambda$ like so
\begin{equation}\label{eq:LLHRatio}
\mathcal{\lambda} = \frac{\mathcal{L}(\eta_1 | X)}{\mathcal{L}(\eta_2 | X)}
\end{equation}
where $\eta_{1,2}$ are the choices of signal parameters and $X$ is the dataset to which the hypotheses apply. In the ratio given by Eq. \ref{eq:LLHRatio}, large values of $\lambda$ (i.e. $\lambda>1$) indicate a favoring of hypothesis 1 by the dataset while values below unity indicate the data is better modeled by hypothesis 2.

The particular formulation of the likelihood ratio used in this analysis differs slightly from Eq. \ref{eq:LLHRatio} but it is functionally the same. Rather than compare competing signal hypotheses, we find a best fit to the data by comparing signal hypotheses to the null hypothesis for which $n_s=0$. We then maximize the likelihood ratio through variation of the signal terms to yield a best fit to the data. The exact form of the likelihood ratio used in this process is given by
\begin{equation}\label{eq:unmaxedts}
\lambda = 2\log \left[\mathcal{M}(\sigma_w,T)\frac{\mathcal{L}(\mathbf{x}_s,n_s,t_o,\sigma_w)}{\mathcal{L}(n_s = 0)} \right]
\end{equation}
with $\mathcal{L}(n_s = 0)$ being the null hypothesis and $\mathcal{L}(\mathbf{x}_s,n_s,t_o,\sigma_w)$ as the signal hypothesis being tested. The additional $\mathcal{M}(\sigma_w,T)$ term located within the brackets is a marginalization term whose purpose will be discussed below.

The test statistic defined by Eq. \ref{eq:unmaxedts} has the property that the distribution of $\lambda$ values from datasets consisting of only background events are well-modeled by a $\chi^2$ distribution with degrees of freedom equivalent to the number of parameters being fitted. Because the background distribution is $\chi^2$ distributed, Wilks's theorem can be used to estimate the probability or p-value of seeing a clustering of neutrino events with a test statistic value $\hat{\lambda}$ \cite{wilks1938}. This p-value allows the method to reliably identify the most statistically significant neutrino flare in the dataset. 

The previously mentioned marginalization term $\mathcal{M}(\sigma_w,T)$ is introduced into the test statistic definition to ensure that values are $\chi^2$ distributed. It is defined as follows, 
\begin{equation}
\mathcal{M}(\sigma_w,T) = \frac{\sqrt{2\pi}\sigma_w}{T}
\end{equation}
where $\sigma_w$ is the width of the neutrino flare and $T$ is the livetime of the dataset under examination. The test statistic distribution for time-independent point source searches in IceCube takes the form of a $\chi^2$ distribution without this additional term. However, use of the time-independent version of the likelihood in time-dependent analyses showed that the test statistic is no longer $\chi^2$ distributed. This is due to a bias towards shorter flare duration in the analysis method. For a dataset of a given livetime, it is possible to divide the data into many more short flares than long flares. This creates an effective trials factor for shorter flares and therefore the test statistic definition must be changed. The introduction of the marginalization factor $\mathcal{M}$ brings the background $\lambda$ distribution back into agreement with the appropriate $\chi^2$ distribution \cite{2012ApJ...744....1A}.

Maximization of $\lambda$ through variation of the flare parameters $n_s$, $t_o$, and $\sigma_w$ at a fixed location $\mathbf{x}_s$ yields a test statistic
\begin{equation}
\hat{\lambda} = 2\log \left[\frac{\sqrt{2\pi}\hat{\sigma}_w}{T}\frac{\mathcal{L}(\mathbf{x}_s,\hat{n}_s,\hat{t}_o,\hat{\sigma}_w)}{\mathcal{L}(n_s = 0)} \right]
\end{equation}
where $\hat{n}_s$, $\hat{t}_o$,and $\hat{\sigma}_w$ are the optimized flare parameters and $\hat{\lambda}$ is the maximized value of test statistic. This result gives the most significant signal-like hypothesis for the data. The value of the maximized test statistic serves as figure of merit that characterizes the degree to which the data is better explained by this best fit signal hypothesis than the background only scenario. Because this is an untriggered analysis, we are not selecting any specific locations to test for flaring. Therefore the test statistic must be maximized at each possible location in the sky to search for the most significant hotspot in the entire dataset. The details of this process are described in the following section.

\section{Sky Scan}

The analysis performed is not a triggered search, and therefore it is necessary to examine the entire solid angle domain of the analysis for any possible transient sources. The difficulty in rejecting background muons at lower energies limits the analysis to up-going and horizontal events ($< 5^{\circ}$ above the horizon). Because of IceCube's location at the South Pole, this results in a search over all right ascension in a declination band ranging from -$5^{\circ}$ to $90^{\circ}$. The search method discretizes the northern portion of the sky into many bins, and the coordinates of these bins serve as the location of a hypothetical flaring source to be tested. The fairly large median resolution of the event sample (see Fig. \ref{fig:EventRes}) allows the size of the search bins to be set to a relatively coarse 2$^{\circ}$ by 2$^{\circ}$. 

Maximization of the likelihood given by eq. \eqref{eq:LLH} is done at each location in the grid with $\mathbf{x_s}$ set to the location of the bin. This results in each bin having a best-fit neutrino flare with its own values of $\hat{n_s}$, $\hat{t}_o$, $\hat{\sigma}_w$, and test statistic $\hat{\lambda}$. After this scan over the 2$^{\circ}$ by 2$^{\circ}$ bins is completed, a finer follow-up scan with 0.5$^{\circ}$ by 0.5$^{\circ}$ binning is performed on any coarse bins whose best fit flare has a test statistic value $\hat{\lambda}$ with an estimated p-value more significant than a preset threshold ($-\log_{10}(p_{val}) > 1.75$). Following the completion of the fine-scan, the best-fit flare from the bin with the most significant test statistic is returned as the final result of the search. The results of a scan over a scrambled dataset consisting of only background events is shown in Figure \ref{fig:NullTrialSkyMap}.
\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=1.0\textwidth,keepaspectratio]{NullMap996.png}
  \end{center}
  \caption[Sky Map of Results from a Background Scrambled Trial]{Sky map in celestial coordinates of pre-trials p-values for best-fit flares per bin for a randomized dataset. Random map generated by scrambling arrival times of real events. The black circle shows the location of the most significant flare found by the method.}
  \label{fig:NullTrialSkyMap}
\end{figure}

\section{Significance and Trials Factors}
In order to determine which bin has the most significant flare, we evaluate an estimated p-value based on the maximized test statistic $\lambda$ for that bin. The distribution of test statistic values for individual bins is assumed to be $\chi^2$ distributed, however the background distribution of test statistic values from the most significant flare identified in the sky scan is not known \textit{a priori}. Therefore it is necessary to perform many iterations of the analysis method using scrambled versions of the dataset to determine the distribution of the test statistic of the most significant flare with no real signal present in the data. Background datasets are generated by scrambling the time of arrival of the events. Due to the detector's location at the South Pole, this effectively scrambles the events in azimuth while keeping the declination distribution the same. The desired level of statistical significance determines the number of background trials required, e.g. determining what value of the test statistic constitutes a 3$\sigma$ outlier would require approximately $10^4$ scramblings. The background test statistic distribution for $2\times 10^4$ such scramblings is shown in \ref{fig:NullScramblingDistribution}.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.98\textwidth,keepaspectratio]{TestStatisticDistribution_Null.png}
  \end{center}
  \caption[Background Test Statistic Distribution]{Distribution of maximized test statistic $\lambda$ for $2\times 10^4$ searches performed on randomized datasets. Dashed lines mark the location of one-sided $\sigma$ deviations.}
  \label{fig:NullScramblingDistribution}
\end{figure}

Once this distribution is adequately determined, the test statistic from any result we obtain from the analysis can be compared with the distribution of test statistic values generated from background trials. This allows us to determine the probability of seeing a flare as or more significant as the flare observed from just background fluctuations. This is sometimes referred to as the ``p-value of the p-value", and it is essentially the true p-value for the flare after properly accounting for all trials factors. This corrected p-value will be used when citing the significance of the final analysis result. 

The scrambled background trials also serve as a check for any biases in the analysis method with respect to best-fit parameters. The recovered flare parameters for these trials show no strong pull towards certain values, though their is some mild declination dependence which is expected (the atmospheric background distribution is declination dependent). The distributions of best fit flare parameters for background trials can be seen in Figure \ref{fig:scramble_trials_parameters}. The symmetry of the detector is evident through the lack of preference in the azimuthal location of the best-fit flares. Additionally, the smooth distribution in recovered flare times reveals that the analysis method is able to avoid locking on to specific timescales.

\begin{figure}
\centering
\subfigure[Right Ascension]{\label{fig:a}\includegraphics[width=70mm]{RightAcensionDistribution_Null.png}}
\subfigure[Declination]{\label{fig:b}\includegraphics[width=70mm]{DeclinationDistribution_Null.png}}
\subfigure[Signal Strength $n_s$]{\label{fig:c}\includegraphics[width=70mm]{NSignalDistribution_Null.png}}
\subfigure[Flare Width $\sigma_w$]{\label{fig:d}\includegraphics[width=70mm]{SigmaDistribution_Null.png}}
\caption[Background Trial Flare Parameters]{Distribution of flare parameters for the most significant flares identified by analysis method in $2\times 10^4$ trials on scrambled background-only data. The values plotted are the location of the flare in R.A. (a) and Declination (b). Also, the distribution of flare strengths measured in number of signal events is shown in (c) while the best fit flare duration is given by (d).} 
\label{fig:scramble_trials_parameters}
\end{figure}

\section{Analysis Sensitivity}

We calculate the sensitivity of this method by checking how well the search is able to pick out various levels of signal strength from background fluctuations. This procedure begins by selecting a point in the sky to perform our likelihood maximization. A background dataset is then generated by scrambling the time of arrival of the events. The likelihood is then maximized for this set of scrambled data and the p-value obtained from the best fit is stored. This process is iterated many times to build a background p-value distribution at the chosen location in the sky. Injected signal events are now included in addition to the scrambled background data. Events are injected from an assumed source spectrum with a Poisson mean of $n_s$ and once again the likelihood is maximized to obtain a p-value for the best fit flare on the dataset featuring injected signal. The mean $n_s$ for signal injections is increased until the desired fraction of p-values from signal injection trials (typically 90\%) beat the median p-value from the background only distribution. This is repeated for several different timescales for a generic source with an $E^{-3}$ spectrum in Figure \ref{fig:SensE3}.
\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=.8\textwidth,keepaspectratio]{LowEnTransient_NEventDiscoveryPotentialANDSensitivity_E3.png}
  \end{center}
  \caption[Analysis Event Sensitivity]{Calculated event sensitivity of the analysis at Declination $\delta$=$16^{\circ}$ for an $E^{-3}$ spectrum. The flat slope of the sensitivity at short timescales indicates flare durations for which the expected rate of accidental background events is very low.}
  \label{fig:SensE3}
\end{figure}

The discovery potential (which is also plotted in Figure \ref{fig:SensE3}) is calculated in a similar fashion. Rather than building a background p-value distribution, a threshold p-value is chosen. In this case, the threshold value is set to the probability equivalent to that of a one-sided 5$\sigma$ deviation. Injections are performed until the mean value of injected $n_s$ results in injection trials whose recovered p-value exceeds the threshold p-value 50\% of the time. Since we are only examining a single location in the sky, this does not take into account any trials factors that arise from performing the scan over the whole northern sky. It is clear that the sensitivity of this method will suffer greatly for flares of longer duration. This limits the application of the method to sources with duration of approximately $10^4$s or shorter. However, given that the analysis was developed specifically for short transient sources such as jetted emission from core-collapse SNe ($\Delta T \sim 1-100s$), this degradation in performance at longer timescales is not a major be an issue.

\chapter{Results}
Applying the defined analysis method on the unscrambled dataset yields a skymap of the pre-trials p-values derived from the maximized test statistic for each bin. This map is shown in Figure \ref{fig:RealSkyMap}. The black circle in Figure \ref{fig:RealSkyMap} shows the hottest spot after the completion of the sky scan. The best fit to flare parameters for this hot spot are listed in Table \ref{tab:best_fit_flare}. The lack of a high test statistic value in any adjacent bins in the sky map indicates that our hottest spot is dominated by a single highly resolved track event. Examination of the most signal-like events for this flare reveals that this is indeed the case. Figure \ref{fig:TopEvents} shows the spatial p.d.f. values for these events in addition to their time of arrival. Because the event with the highest spatial p.d.f. value is well-localized ($<$1$^{\circ}$ estimated error), its signal-like contribution to the likelihood is limited solely to the bin it in which its reconstructed direction lies.

\begin{table}[h]
\caption[Best-fit signal parameters]{Best-fit values for flare location, duration, strength and time. The value listed for -log$_{10}(p)$ is the probability of seeing such a flare before applying the appropriate trials factor correction. The real probability of seeing a flare as or more significant as the one observed is given by the post-trials corrected $p$-value listed in the last row.\label{tab:best_fit_flare}}
\begin{center}
\begin{tabular}{cc}
  \toprule
 \textbf{Flare Parameter} &\textbf{ Best-fit Value} \\
\midrule
R.A. & 268.75$^{\circ}$ \\ 
Dec & 54.25$^{\circ}$ \\ 
$\hat{n}_s$ & 13.528 \\ 
$\hat{t}_0$ & 56107.8 MJD \\ 
$\hat{\sigma}_w$ & 5.89 days \\ 
-log$_{10}(p)$ & 4.1751 \\
Post-trials $p$ & 0.56 \\
\end{tabular}
\end{center}
\end{table}

%% SkyMap
\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=1.0\textwidth,keepaspectratio]{RealResultSkyMap.png}
  \end{center}
  \caption[Results Sky Map]{Sky map of pre-trials p-values for best fit flares per bin. The black circle identifies the location of the most significant flare found at RA = 268.75$^\circ$ and Declination = 54.25$^\circ$.}
  \label{fig:RealSkyMap}
\end{figure}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.6\textwidth,keepaspectratio]{SpatialPDFValue_EventsArrivingNearFlareTime.png}
  \end{center}
  \caption[Time of Arrival for Most Signal-like Events]{Plot above shows the signal spatial p.d.f. values for several of the most signal-like events for the best fit flare. The y-axis denotes the value of the spatial p.d.f. while the x-axis gives the time of arrival for the event in MJD. The blue Gaussian is the time profile for the best fit flare found by the method.}
  \label{fig:TopEvents}
\end{figure}

The pre-trials significance of this flare was found to be about 3.82 $\sigma$. However, this flare represents the most significant flare found after examination of the entire sky, and so the true significance must factor in a trials penalty. The exact penalty that must be factored in is not known \textit{apriori}, but it can be determined from background trials via the method described in section 9.3. The test statistic value for our result flare is compared to the distribution of test statistics from the most significant flares found in background-only trials in Figure \ref{fig:PostTrialsComparison}. Comparing the best-fit flare result from the analysis to this background distribution yields a highly background compatible trials-corrected $p$-value of 56$\%$. Despite the low significance of the best-fit flare, some cursory checks with high-energy astrophysical source catalogs were performed to see if any known sources may be associated with the found flare. The TeVCat catalog \cite{TeVCat}, which catalogs TeV gamma ray sources, showed no known sources at or nearby the flare location. An additional check with the IAU list of known supernovae also revealed no known sources in temporal and spatial coincidence with the best-fit flare result \cite{SNeCatIAU}.

%% Post-trials p-value
\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.75\textwidth,keepaspectratio]{TestStatisticDistribution_WithResult.png}
  \end{center}
  \caption[Test Statistic Distribution With Result]{Distribution of test statistic $TS =$ log$_{10}(\lambda)$ of most signficant flare found in 1,985 background trials. The test statistic value for the best fit flare on the unscrambled data set is given by the dashed line.}
  \label{fig:PostTrialsComparison}
\end{figure}

The flare found by the analysis method is entirely consistent with a dataset consisting of only background events suggesting no significant sources of neutrino emission for which the analysis is sensitive. In light of this null result, we can construct an upper limit on the neutrino flux of an unobserved neutrino flare that may have occurred during the observation period. Making this limit requires some assumptions about the shape of the neutrino spectrum. The generic source limit provided in this chapter will assume a simple E$^{-3}$ power-law neutrino spectrum from a source with a Gaussian time profile. A detailed examination of the neutrino emission model for the primary target source of this analysis, choked GRBs, is given in the following chapter.

The standard IceCube procedure in the event that the $p$-value of the result of the analysis is greater than 50$\%$ is to derive an upper limit from the calculated sensitivity of the analysis (which by definition corresponds to a $p$-value of 50$\%$). Figure \ref{fig:eh} shows the event sensitivity of the analysis for a neutrino source with a E$^{-3}$ spectrum. This plot details the number of neutrino events $n_s$ required for a source to be visible to the analysis method with 90$\%$ confidence. In order to calculate a limit on source neutrino flux, we must determine the magnitude of the flux required to generate the number of events in the detector given by the event upper limit. The relationship between neutrino flux and number of expected events is given in Eq. \ref{eq:n_obs_from_flux}.
\begin{equation}\label{eq:n_obs_from_flux}
N_{exp} = \int dEA_{eff}(E)F_{\nu}(E)dE
\end{equation}
Integration of the analysis effective area $A_{eff}$ with an assumed neutrino flux $F_{\nu}$ yields the event expectation in the detector. Setting $N_{exp}$ to our calculated event upper limit from Figure \ref{fig:eh} gives the required normalization constant for the neutrino flux. Once the normalization has been determined, the upper limit can be given in terms of time-integrated flux (Figure \ref{fig:bee}). The degradation of the limit at longer flare widths is due to the increasing probability of background events to be found in the on-source region.
\begin{figure}\label{fig:time_int_flux_limit}
\centering
\subfigure[Event Upper Limit for E$^{-3}$ Spectrum]{\label{fig:eh}\includegraphics[width=100mm]{LowEnTransient_NEventDiscoveryPotentialANDSensitivity_E3.png}}
\subfigure[Time-Integrated Flux Upper Limit for E$^{-3}$ Spectrum]{\label{fig:bee}\includegraphics[width=100mm]{LowEnTransient_TIFluxSensitivity_E3_MergedSim_ActualFinalCut.png}}
\caption[Generic Neutrino Spectrum Limits]{(a) Event upper limit as a function of flare width for an E$^{-3}$ source flux. Event upper limit is defined as the number of signal events required for a source to be detectable by the analysis method with 90$\%$ confidence. (b) Upper limit on the total time-integrated neutrino flux for a neutrino flare with E$^{-3}$ power law emission. Flux upper limit derived directly from event upper limit and the analysis effective area.} 
\end{figure}


\chapter{Limits on Choked GRBs}
The upper limits described in the previous chapter provide an estimation of the sensitivity of the analysis method with respect to a generic neutrino source. If we assume a specific source model, however, the event upper limit given by the analysis result can be used to examine the detectability of the source under certain choices of model parameters. In this chapter we will examine the neutrino emission model for choked GRBs (see section 4.4) in light of this source class being the primary motivation for the development of this analysis.

The model for neutrino emission put forward by Razzaque, M\'{e}sz\'{a}ros and Waxman predicts a doubly broken power law with break energies set by hadronic and radiative cooling of the parent pions of the neutrinos \cite{2004PhRvL..93r1101R}. The model was extended upon by Ando and Beacom to include neutrinos from kaon decay resulting in a slightly more complicated spectral shape due to the different break energies for the respective mesons \cite{2005PhRvL..95f1103A}. In the combined model, the neutrino spectrum shape and total fluence depend primarily on the energy E$_{j}$ and bulk Lorentz factor $\Gamma_{b}$ of the relativistic jet in which the parent mesons are produced. For this reason, we examine the RMW/AB model in E$_{j}$--$\Gamma_{b}$ phase space to determine what choices of E$_{j}$ and $\Gamma_{b}$ would have been detectable by the analysis method if a choked GRB occurred during the livetime of the analysis. We evaluate limits on a range of possible values for E$_{j}$ and $\Gamma_{b}$ that includes the nominal values predicted by the model (see Table \ref{tab:chkgrb_range}). This range of values should encompass most physically realistic choices for jet parameters.

\begin{table}[h]
\caption[Choked GRB Parameter Limit Range]{Range of jet parameters for which limits on potential choked GRBs is calculated. Specific limits for 20 values of both $\Gamma_b$ and E$_j$ are calculated.}
\begin{center}
\begin{tabular}{cccc}
\toprule
\textbf{Jet Parameter} & \textbf{Minimum Value} &\textbf{Maximum Value} & \textbf{Nominal}\\
\midrule
Bulk Lorentz Factor $\Gamma_b$ & 0.5 & 10.0 & 3.0 \\
Jet Energy E$_j$ & 10$^{50.5}$ & 10$^{52.5}$ & 10$^{51.5}$ \\
\hline
\end{tabular}
\end{center}
\label{tab:chkgrb_range}
\end{table}

\section{Parameter Dependent Upper Limit}
The event upper limit for each combination of jet energy and Lorentz boost values is calculated via the method described in section 9.4. This method injects signal events into the likelihood analysis from neutrino with a Poisson mean of $n_s$. The distribution in energy for these injected events is set by a specified injection spectrum representative of the source model that is being tested. The value of $n_s$ is varied until the recovered p-value for 90$\%$ of the injection trials exceeds the median background p-value in significance. This $n_s$ value is then cited as an event upper limit for the analysis.

Because the quality of neutrino events varies with energy, the upper limit calculated via the method just described is spectrum dependent. Signal injections from softer spectra are more likely to select low energy events whose resolution may be quite poor. Due to the likelihood construction of the analysis method, it may take several more events of poor quality to match the significance provided by a few highly resolved events. Therefore, signal injections featuring harder spectra will have lower event upper limits in comparison to softer spectra.

The choked GRB neutrino spectrum, described in 4.4.1, exhibits spectral breaks due to hadronic and radiative cooling of pions and kaons. The energies at which these breaks occur are dependent on the values of the jet parameters $\Gamma_b$ and E$_j$. Thus, it is necessary to calculate the event upper limit for each possible choice of these values. It should also be noted that the event upper limit calculation depends on the emission timescale as well. The neutrino emission timescale predicted for choked GRBs is between 10-100s. This motivates our choice of 10$^{-3}$ days ($\sim$86 s) as the width of the Gaussian time profile used in event injection.

We must also consider how the event upper limit changes as a function of source declination. The final level neutrino effect area does indeed show declination dependence. As a result, the event upper limit for a sample source can vary as much as a factor of two based on the source's position in the sky. We compensate for this effect by calculating the event upper limits for six separate declination bands centered about declination values of 0$^{\circ}$, 16$^{\circ}$, 30$^{\circ}$, 45$^{\circ}$, 60$^{\circ}$, and 75$^{\circ}$. A weighted average of these upper limits is then calculated where the values for each limit are weighted by the solid angle of their respective declination band. In this way, a sky-averaged event upper limit is obtained.

A two-dimensional histogram detailing the upper limits for all combinations of $\Gamma_b$ and E$_j$ values is displayed in Figure \ref{fig:ParameterDep_UL}. The color scale indicates the number of events required for injections to be seen with a p-value more significant than the median background p-value with 90$\%$ confidence. The plot clearly shows that fewer events are required to detect bursts featuring higher values of $\Gamma_b$. This is due to the strong dependence on $\Gamma_b$ for both the hadronic and radiative spectral break energies ($E_{b}^{had} \sim \Gamma_b^{5} \cdot E_j^{-1}$ and $E_{b}^{rad} \sim \Gamma_b^{1}$).
\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=1.0\textwidth,keepaspectratio]{UpperLimit_2DHisto_SysAdj.png}
  \end{center}
  \caption[Choked GRB Parameter Dependent Upper Limit]{A two-dimensional histogram of the calculated event upper limit for different choices of $\Gamma_b$ and E$_j$. The upper limit value is given by the bin color. The event upper limit depends solely on the shape of the neutrino spectrum.}
  \label{fig:ParameterDep_UL}
\end{figure}

\section{Visibility Distance}
The parameter dependent event upper limits can be used to determine the strength of the neutrino fluence that would be required to detect a choked GRB with the presented analysis method. Once the required neutrino fluence is known, the distances at which the choked GRB event would be visible can be determined. The process of converting the event upper limit into a limit on the visibility distance is similar to that used to construct the time-integrated flux limit given in Chapter 10. We will need to use a version of the event expectation formula from Eq. \ref{eq:n_obs_from_flux}:
\begin{equation}\label{eq:n_obs_from_chkgrb}
N_{exp} = \int A_{eff}(E) \cdot F_{\nu}(E, E_j, \Gamma_b, D_{ref})dE
\end{equation}
In the equation above, $N_{exp}$ is the event expectation in the detector, $A_{eff}(E)$ is the neutrino effective area, and $F_{\nu}(E, E_j, \Gamma_b, D_{ref})$ is the neutrino fluence at Earth for a choked GRB with bulk Lorentz factor $\Gamma_b$ and jet energy $E_j$ at a reference distance $D_{ref}$ (which is set to 10 Mpc for this calculation). Using Eq. \ref{eq:n_obs_from_chkgrb}, the number of expected neutrino events in the detector from a sample choked GRB for various parameter choices can be calculated. If the event expectation at 10 Mpc is known, we can then determine the distance at which the event expectation will match the previously calculated event upper limit.

The neutrino fluence at Earth should be proportional to the inverse square of the distance to the choked GRB source. If this is the case, then the visibility distance is given by
\begin{equation}\label{eq:vis_dist}
D_{vis} = D_{ref} \cdot \left(\frac{Event\: U.L.}{N_{exp}}  \right)^{-0.5} \: \: \text{(Mpc)}
\end{equation}
where $Event \: U.L.$ is the upper limit for the choked GRB with bulk Lorentz factor $\Gamma_b$ and jet energy $E_j$ and $N_{exp}$ is given by Eq. \ref{eq:n_obs_from_chkgrb}. This distance corresponds to maximal distance at which a choked GRB event would have been detectable with the analysis method with 90$\%$ confidence. A histogram of the visibility distance for possible choices of jet parameter values is shown in Figure \ref{fig:ParameterDepVisDist}.
\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=1.0\textwidth,keepaspectratio]{DistanceLimit_2DHisto_WithContours_SysAdj.png}
  \end{center}
  \caption[Choked GRB Visibility Distance]{Histogram of visibility distance for choices of choked GRB jet parameters. Reference distance contours are provided for easier interpretation of the plot. For particularly energetic bursts with large boost factors, the distance to which the analysis method is sensitive is quite far. However, choked GRBs with more pessimistic jet parameter values would only be visible in our own galaxy. The yellow star denotes the bin for the canonical model parameters.}
  \label{fig:ParameterDepVisDist}
\end{figure}

Conversion of the event upper limit into a visibility distance provides an arguably more intuitive way to interpret the result. The regions of parameter space for which the analysis limits are meaningful will be restricted to visibility distances at which the expected number of choked GRBs during the observation period is not minuscule. This distance also serves as a good measure of the detection feasibility for choked GRBs with certain values of jet parameters. If one only considers canonical values of jet energy, then for $\Gamma_b > 5$ the visibility range of this analysis begins to approach distances upwards of 10 Mpc which would probe interesting regions of high star formation rate such as M82 \cite{1979AN....300..181K} and NGC 660 \cite{1995AJ....109..942V}.

\section{Volumetric Rate Limit on Choked GRBs}
The upper limits provided by the analysis result can also be interpreted as limits on the rate of core-collapse supernovae featuring soft jets in the nearby universe. This rate limit depends on the size of the monitored volume during the livetime of the analysis. The size of this volume will of course depend on what values for the jet parameters we assume. To obtain this rate limit, we will use the parameter dependent visibility distance from section 11.2. The derivation of this volumetric rate limit is not complicated, but it does require some assumptions that must be clarified.

The visibility distance given in Eq. \ref{eq:vis_dist} implies that the analysis method is capable of monitoring a volume of the nearby universe. Combining $D_{vis}$ and the solid angle acceptance of the analysis $\Omega_A$ yields
\begin{equation}\label{eq:vis_vol}
V_{90} = \frac{\Omega_{A}}{3}D_{vis}^{3} \: \: \: \: \text{(Mpc$^3$)}
\end{equation}
The monitored volume $V_{90}$ corresponds to the volume in which a choked GRB event would have been detected by the analysis method with 90$\%$ confidence. This in turn can be converted into a rate limit, but doing so requires two important assumptions:
\begin{enumerate}
\item \textbf{Jet Orientation} - The jets produced during core-collapse of the progenitor are \textit{always} aligned with Earth.
\item \textbf{Homogeneity of the Nearby Universe} - We assume a uniform distribution of possible progenitor locations rather than factoring in the actual distribution of nearby galaxies.
\end{enumerate}
Our treatment of the rate limit could factor in the opening angle of the jet rather than assuming alignment with Earth. However, doing so would require us to commit to a model of the jet opening angle. We therefore only consider the simple case in which the jet is always aligned. The second assumption that the nearby universe is homogeneous is obviously incorrect, but the effect of this assumption on the derived limit is not large compared to the other uncertainties present in this calculation.

Under these assumptions, we can arrive at rate limit using the monitored volume $V_{90}$
\begin{equation}\label{eq:v90}
R = \left(\frac{U.L.(0|\mu)}{\tau \cdot V_{90}}\right) \; \text{(Mpc$^{-3} \cdot$yr$^{-1}$)}
\end{equation}
where $\tau$ is the livetime of the analysis and $U.L.(0|\mu)$ is the null observation upper limit on the number of choked GRBs that occurred in our monitored volume with background expectation of $\mu$. Although no choked GRB events were observed, the observation time is not nearly long enough to place a stringent limit on the yearly rate. We choose to use the Neyman construction of the upper limit \cite{1937} for $U.L.(0|\mu)$, but in order for us to determine its value we need to understand the background rate $\mu$.

The background expectation $\mu$ is defined here as the number of false positive flares arising from background coincidence that we expect per search. To determine this number, we must examine how likely it is for background fluctuations to reproduce flares with significance comparable to that of injections with strength $n_s = Event \: U.L.$. First, we find the median test statistic value $\lambda_{inj}^{med}$ for injections of strength equivalent to the event upper limit at a specific declination. We then perform the sky scan used in the analysis method over this declination band using a randomized dataset with no signal injections. The test statistic values returned by this scan comprise a distribution of background test statistic values. Once a sufficient number of background test statistic values have been generated, we can count the number of times the background test statistic exceeded $\lambda_{inj}^{med}$ from the signal injections. The number of background test statistic values that exceeded $\lambda_{inj}^{med}$ is then divided by the number of sky scans performed to obtain a background false positive rate. Figure \ref{fig:AccidentalChkGRBBackground} shows the test statistic distributions for background (black) and injected signal (red). An investigation into this background rate revealed that no background test statistic values were found to exceed the $\lambda_{inj}^{med}$ after 2,000 trials. We therefore conclude that the false positive rate must be very small ($\leq 10^{-3}$).
\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.7\textwidth,keepaspectratio]{NSInjection_TS_Distribution_WithBkgTSDistribution.png}
  \end{center}
  \caption[Background Test Statistic Check]{Test statistic distribution for signal injections (red) and background trials in the same declination band. The bi-modal shape of the background test statistic distribution arises from the inclusion of test statistic values returned by both the coarse and fine sky scans. The second background distribution does not begin at zero because the fine scan follow up is only performed for best fit flares that reach a certain test statistic threshold}.
  \label{fig:AccidentalChkGRBBackground}
\end{figure}

In the small background limit, the Neyman 90$\%$ confidence upper limit for a null observation is 2.3. Thus our volumetric rate limit is given by
\begin{equation}
R = \left(\frac{2.3}{\tau \cdot V_{90}}\right) \; \text{(Mpc$^{-3} \cdot$yr$^{-1}$)}
\end{equation}
A histogram of the rate limit for possible choices of jet parameters is given in Figure \ref{fig:ParameterDepRateLimit}. Two estimates of the local core-collapse SNe rate are plotted as well to provide context to the limits. The plot indicates that the analysis method is currently only sensitive to the expected choked GRB for the most powerful values of the jet parameters. This is especially true after one takes into account that the plotted limits are assuming all core-collapse SNe with these choked jets are in alignment with Earth. 

Several improvements to the method will be required before more meaningful limits can be placed on more realistic choices of the model parameters. Nonetheless, this represents the first results of a point source search using the low-energy data stream of the IceCube detector. As such, any limits that can be derived are interesting at this point as they will be complementary to other IceCube analyses. There is likely a large amount of sensitivity to be gained in further development of low-energy reconstruction techniques and analysis methods. If these advancements are made, then the method presented here could begin probe the most interesting regions of parameter space for this model of core-collapse SNe neutrino emission.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=1.0\textwidth,keepaspectratio]{RateLimit_2DHisto_wContours_SysAdj.png}
  \end{center}
  \caption[Choked GRB Volumetric Rate Limit]{Histogram of the rate limit on choked GRBs in the nearby universe. The bin for canonical values of the neutrino emission model is marked by the yellow star. The dashed line contour gives the rate of core-collapse supernovae within 10 Mpc as measured by Kistler et al. \cite{2011PhRvD..83l3008K}. The dot-dashed line is the volumetric rate extracted from a large survey of SNe in the local universe \cite{2011MNRAS.412.1419L}}.
  \label{fig:ParameterDepRateLimit}
\end{figure}

\chapter{Conclusion}
In the presented analysis, we performed a search for time-dependent sources of neutrino emission in an event sample obtained through an entirely new event selection focused on low energy muon neutrino events. The final level dataset consisted of 22,040 events from the Northern sky acquired over the course of 330 days. A search over these events for the most significant instance of spatial and temporal clustering found a hot spot located at 268.75$^{\circ}$ R.A. and 54.25$^{\circ}$ declination. The p-value for this flare after appropriate trials factor corrections is 56$\%$, and so the flare is completely consistent with background expectations. We are able to use this non-detection to constrain the level of neutrino emission for possible sources during the observation period.

In particular, this analysis focused on a slow jet model of neutrino emission from core-collapse SNe. Some of the physical processes at work in these spectacular explosions is still unknown, but detection of neutrino emission would provide great insight into the dynamics of the central engine powering these cataclysmic events. In the event that a core-collapse supernova event did occur during the observation period of this analysis, we can use the null result to constrain any neutrino emission models for the event. The neutrino emission expected from the slow jet model proposed by Razzaque, M\'{e}sz\'{a}ros and Waxman presents exciting detection prospects for IceCube analyses focusing on lower energy events. We therefore choose to set limits on the jet parameters of this model.

While the limits derived in Chapter XI are interesting for certain values of the jet energy and boost factor, it is clear that the analysis method in its current status is insensitive to the nominal values of the slow jet model. However, the outlook for future analyses of this type is rather optimistic. The analysis presented in this document represents the first iteration of a modified analysis technique applied on a event selection that has never been used before. Thus, there is substantial room for improvement that will ultimately lead to large gains in analysis sensitivity.

Because the event selection used in this analysis was developed independently from the filter level, there are many unexplored ways in which the selection could be optimized for signal retention. Smarter use of veto information, the inclusion of advanced reconstruction techniques, and the use of machine learning techniques at earlier levels of the event selection could possibly lead to significant improvement of the effective area at the final level. This in turn will allow the analysis method to probe deeper into the nearby universe, enhancing the possibility of catching a potential core-collapse event. Due to the inverse square dependence of the neutrino fluence, a factor of two improvement in the final level effective area from better selection techniques should lead to a $\sqrt{2}$ improvement in the visibility distance of the analysis. This in turn would lead to a improvement in the volumetric rate limit by a factor of 2$\sqrt{2}$. Increased exposure time for the analysis should also lead to improvement of the volumetric rate with the sensitivity scaling like $\tau^{1/2}$.

Improvement of the reconstruction of low energy events and refinement of the point spread function for these events will likely provide the most dramatic increases in analysis performance. The reconstruction method used for the final level sample is primarily suited to energetic muon tracks featuring many DOM hits in the detector. However, the low energy events in the sample will generally only produce a handful of hits and are also more likely to include noise hits due to their location in DeepCore. There has recently been heavy development of reconstruction methods for low energy events in DeepCore. This is largely due to the recent success of oscillation parameter studies using low energy events. Adaptation of these advanced reconstruction techniques currently used for 10-50 GeV atmospheric neutrinos will likely result in significant improvement in resolution of the low energy tracks of our analysis. Lastly, research and development work on a low-energy extension to the IceCube detector, PINGU (Phased-In Next Generation Upgrade) \cite{2014arXiv1401.2046T}, should provide additional improvements to reconstruction methods at low energy.

One final aspect of the analysis that has potential for optimization is the event error modeling. As Figure \ref{fig:PSFModel} illustrates, the fit of the assumed event PSF to the data is rather poor at low energies. Disagreement between the model and actual data will necessarily result in worse sensitivity for the analysis method. It is the author's opinion that better modeling of the PSF in the likelihood calculation will result in a large increase in the sensitivity of the analysis and that this should be the primary focus in future development of the analysis.

If these improvements to the analysis method are taken into consideration, then it seems likely that future iterations of this analysis will be able to markedly improve upon the presently reported limits. Maturation of this analysis technique would complement the current capabilities of IceCube quite nicely as it would fill in a relatively unexplored energy range. The physics potential provided by time-dependent neutrino searches should not be underestimated. The full-sky monitoring capabilities of IceCube combined with the exemplary reliability of the data acquisition system allow these searches to be prepared for any potential neutrino signal. The recent discovery of the diffuse flux of astrophysical neutrinos signals the beginning of the neutrino astronomy era. And with any luck, the first confirmed sources will be discovered in short order.

%% Sensitivity Gains T^{1/2}, PSF modeling, signal retention, reconstruction accuracy
\appendix

\chapter{Appendix A -- Event Selection Details}
This appendix contains additional details of event selection cuts from Chapter 7. Distributions of cut parameters for simulation and data belonging to both the low-energy stream (LES) and high-energy stream (HES) are given.
\section{Hit Series Cleaning}
The ``nominal DOM hit map" for an event is referred to at several points in the event selection chapter without receiving an explicit definition. We will describe the hit cleaning method used to produce this hit map in this section. An additional hit cleaning method used in some of the event selection cuts will also be described.
\begin{adjustwidth}{1.5em}{1.5em}
\setlength{\parindent}{0pt}
\textbf{SeededRT Hit Cleaning} - Given the relatively high rate of noise-induced DOM hits in IceCube, it is generally inadvisable to use the complete hit map of an event without making any attempt to remove spurious hit information. For many IceCube analyses, this issue is handled by simply selecting only DOM hits that show local coincidence. These hits are known as hard local coincidence or HLC hits. DOM hits that lack this coincidence condition are referred to as soft local coincidence or SLC hits. Not all physically relevant DOM hits will manage to trigger the HLC condition, and there is often relevant timing and spatial information to be gained from SLC hits. For this reason we choose to employ an algorithm that will attempt find SLC hits that are likely physics related due to their close proximity to HLC hits in the total DOM hit map for the event.

The hit cleaning begins by selecting all HLC events in the DOM hit series. These hits will function as the seed of the SeededRT cleaning. Each HLC seed hit is then used to search for any nearby SLC hits as these hits are probably correlated. DOMs which lie inside of a sphere about the seed DOM of radius $R=150$m are examined for any SLC hits. Those that do show hits will then have the time of their hit compared to the time of the hit in the seed DOM. If the time residual is less than 1000 ns then the event is kept in the hit series. This process is iterated for each HLC seed hit to obtain the final hit series.

\textbf{ClassicRT Hit Cleaning} -The ClassicRT hit cleaning method is an alternative hit cleaning technique that is also used during the event selection process. Just like the SeededRT cleaning, CRT will search over DOM hits to look for any hits in any nearby DOMs within a certain time window. The SeededRT method used HLC hits as the seeds to look for SLC hits to include in the final hit map. The `classic' method will use \textit{all} DOM hits (SLC and HLC) as a seed. The coincidence algorithm is iterated for each DOM hit for the event. Any hits that show no coincidence with any other DOM hits are dropped from the hit map. The standard definition for the spatial and time window parameters (R and T respectively) used to search for coincident hits are set to 150 meters and 1000 ns. This method will typically result in the inclusion of more noise related DOM hits, and it is often not used for event reconstruction purposes. It is useful in maximizing the retention of possibly physics related veto region hits.

\end{adjustwidth}
\setlength{\parindent}{17.5pt}

\section{Level 3 Cuts}
While the cuts used to eliminate noise events at this level were explicitly described, the other cuts applied at this data level were only referred to without defining them. Descriptions of these cuts are provided. LES events that fail only the C2QR6 and Charge Ratio cuts are moved to the HES branch of the event selection instead of being dropped altogether. These events will continue on to the next selection level provided they pass all L3 cuts for HES events.
\subsection{LES}
\begin{adjustwidth}{1.5em}{1.5em}
\setlength{\parindent}{0pt}
\textbf{Causal Veto PE} - This cut examines the results of application of the DeepCore filter algorithm to an uncleaned hit map for the event. The charge of all veto region hits is summed. Events with 7 or more photo-electrons (pe) worth of charge fulfilling the veto criteria are cut.

\textbf{NAbove200PE} - This cut searches for any hits above -200 m in detector Z-coordinate prior to the DeepCore trigger time. The sum of the charge of all pulses in these hits is then summed. Events with 12 or more photo-electrons (pe) worth of charge are cut.

\textbf{C2QR6} -This cut examines the amount of charge deposited in DOM PMTs during the first 600 ns of the event compared to the total deposited charge of the event. In order to avoid noise hit contribution, the SeededRTCleaned hit series with the first 2 hits removed is used. Background throughgoing muons typically have lower values as their charge is deposited over longer timescales. Events whose C2QR6 value is less than 0.4 are removed. This cut is muon track length dependent and therefore energy dependent. For this reason it is not used in the HES event selection as it no longer provides good separation between signal and background.

\textbf{VertexZ} - A simple vertex hypothesis for the event is given by the earliest pulse in the SeededRTCleaned hit series for the event. The Z-location of this vertex must be located below -120 m in detector coordinates or it will be cut. 

\textbf{RTVeto250PE} - An algorithm searching for clusters of hits returns the largest cluster found in the veto region. This algorithm scans over veto region hits and performs a RT cleaning method to identify correlated hits (R=250 m, T= 1000 ns). The largest cluster of hits in the veto region is then recorded. If this hit cluster contains 4 or more pe worth of deposited charge, then the event is cut.

\textbf{Charge Ratio} - The SeededRTCleaned pulse map for the event is split into fiducial and veto regions. The ratio of the total charge deposited in DOM PMTs in the veto region over charge deposited in the fiducial region is used as a cut parameter. Events whose ratio is greater than 1.5 are removed.

\end{adjustwidth}
\setlength{\parindent}{17.5pt}
\pagebreak

\begin{figure}
\centering
\subfigure[Causal Veto PE]{\label{fig:lesl31}\includegraphics[width=75mm]{StdDC_CausalVetoPE.png}}
\subfigure[NAbove200 PE]{\label{fig:lesl32}\includegraphics[width=75mm]{StdDC_NAbove200PE.png}}
\subfigure[Vertex Z]{\label{fig:lesl33}\includegraphics[width=75mm]{StdDC_VertexZ.png}}
\subfigure[C2QR6]{\label{fig:lesl34}\includegraphics[width=75mm]{C2QR6_LES.png}}
\subfigure[RTVeto250 PE]{\label{fig:lesl35}\includegraphics[width=75mm]{StdDC_RTVeto250PE.png}}
\caption[L3 LES Cut Distributions]{Normalized distributions of the L3 LES cut variables for simulation and real data.} 
\label{fig:LES_L3_Plots}
\end{figure}

\subsection{HES}
\begin{adjustwidth}{1.5em}{1.5em}
\setlength{\parindent}{0pt}
\textbf{NAbove200PE} - This cut searches for any hits above -200 m in Z in the detector prior to the DeepCore trigger time. The sum of the charge of all pulses in these hits is then summed. Events with 5 or more photo-electrons (pe) worth of charge are cut.

\textbf{VertexZ} - A simple vertex hypothesis for the event is given by the earliest pulse in the SeededRTCleaned hit series for the event. The Z-location of this vertex must be located below -200 m in detector coordinates or it will be cut. 

\textbf{VertexXY} - A simple vertex hypothesis for the event is given by the earliest pulse in the SeededRTCleaned hit series for the event. The location of this vertex in the detector X-Y plane must lie within the polygon defining the fiducial volume of the HES branch. 

\textbf{RTVeto250PE} - An algorithm searching for clusters of hits returns the largest cluster found in the veto region. This algorithm scans over veto region hits and performs a RT cleaning method to identify correlated hits (R=250 m, T= 1000 ns). The largest cluster of hits in the veto region is then recorded. If this hit cluster contains 3 or more pe worth of deposited charge, then the event is cut.

\textbf{Causal Veto PE} - This cut examines the results of application of the DeepCore filter algorithm to an uncleaned hit map for the event. The charge of all veto region hits is summed. Events with more than 4 or more photo-electrons (pe) worth of charge fulfilling the veto criteria are cut.
\end{adjustwidth}
\setlength{\parindent}{17.5pt}

\begin{figure}[ht]
\centering
\subfigure[Causal Veto PE]{\label{fig:hesl31}\includegraphics[width=75mm]{ExpDC_CausalVetoPE.png}}
\subfigure[NAbove200 PE]{\label{fig:hesl32}\includegraphics[width=75mm]{ExpDC_NAbove200PE.png}}
\subfigure[Vertex Z]{\label{fig:hesl33}\includegraphics[width=75mm]{ExpDC_VertexZ.png}}
\subfigure[RTVeto250 PE]{\label{fig:hesl34}\includegraphics[width=75mm]{ExpDC_RTVeto250PE.png}}
\caption[L3 HES Cut Distributions]{Normalized distributions of the L3 HES cut variables for simulation and real data.} 
\label{fig:HES_L3_Plots}
\end{figure}

\pagebreak

\section{Level 4 Cuts}
The cuts used at level 4 of the data selection process are described qualitatively in Chapter 7. Explicit definitions will be provided in this Appendix in addition to plots showing the data and simulation event distributions for these cut parameters.
\subsection{LES}
\begin{adjustwidth}{1.5em}{1.5em}
\setlength{\parindent}{0pt}
\textbf{LineFit Zenith} - The LineFit reconstruction is applied to the standard SeededRT cleaned hit series. This reconstruction makes no use of likelihood techniques. It simply tries to provide a straight track fit to the DOM hit series based on the timing and location of DOM hits. All events whose reconstructed zenith lies 5$^{\circ}$ above the horizon are removed from the analysis.

\textbf{SPE 6 Zenith} - A 6-iteration SPE likelihood reconstruction is applied to the standard SeededRT cleaned hit series. All events whose reconstructed zenith lies 5$^{\circ}$ above the horizon are removed from the analysis.

\textbf{NChannel Requirement} - A minimum on the number of DOM hits in the standard SeededRT cleaned hit series is enforced. Events having fewer than 10 hits after hit cleaning are removed.

\textbf{NVetoEarlySRT} - A SeededRT hit cleaning is performed with more relaxed spatial coincidence requirements (R=300 m). The total number of veto region hits that occur prior to the first fiducial volume hit are counted. Events that show more than one veto region hits are removed.

\textbf{CRT DeepCore Filter Hits} - A ClassicRT hit cleaning is used to generate a hit series possibly containing veto region hit information cleaned during the standard hit cleaning. The search radius parameter R is set to 150 meters and the T parameter is set to 1000 ns. This hit series is then fed to the standard DeepCore Filter algorithm (see section 7.1). Events with more than one veto hit identified by the filter are cut from the analysis.
\end{adjustwidth}
\setlength{\parindent}{17.5pt}

\pagebreak
\begin{figure}
\centering
\subfigure[NChannel]{\label{fig:lesl41}\includegraphics[width=75mm]{L4_LES_NchCleaned.png}}
\subfigure[NVetoEarly SRT]{\label{fig:lesl42}\includegraphics[width=75mm]{L4_LES_NVetoEarlySRTHits.png}}
\subfigure[LineFit Zenith Distribution]{\label{fig:lesl43}\includegraphics[width=75mm]{L4_LES_LFZenithDist.png}}
\subfigure[SPE6 Zenith Distribution]{\label{fig:lesl44}\includegraphics[width=75mm]{L4_LES_SPE6ZenithDist.png}}
\subfigure[CRT DeepCore Filter Hits]{\label{fig:lesl45}\includegraphics[width=75mm]{L4_LES_CRTDCFilterHits.png}}
\caption[L4 LES Cut Distributions]{Normalized distributions of the selection variables used for data cuts on the LES stream at Level 4. The cut region is given by the shaded area.} 
\label{fig:LESL4ParameterPlots}
\end{figure}

\subsection{HES}
\begin{adjustwidth}{1.5em}{1.5em}
\setlength{\parindent}{0pt}
\textbf{LineFit Zenith} - The LineFit reconstruction is applied to the standard SeededRT cleaned hit series. This reconstruction makes no use of likelihood techniques. It simply tries to provide a straight track fit to the DOM hit series based on the timing and location of DOM hits. All events whose reconstructed zenith lies 5$^{\circ}$ above the horizon are removed from the analysis.

\textbf{SPE 6 Zenith} - A 6-iteration SPE likelihood reconstruction is applied to the standard SeededRT cleaned hit series. All events whose reconstructed zenith lies 5$^{\circ}$ above the horizon are removed from the analysis.

\textbf{Space-Angle between LineFit and SPE6 Fit} - This cut is pretty self-explanatory. The space angle between reconstructed directions given by LineFit and SPE6 is calculated. Events whose reconstructions disagree by 30$^{\circ}$ or more are removed.

\textbf{NVetoEarlySRT} - A SeededRT hit cleaning is performed with more relaxed spatial coincidence requirements (R=300 m). The total number of veto region hits that occur prior to the first fiducial volume hit are counted. Events that show any number of veto region hits are removed.

\textbf{NChannel Requirement} - A minimum on the number of DOM hits in the standard SeededRT cleaned hit series is enforced. Events having fewer than 10 hits after hit cleaning are removed.

\textbf{Topological Split} - The Topological Splitter module is run on all HES stream events to search for evidence of coincident muon events. Any event that shows evidence of two muon tracks is cut.

\textbf{OnlineL2 Geo Split} - The vertex and direction of the two reconstructions provided by L2 geometrical event splitter are examined. If one of the splits has an event vertex located above -100 m in detector z-coordinate, the event is removed.
\end{adjustwidth}
\setlength{\parindent}{17.5pt}

\begin{figure}
\centering
\subfigure[NChannel]{\label{fig:hesl41}\includegraphics[width=75mm]{L4_HES_NchCleaned.png}}
\subfigure[NVetoEarly SRT]{\label{fig:hesl42}\includegraphics[width=75mm]{L4_HES_NVetoEarlySRTHits.png}}
\subfigure[LineFit Zenith Distribution]{\label{fig:hesl43}\includegraphics[width=75mm]{L4_HES_LFZenithDist.png}}
\subfigure[SPE6 Zenith Distribution]{\label{fig:hesl44}\includegraphics[width=75mm]{L4_HES_SPE6ZenithDist.png}}
\subfigure[Space Angle between LineFit-SPE6]{\label{fig:hesl45}\includegraphics[width=75mm]{L4_HES_AngularSeparationLFSPE.png}}
\caption[L4 HES Cut Distributions]{Normalized distributions of the selection variables used for data cuts on the HES stream at Level 4. The cut region is given by the shaded area.} 
\label{fig:HESL4ParameterPlots}
\end{figure}

\pagebreak
\section{Level 5 Cuts}
Level 5 of the cut selection only features cuts on the directions of events after re-fitting to check for possibly mis-reconstructed downgoing muon events. These cuts were described in detail and the zenith distributions for both the LES and HES streams were plotted for one of the reconstructions (SPE2 with Early Hits Removed Zenith Cut). Here we will plot the other reconstruction used at this level in the LES branch of the event selection (Figure \ref{fig:LES_TightSPE6Reco}).

\textbf{TightSPE6} - The standard 6-iteration SPE likelihood reconstruction is applied to a DOM hit series featuring tighter settings for SeededRT cleaning (R= 75 m, T = 625 ns). This reconstruction is only used on the LES branch of the event selection.

\begin{figure}
  \begin{center}
    \includegraphics[width=0.70\textwidth,keepaspectratio]{LES_TightSPE6ZenithDistribution_L5.png}
  \end{center}
  \caption[L5 TightSPE6 Reconstruction Zenith Distribution]{Distribution of reconstructed Cos($\Theta_{zen}$) for simulation and data. The TightSPE6 reconstruction is applied on the LES branch of the dataset. The SeededRT cleaning used to generate the hit map over which the reconstruction is applied has tighter settings for radius and time (R = 75 m [150 m nominal], T = 625 ns [1000 ns nominal]). The shaded area denotes the cut region.}
  \label{fig:LES_TightSPE6Reco}
\end{figure}
\pagebreak

\section{Level 6 BDT}
The BDT input parameters were described in detail in Chapter 7. The distributions for these selection variables are plotted here. The large disparity between CORSIKA and data in the HES distributions stems from a lack of statistics. This is not problematic issue though, as we use the real data as opposed to simulation for our background estimate. Examination of the final level distributions in Appendix B shows that the data at the final level is well characterized by the neutrino simulation.

\begin{figure}
\centering
\subfigure[Reconstructed Event Vertex Z]{\label{fig:lesbdt1}\includegraphics[width=75mm]{LES_BDTParam_FiniteReco_Z_L5PreSelected_Rates.png}}
\subfigure[Reconstructed Event Vertex R]{\label{fig:lesbdt2}\includegraphics[width=75mm]{LES_BDTParam_FiniteReco_R_L5PreSelected_Rates.png}}
\subfigure[Downgoing Veto Track Charge]{\label{fig:lesbdt3}\includegraphics[width=75mm]{LES_VetoTrackCharge_L5PreSelected_Rates.png}}
\subfigure[Direct Hits D]{\label{fig:lesbdt4}\includegraphics[width=75mm]{LES_DirectHitsD_L5PreSelected_Rates.png}}
\subfigure[SplineMPE16 Reduced LogLikelihood]{\label{fig:lesbdt5}\includegraphics[width=75mm]{LES_BDTParam_SplineMPEMod_RLogL_PreBDTCutL5_Rates.png}}
\subfigure[Avg DOM-Weighted Track Distance]{\label{fig:lesbdt6}\includegraphics[width=75mm]{LES_AvgDomDistQ_L5PreSelected_Rates.png}}
\caption[L6 LES BDT Selection Variables]{Distributions of the LES BDT selection variables for simulation and real data.} 
\label{fig:LESBDTParameterPlots}
\end{figure}

\begin{figure}
\centering
\subfigure[Direct Length D]{\label{fig:hesbdt1}\includegraphics[width=75mm]{750px-HES_DirectHitsD_L5PreSelected_Rates.png}}
\subfigure[Direct Hits D]{\label{fig:hesbdt2}\includegraphics[width=75mm]{750px-HES_LDirD_L5PreSelected_Rates.png}}
\subfigure[SplineMPE16 Reduced LogLikelihood]{\label{fig:hesbdt3}\includegraphics[width=75mm]{HES_BDTParam_SplineMPEMod_RLogL_L5PreSelected_Rates.png}}
\subfigure[Avg DOM-Weighted Track Distance]{\label{fig:hesbdt4}\includegraphics[width=75mm]{HES_AvgDomDistQ_L5PreSelected_Rates.png}}
\caption[L6 HES BDT Selection Variables]{Distributions of the HES BDT selection variables for simulation and real data. There is very little CORSIKA simulation present at this level for the HES branch of the event selection.} 
\label{fig:HESBDTParameterPlots}
\end{figure}

\chapter{Appendix B -- Final Level Distributions}
This appendix contains distributions of select event parameters for both simulation and real events at the final selection level. The plots here show that the data is mostly well modeled by our neutrino simulation. There are some differences in absolute rate that are largely due to uncertainty in the normalization of the atmospheric neutrino spectrum. Definitions for the plotted parameters are also provided.
\begin{adjustwidth}{2.5em}{2.5em}
\setlength{\parindent}{0pt}
\textbf{N Channel} - N Channel or Nch refers to the number of DOMs receiving light from the event. This can be interpreted as a crude energy proxy as more energetic events will have a larger light yield.

\textbf{Detector Azimuth} - The detector azimuth for a given event is the best fit azimuthal direction from the track reconstruction method. A azimuth value of 0$^{\circ}$ corresponds to due East in local grid coordinates.

\textbf{Detector Zenith} - The detector zenith for a given event is the best fit zenith direction from the track reconstruction method. This value ranges from 0$^{\circ}$ to 180$^{\circ}$ with 0$^{\circ}$ representing events traveling downward from the Southern sky above the detector and 180$^{\circ}$ representing events coming upward through the Earth.

\textbf{Reduced Log-likelihood} - The reconstruction method used in this analysis makes use of a likelihood method to obtain a best fit to the event data. The method references a splined table of ice model parameters in addition to probabilities for certain DOMs in the array to generate hits from light emitted by a hypothetical particle track. Low values of reduced log-likelihood (rllh) indicate the track hypothesis has a good fit to the data.

\textbf{Finite Reco Z} - A separate reconstruction makes an attempt to fit a starting and stopping point to the best fit track of the event. This reconstruction also uses a likelihood method to arrive at an optimized solution. Finite Reco Z is the location of the starting vertex of the event in the detector z-coordinate. Events that start at lower depths in the ice are more likely to be upgoing neutrinos as opposed to downgoing atmospheric muons.
\end{adjustwidth}
The plotted distributions are also separated by whether they belong to the high-energy event selection stream (HES) or the low-energy stream (LES). This allows one to compare the data-simulation agreement between the two branches. Additionally, the simulation sets used for development of both branches are not the same which can lead to differing levels of agreement at the final level.

\begin{figure}\label{fig:FL_nch_by_energy}
\centering
\subfigure[LES Nch-Energy Relation]{\label{fig:i}\includegraphics[width=75mm]{FinalLevel_Nch_Vs_Energy_NuMu_LESOnly.png}}
\subfigure[HES Nch-Energy Relation]{\label{fig:ii}\includegraphics[width=75mm]{FinalLevel_Nch_Vs_Energy_NuMu_HESOnly.png}}
\caption[Final Level NChannel-Energy Relation]{NCh-Energy relation for simulated $\nu_\mu$ neutrino events at the final level. Nch is a shorthand term for the number of DOMs receiving light from an event. Subfigure (a) shows the correlation for events belonging to the Low-Energy Stream (LES) branch of the event selection while (b) shows the same for events belonging to the High-Energy Stream or HES branch.} 
\end{figure}

\begin{figure}
\centering
\subfigure[Reconstructed Event Azimuth]{\label{fig:reco_azi}\includegraphics[width=75mm]{LES_SplineMPE_AzimuthDist_FinalLevelDist.png}}
\subfigure[Reconstructed Event Zenith]{\label{fig:reco_zen}\includegraphics[width=75mm]{LES_SplineMPE_ZenithDist_FinalLevelDist.png}}
\subfigure[Reconstruction Reduced Log-likelihood]{\label{fig:reco_rlogl}\includegraphics[width=75mm]{LES_BDTParam_SplineMPEMod_RLogL_FinalLevelDist.png}}
\subfigure[Reconstructed Event Vertex Detector Z-position]{\label{fig:reco_z}\includegraphics[width=75mm]{LES_BDTParam_FiniteReco_Z_FinalLevelDist.png}}
\caption[Final Level LES Event Distributions]{Distributions of  for simulation and real data at the final event level. Plots above for the Low-Energy Stream (\textbf{LES}) branch of the event sample.} 
\label{fig:final_level_distros}
\end{figure}

\begin{figure}
\centering
\subfigure[Reconstructed Event Azimuth]{\label{fig:hes_reco_azi}\includegraphics[width=75mm]{HES_BDTParam_SplineMPEMod_AzimuthDist_FinalLevel_Rates.png}}
\subfigure[Reconstructed Event Zenith]{\label{fig:hes_reco_zen}\includegraphics[width=75mm]{HES_BDTParam_SplineMPEMod_ZenithDist_FinalLevel_Rates.png}}
\subfigure[Reconstruction Reduced Log-likelihood]{\label{fig:hes_reco_rlogl}\includegraphics[width=75mm]{HES_BDTParam_SplineMPEMod_RLogL_FinalLevel_Rates.png}}
\subfigure[Reconstructed Event Vertex Detector Z-position]{\label{fig:hes_reco_z}\includegraphics[width=75mm]{HES_BDTParam_FiniteReco_Z_L5PreSelected_Rates.png}}
\caption[Final Level HES Event Distributions]{Distributions of event parameters for simulation and real data at the final event level. Plots above for the High-Energy Stream (\textbf{HES}) branch of the event sample.} 
\label{fig:hes_final_level_distros}
\end{figure}

\begin{postliminary}
\references
%\bibliography{jdthesis}{}

\begin{vita}
Jacob Daughhetee was born in Augusta, GA in 1987, but he spent the entirety of his grade school years in Birmingham, AL. Jacob attended Auburn University as an undergrad where he received a Bachelor of Arts in Philosophy in addition to a Bachelor of Science in Physics. During his time at Auburn, Jacob participated in active research in the field of atomic and molecular optics. After graduating from Auburn University, Jacob enrolled in graduate school at the Georgia Institute of Technology where he would begin his study of particle astrophysics.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.98\textwidth,keepaspectratio]{HeroShot.jpg}
  \end{center}
  \label{fig:HeroShot}
\end{figure}

\end{vita}
\end{postliminary}
\end{document}
